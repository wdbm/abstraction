{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# TensorFlow\n",
    "\n",
    "TensorFlow is an open source software library for numerical computation using data flow graphs. In a data flow graph, nodes represent mathematical operations and edges represent the multidimensional data arrays (tensors) communicated between them.\n",
    "\n",
    "![](https://raw.githubusercontent.com/wdbm/abstraction/master/media/2016-05-14T1754Z.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "tf.TF_CPP_MIN_LOG_LEVEL = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello world\n"
     ]
    }
   ],
   "source": [
    "# Create a constant operation. This operation is added as a node to the default graph.\n",
    "hello = tf.constant(\"hello world\")\n",
    "\n",
    "# Start a TensorFlow session.\n",
    "sess = tf.Session()\n",
    "\n",
    "# Run the operation and get the result.\n",
    "print(sess.run(hello))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# tensors, ranks, shapes and types\n",
    "\n",
    "|**rank**|**mathamatical object**|**shape**  |**example**                       |\n",
    "|--------|-----------------------|-----------|----------------------------------|\n",
    "|0       |scalar                 |`[]`       |`3`                               |\n",
    "|1       |vector                 |`[3]`      |`[1. ,2., 3.]`                    |\n",
    "|2       |matrix                 |`[2, 3]`   |`[[1., 2., 3.], [4., 5., 6.]]`    |\n",
    "|3       |3-tensor               |`[2, 1, 3]`|`[[[1., 2., 3.]], [[7., 8., 9.]]]`|\n",
    "|n       |n-tensor               |...        |...                               |\n",
    "\n",
    "|**data type**|Python type|**description**       |\n",
    "|-------------|-----------|----------------------|\n",
    "|`DT_FLOAT`   |`t.float32`|32 bits floating point|\n",
    "|`DT_DOUBLE`  |`t.float64`|64 bits floating point|\n",
    "|`DT_INT8`    |`t.int8`   |8 bits signed integer |\n",
    "|`DT_INT16`   |`t.int16`  |16 bits signed integer|\n",
    "|`DT_INT32`   |`t.int32`  |32 bits signed integer|\n",
    "|`DT_INT64`   |`t.int64`  |64 bits signed integer|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# TensorFlow mechanics\n",
    "\n",
    "- 1 Build a graph using TensorFlow operations.\n",
    "- 2 Feed data to TensorFlow and run the graph.\n",
    "- 3 Update variables in the graph and return values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "node1: Tensor(\"Const_1:0\", shape=(), dtype=float32)\n",
      "node2: Tensor(\"Const_2:0\", shape=(), dtype=float32)\n",
      "node3: Tensor(\"Add:0\", shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "node1 = tf.constant(3.0, tf.float32)\n",
    "node2 = tf.constant(4.0) # (also tf.float32 by default)\n",
    "node3 = tf.add(node1, node2)\n",
    "\n",
    "print(\"node1: {node}\".format(node = node1))\n",
    "print(\"node2: {node}\".format(node = node2))\n",
    "print(\"node3: {node}\".format(node = node3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sess.run(node1, node2): [3.0, 4.0]\n",
      "sess.run(node3):        7.0\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "\n",
    "print(\"sess.run(node1, node2): {result}\".format(\n",
    "    result = sess.run([node1, node2])\n",
    "))\n",
    "print(\"sess.run(node3):        {result}\".format(\n",
    "    result = sess.run(node3)\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.5\n"
     ]
    }
   ],
   "source": [
    "a = tf.placeholder(tf.float32)\n",
    "b = tf.placeholder(tf.float32)\n",
    "\n",
    "# Create a node that is a shortcut for tf.add(a, b).\n",
    "adder_node = a + b\n",
    "\n",
    "result = sess.run(\n",
    "    adder_node,\n",
    "    feed_dict = {\n",
    "        a: 3,\n",
    "        b: 4.5\n",
    "    }\n",
    ")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 3.  7.]\n"
     ]
    }
   ],
   "source": [
    "result = sess.run(\n",
    "    adder_node,\n",
    "    feed_dict = {\n",
    "        a: [1,3],\n",
    "        b: [2, 4]\n",
    "    }\n",
    ")\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22.5\n"
     ]
    }
   ],
   "source": [
    "add_and_triple = adder_node * 3.\n",
    "\n",
    "result = sess.run(\n",
    "    add_and_triple,\n",
    "    feed_dict = {\n",
    "        a: 3,\n",
    "        b: 4.5\n",
    "    }\n",
    ")\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# session\n",
    "\n",
    "A `Session` is a class for running TensorFlow operations. A session object encapsulates the environment in which operations are executed and tensors are evaluated. For example, `sess.run(c)` evaluates the tensor `c`.\n",
    "\n",
    "A session is run using its `run` method:\n",
    "\n",
    "```Python\n",
    "tf.Session.run(\n",
    "    fetches,\n",
    "    feed_dict    = None,\n",
    "    options      = None,\n",
    "    run_metadata = None\n",
    ")\n",
    "```\n",
    "\n",
    "This method runs operations and evaluates tensors in fetches. It returns one epoch of TensorFlow computation, by running the necessary graph fragment to execute every operation and evaluate every tensor in fetches, substituting the values in `feed_dict` for the corresponding input values. The `fetches` option can be a single graph element, or an arbitrary nested list, tuple, namedtuple, dict or OrderedDict containing graph elements at its leaves. The value returned by `run` has the same shape as the fetches argument, where the leaves are replaced by the corresponding values returned by TensorFlow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10 20]\n",
      "[array([10, 20], dtype=int32), array([ 1.,  2.], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "\n",
    "a = tf.constant([10 ,  20])\n",
    "b = tf.constant([1.0, 2.0])\n",
    "\n",
    "print(sess.run(a))\n",
    "print(sess.run([a, b]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Create two variables.\n",
    "weights = tf.Variable(\n",
    "    tf.random_normal(\n",
    "        [784, 200],\n",
    "        stddev = 0.35\n",
    "    ),\n",
    "    name = \"weights\"\n",
    ")\n",
    "biases = tf.Variable(\n",
    "    tf.zeros([200]),\n",
    "    name = \"biases\"\n",
    ")\n",
    "\n",
    "# Create an operation to initialize the variables.\n",
    "init_op = tf.global_variables_initializer()\n",
    "\n",
    "# more code\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init_op)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "# single variable linear regression\n",
    "\n",
    "- hypothesis: ${H\\left(x\\right)=Wx+b}$\n",
    "\n",
    "- cost function: ${\\textrm{cost}\\left(W,b\\right)=\\frac{1}{m}\\Sigma_{i=1}^{m}\\left(H\\left(x^{i}\\right)-y^{i}\\right)^{2}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 0, cost: 0.327590852976, W: [ 1.69378853], b: [-1.30574894]\n",
      "step: 500, cost: 0.0235481653363, W: [ 1.17822742], b: [-0.40515277]\n",
      "step: 1000, cost: 0.00212170509622, W: [ 1.05349815], b: [-0.12161381]\n",
      "step: 1500, cost: 0.000191168233869, W: [ 1.01605844], b: [-0.0365047]\n",
      "step: 2000, cost: 1.72240033862e-05, W: [ 1.00482023], b: [-0.01095748]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.set_random_seed(777)\n",
    "\n",
    "# Create some data.\n",
    "x_train = [1, 2, 3]\n",
    "y_train = [1, 2, 3]\n",
    "\n",
    "# Build the graph using TensorFlow operations. With the hypothesis H(x) = Wx + b, the goal is to try to find values for W and b to in order to calculate y_data = x_data * W + b. Analytically, W should be 1 and b should be 0.\n",
    "W = tf.Variable(tf.random_normal([1]), name = \"weight\")\n",
    "b = tf.Variable(tf.random_normal([1]), name = \"bias\")\n",
    "\n",
    "# Define the hypothesis.\n",
    "hypothesis = x_train * W + b\n",
    "\n",
    "# Define the cost function.\n",
    "cost = tf.reduce_mean(tf.square(hypothesis - y_train))\n",
    "\n",
    "# Define a method of minimisation, in this case gradient descent. In gradient descent, steps proportional to the negative of the function gradient at the current point are taken. It is the method of steepest descent to find the local minimum of a function.\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate = 0.01)\n",
    "train = optimizer.minimize(cost)\n",
    "\n",
    "# Launch the graph in a session.\n",
    "sess = tf.Session()\n",
    "\n",
    "# Initialize global variables in the graph.\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# Fit.\n",
    "for step in range(2001):\n",
    "    sess.run(train)\n",
    "    if step % 500 == 0:\n",
    "        print(\"step: {step}, cost: {cost}, W: {W}, b: {b}\".format(\n",
    "            step = step,\n",
    "            cost = sess.run(cost),\n",
    "            W    = sess.run(W),\n",
    "            b    = sess.run(b)\n",
    "        ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# placeholders and variables\n",
    "\n",
    "A variable (`tf.Variable`) is used generally for trainable variables such as weights and biases for a model. A placeholder (`tf.placeholder`) is used to feed actual training examples. A variable is set with an initial value on declaration while a placeholder doesn't require an initial value on declaration, but has its value specified at run time using the session `feed_dict`. In TensorFlow, variables are trained over time while placeholders are input data that doesn't change as the model trains (e.g. input images and class labels for the images).\n",
    "\n",
    "A placeholder is a value that is input when TensorFlow is set to run a computation. A variable is a modifiable tensor that exists in TensorFlow's graph of interacting operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 0, cost: 19.0289134979, W: [-0.81955934], b: [ 0.05452838]\n",
      "step: 500, cost: 0.0066645629704, W: [ 0.90541184], b: [ 0.21502104]\n",
      "step: 1000, cost: 0.000600479834247, W: [ 0.97160774], b: [ 0.06454235]\n",
      "step: 1500, cost: 5.41025183338e-05, W: [ 0.99147761], b: [ 0.01937346]\n",
      "step: 2000, cost: 4.87493980472e-06, W: [ 0.99744177], b: [ 0.00581537]\n",
      "[ 1.00325716  2.0006988   2.99814057]\n",
      "[ 1.00325716  2.0006988   2.99814057]\n",
      "[ 1.00325716  2.0006988   2.99814057]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "W = tf.Variable(tf.random_normal([1]), name = \"weight\")\n",
    "b = tf.Variable(tf.random_normal([1]), name = \"bias\")\n",
    "\n",
    "# Create placeholders for tensors for x and y data.\n",
    "X = tf.placeholder(tf.float32, shape = [None])\n",
    "Y = tf.placeholder(tf.float32, shape = [None])\n",
    "\n",
    "hypothesis = x_train * W + b\n",
    "cost = tf.reduce_mean(tf.square(hypothesis - y_train))\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate = 0.01)\n",
    "train = optimizer.minimize(cost)\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# Fit.\n",
    "for step in range(2001):\n",
    "    cost_value, W_value, b_value, _ = sess.run(\n",
    "        [cost, W, b, train],\n",
    "        feed_dict = {\n",
    "            X: [1, 2, 3],\n",
    "            Y: [1, 2, 3]\n",
    "        }\n",
    "    )\n",
    "    if step % 500 == 0:\n",
    "        print(\"step: {step}, cost: {cost}, W: {W}, b: {b}\".format(\n",
    "            step = step,\n",
    "            cost = cost_value,\n",
    "            W    = W_value,\n",
    "            b    = b_value\n",
    "        ))\n",
    "\n",
    "# Test the trained model.\n",
    "print(sess.run(hypothesis, feed_dict={X: [5]}))\n",
    "print(sess.run(hypothesis, feed_dict={X: [2.5]}))\n",
    "print(sess.run(hypothesis, feed_dict={X: [1.5, 3.5]}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# cost minimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "- simplified hypothesis: ${H\\left(x\\right)=Wx}$\n",
    "\n",
    "- cost function: ${\\textrm{cost}\\left(W\\right)=\\frac{1}{m}\\Sigma_{i=1}^{m}\\left(H\\left(x^{i}\\right)-y^{i}\\right)^{2}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQwAAAEKCAYAAADn1WuOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fdd98041490>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "X = [1, 2, 3]\n",
    "Y = [1, 2, 3]\n",
    "\n",
    "W = tf.placeholder(tf.float32)\n",
    "\n",
    "hypothesis = X * W\n",
    "cost = tf.reduce_mean(tf.square(hypothesis - Y))\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# Variables for plotting cost function\n",
    "W_value    = []\n",
    "cost_value = []\n",
    "\n",
    "for i in range(-30, 50):\n",
    "\n",
    "    feed_W = i * 0.1\n",
    "    cost_current, W_current = sess.run(\n",
    "        [cost, W],\n",
    "        feed_dict = {W: feed_W}\n",
    "    )\n",
    "\n",
    "    W_value.append(W_current)\n",
    "    cost_value.append(cost_current)\n",
    "\n",
    "plt.xlabel(\"W\"); plt.ylabel(\"cost(W)\")\n",
    "plt.plot(W_value, cost_value)\n",
    "plt.axes().set_aspect(1 / plt.axes().get_data_ratio())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# multivariable linear regression\n",
    "\n",
    "- hypothesis defined without using a matrix: ${H\\left(x_{1},x_{2},x_{3}\\right)=x_{1}w_{1}+x_{2}w_{2}+x_{3}w_{3}}$\n",
    "\n",
    "test scores for general psychology:\n",
    "\n",
    "|**${x_{1}}$**|**${x_{2}}$**|**${x_{3}}$**|**${Y}$**|\n",
    "|-------------|-------------|-------------|---------|\n",
    "|73           |80           |75           |152      |\n",
    "|93           |88           |93           |185      |\n",
    "|89           |91           |90           |180      |\n",
    "|96           |98           |100          |196      |\n",
    "|73           |66           |70           |142      |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "step: 0, cost: 1221.08886719,\n",
      "prediction: [ 123.09977722  145.56219482  144.64117432  158.4463501   109.52912903]\n",
      "\n",
      "step: 500, cost: 5.31126022339,\n",
      "prediction: [ 153.60321045  183.01623535  181.1355896   198.21069336  138.27522278]\n",
      "\n",
      "step: 1000, cost: 4.28688764572,\n",
      "prediction: [ 153.2394104   183.26794434  181.02700806  198.10955811  138.62460327]\n",
      "\n",
      "step: 1500, cost: 3.50106287003,\n",
      "prediction: [ 152.92288208  183.48718262  180.93289185  198.01914978  138.9311676 ]\n",
      "\n",
      "step: 2000, cost: 2.89726567268,\n",
      "prediction: [ 152.6476593   183.67808533  180.85136414  197.93815613  139.20033264]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "x1_data = [ 73.,  93.,  89.,  96.,  73.]\n",
    "x2_data = [ 80.,  88.,  91.,  98.,  66.]\n",
    "x3_data = [ 75.,  93.,  90., 100.,  70.]\n",
    "\n",
    "y_data  = [152., 185., 180., 196., 142.]\n",
    "\n",
    "x1 = tf.placeholder(tf.float32)\n",
    "x2 = tf.placeholder(tf.float32)\n",
    "x3 = tf.placeholder(tf.float32)\n",
    "Y  = tf.placeholder(tf.float32)\n",
    "\n",
    "w1 = tf.Variable(tf.random_normal([1]), name = \"weight1\")\n",
    "w2 = tf.Variable(tf.random_normal([1]), name = \"weight2\")\n",
    "w3 = tf.Variable(tf.random_normal([1]), name = \"weight3\")\n",
    "b  = tf.Variable(tf.random_normal([1]), name = \"bias\"   )\n",
    "\n",
    "hypothesis = x1 * w1 + x2 * w2 + x3 * w3 + b\n",
    "cost       = tf.reduce_mean(tf.square(hypothesis - Y))\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate = 1e-5)\n",
    "train     = optimizer.minimize(cost)\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for step in range(2001):\n",
    "    cost_value, hy_value, _ = sess.run(\n",
    "        [cost, hypothesis, train],\n",
    "        feed_dict = {\n",
    "            x1: x1_data,\n",
    "            x2: x2_data,\n",
    "            x3: x3_data,\n",
    "            Y:  y_data\n",
    "        }\n",
    "    )\n",
    "\n",
    "    if step % 500 == 0:\n",
    "        print(\"\\nstep: {step}, cost: {cost},\\nprediction: {prediction}\".format(\n",
    "            step       = step,\n",
    "            cost       = cost_value,\n",
    "            prediction = hy_value\n",
    "        ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "- hypothesis defined using a matrix:\n",
    "\n",
    "$${\n",
    "\\begin{pmatrix}\n",
    "x_{1} & x_{2} & x_{3}\\\\\n",
    "\\end{pmatrix}\n",
    "\\cdot\n",
    "\\begin{pmatrix}\n",
    "w_{1} \\\\\n",
    "w_{2} \\\\\n",
    "w_{3} \\\\\n",
    "\\end{pmatrix}\n",
    "=\n",
    "\\left(x_{1}w_{1}+x_{2}w_{2}+x_{3}w_{3}\\right)\n",
    "}$$\n",
    "\n",
    "$${\n",
    "H\\left(X\\right)=XW\n",
    "}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "step: 0, cost: 65613.796875,\n",
      "prediction:\n",
      "[[-70.3801651 ]\n",
      " [-92.29891968]\n",
      " [-86.97892761]\n",
      " [-94.72986603]\n",
      " [-72.28895569]]\n",
      "\n",
      "step: 500, cost: 13.3241577148,\n",
      "prediction:\n",
      "[[ 156.13461304]\n",
      " [ 181.35145569]\n",
      " [ 181.92817688]\n",
      " [ 198.17074585]\n",
      " [ 136.72895813]]\n",
      "\n",
      "step: 1000, cost: 10.2774734497,\n",
      "prediction:\n",
      "[[ 155.48991394]\n",
      " [ 181.79486084]\n",
      " [ 181.73246765]\n",
      " [ 198.01477051]\n",
      " [ 137.32301331]]\n",
      "\n",
      "step: 1500, cost: 7.95214223862,\n",
      "prediction:\n",
      "[[ 154.92767334]\n",
      " [ 182.18174744]\n",
      " [ 181.56201172]\n",
      " [ 197.8772583 ]\n",
      " [ 137.84272766]]\n",
      "\n",
      "step: 2000, cost: 6.17698717117,\n",
      "prediction:\n",
      "[[ 154.43740845]\n",
      " [ 182.51925659]\n",
      " [ 181.41358948]\n",
      " [ 197.75588989]\n",
      " [ 138.29750061]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "x_data = [\n",
    "             [ 73.,  80.,  75.],\n",
    "             [ 93.,  88.,  93.],\n",
    "             [ 89.,  91.,  90.],\n",
    "             [ 96.,  98., 100.],\n",
    "             [ 73.,  66.,  70.]\n",
    "         ]\n",
    "y_data = [\n",
    "             [152.],\n",
    "             [185.],\n",
    "             [180.],\n",
    "             [196.],\n",
    "             [142.]\n",
    "         ]\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=[None, 3])\n",
    "Y = tf.placeholder(tf.float32, shape=[None, 1])\n",
    "\n",
    "W = tf.Variable(tf.random_normal([3, 1]), name = \"weight\")\n",
    "b = tf.Variable(tf.random_normal([1]),    name = \"bias\"  )\n",
    "\n",
    "hypothesis = tf.matmul(X, W) + b\n",
    "cost       = tf.reduce_mean(tf.square(hypothesis - Y))\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate = 1e-5)\n",
    "train     = optimizer.minimize(cost)\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for step in range(2001):\n",
    "    cost_value, hy_value, _ = sess.run(\n",
    "        [cost, hypothesis, train], feed_dict={X: x_data, Y: y_data})\n",
    "    if step % 500 == 0:\n",
    "        print(\"\\nstep: {step}, cost: {cost},\\nprediction:\\n{prediction}\".format(\n",
    "            step       = step,\n",
    "            cost       = cost_value,\n",
    "            prediction = hy_value\n",
    "        ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# reading data\n",
    "\n",
    "There are three main methods to get data into a TensorFlow program:\n",
    "\n",
    "- Feeding: Python code provides the data when running each step.\n",
    "- Reading from files: an input pipeline reads the data from files at the beginning of a graph.\n",
    "- Preloaded data: a constant or variable in the graph holds all the data (for small datasets).\n",
    "\n",
    "## feeding\n",
    "\n",
    "The TensorFlow feed mechanism enables injection of data into any tensor in a computation graph. Thus, a Python computation can feed data directly into the graph. While a tensor can be replaced with feed data, including variables and constants, good practice is to use a placeholder operation node. A `placeholder` exists solely to serve as the target of feeds. It is not initialized and contains no data and it generates an error if it is executed without a feed.\n",
    "\n",
    "## reading from files\n",
    "\n",
    "A typical pipeline for reading records from files has the following stages:\n",
    "\n",
    "- the list of filenames,\n",
    "- optional filename shuffling,\n",
    "- optional epoch limit,\n",
    "- filename queue,\n",
    "- a reader for the file format,\n",
    "- a decoder for a record read by the reader,\n",
    "- optional preprocessing and\n",
    "- an example queue.\n",
    "\n",
    "### file formats\n",
    "\n",
    "Select the reader that matches the input file format and pass the filename queue to the reader's read method. The read method outputs a key identifying the file and record and a scalar string value. One or more of the decoder and conversion operations are used to decode this string into the tensors that make up an example.\n",
    "\n",
    "For CSV files, the `TextLineReader` is available.\n",
    "\n",
    "A recommended format for TensorFlow is a `TFRecords file`, for which the `TFRecordReader` is available.\n",
    "\n",
    "### loading CSV data from file\n",
    "\n",
    "Consider data in an ASCII file of the following CSV form:\n",
    "\n",
    "```\n",
    "73,80,75,152\n",
    "93,88,93,185\n",
    "89,91,90,180\n",
    "96,98,100,196\n",
    "73,66,70,142\n",
    "```\n",
    "\n",
    "This can be loaded na√Øvely into volatile memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "step: 0, cost: 51609.5585938,\n",
      "prediction:\n",
      "[[-63.79487228]\n",
      " [-67.67445374]\n",
      " [-71.40155029]\n",
      " [-78.36078644]\n",
      " [-48.58982849]\n",
      " [-35.12105179]\n",
      " [-61.62991333]\n",
      " [-50.24700165]\n",
      " [-61.59102249]\n",
      " [-57.17690277]\n",
      " [-56.07018661]\n",
      " [-51.35820007]\n",
      " [-73.14998627]\n",
      " [-59.25958252]\n",
      " [-60.58699036]\n",
      " [-70.00971222]\n",
      " [-52.90825653]\n",
      " [-76.68890381]\n",
      " [-71.02648163]\n",
      " [-64.50036621]\n",
      " [-71.00350952]\n",
      " [-64.46847534]\n",
      " [-68.2816925 ]\n",
      " [-62.86103439]\n",
      " [-71.41112518]]\n",
      "\n",
      "step: 500, cost: 26.2186908722,\n",
      "prediction:\n",
      "[[ 150.07359314]\n",
      " [ 188.46737671]\n",
      " [ 181.42796326]\n",
      " [ 197.22511292]\n",
      " [ 146.33998108]\n",
      " [ 108.84999084]\n",
      " [ 145.26107788]\n",
      " [ 104.00246429]\n",
      " [ 177.86880493]\n",
      " [ 165.06048584]\n",
      " [ 142.74584961]\n",
      " [ 144.36811829]\n",
      " [ 187.79936218]\n",
      " [ 157.26612854]\n",
      " [ 147.14537048]\n",
      " [ 190.22279358]\n",
      " [ 152.73558044]\n",
      " [ 171.96018982]\n",
      " [ 177.85375977]\n",
      " [ 158.32931519]\n",
      " [ 171.39477539]\n",
      " [ 176.11552429]\n",
      " [ 162.83569336]\n",
      " [ 152.17649841]\n",
      " [ 194.08108521]]\n",
      "\n",
      "step: 1000, cost: 21.7858181,\n",
      "prediction:\n",
      "[[ 150.3780365 ]\n",
      " [ 188.01315308]\n",
      " [ 181.38641357]\n",
      " [ 197.38243103]\n",
      " [ 145.62249756]\n",
      " [ 108.53780365]\n",
      " [ 145.91044617]\n",
      " [ 105.25130463]\n",
      " [ 177.50161743]\n",
      " [ 165.09494019]\n",
      " [ 142.91879272]\n",
      " [ 144.25445557]\n",
      " [ 187.5333252 ]\n",
      " [ 156.69847107]\n",
      " [ 147.69764709]\n",
      " [ 190.03111267]\n",
      " [ 151.69384766]\n",
      " [ 173.05796814]\n",
      " [ 177.69413757]\n",
      " [ 158.31214905]\n",
      " [ 171.96643066]\n",
      " [ 175.94018555]\n",
      " [ 163.39233398]\n",
      " [ 151.97184753]\n",
      " [ 193.6428833 ]]\n",
      "\n",
      "step: 1500, cost: 18.3485603333,\n",
      "prediction:\n",
      "[[ 150.65576172]\n",
      " [ 187.60858154]\n",
      " [ 181.35401917]\n",
      " [ 197.52157593]\n",
      " [ 144.98640442]\n",
      " [ 108.25173187]\n",
      " [ 146.48138428]\n",
      " [ 106.34899139]\n",
      " [ 177.16543579]\n",
      " [ 165.10324097]\n",
      " [ 143.06945801]\n",
      " [ 144.14411926]\n",
      " [ 187.30708313]\n",
      " [ 156.20980835]\n",
      " [ 148.18023682]\n",
      " [ 189.85543823]\n",
      " [ 150.78691101]\n",
      " [ 174.02487183]\n",
      " [ 177.56436157]\n",
      " [ 158.30769348]\n",
      " [ 172.46786499]\n",
      " [ 175.77868652]\n",
      " [ 163.88313293]\n",
      " [ 151.81015015]\n",
      " [ 193.25665283]]\n",
      "\n",
      "step: 2000, cost: 15.6812143326,\n",
      "prediction:\n",
      "[[ 150.90811157]\n",
      " [ 187.24864197]\n",
      " [ 181.32884216]\n",
      " [ 197.64463806]\n",
      " [ 144.42271423]\n",
      " [ 107.99082947]\n",
      " [ 146.98353577]\n",
      " [ 107.31414032]\n",
      " [ 176.85906982]\n",
      " [ 165.09289551]\n",
      " [ 143.20083618]\n",
      " [ 144.03869629]\n",
      " [ 187.11430359]\n",
      " [ 155.78840637]\n",
      " [ 148.60227966]\n",
      " [ 189.69520569]\n",
      " [ 149.99650574]\n",
      " [ 174.87670898]\n",
      " [ 177.45872498]\n",
      " [ 158.31233215]\n",
      " [ 172.9079895 ]\n",
      " [ 175.63076782]\n",
      " [ 164.31593323]\n",
      " [ 151.68257141]\n",
      " [ 192.91625977]]\n",
      "\n",
      "predictions\n",
      "\n",
      "input data: [[100, 70, 101]],\n",
      "score prediction:\n",
      "[[ 197.1804657]]\n",
      "\n",
      "input data: [[60, 70, 110], [90, 100, 80]],\n",
      "score prediction:\n",
      "[[ 161.40119934]\n",
      " [ 176.64434814]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "xy = np.loadtxt(\n",
    "    \"data.csv\",\n",
    "    delimiter = \",\",\n",
    "    dtype     = np.float32\n",
    ")\n",
    "x_data = xy[:, 0:-1]\n",
    "y_data = xy[:, [-1]]\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=[None, 3])\n",
    "Y = tf.placeholder(tf.float32, shape=[None, 1])\n",
    "\n",
    "W = tf.Variable(tf.random_normal([3, 1]), name = \"weight\")\n",
    "b = tf.Variable(tf.random_normal([1]),    name = \"bias\"  )\n",
    "\n",
    "hypothesis = tf.matmul(X, W) + b\n",
    "cost       = tf.reduce_mean(tf.square(hypothesis - Y))\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=1e-5)\n",
    "train     = optimizer.minimize(cost)\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for step in range(2001):\n",
    "    cost_value, hy_value, _ = sess.run(\n",
    "        [cost, hypothesis, train],\n",
    "        feed_dict = {X: x_data, Y: y_data}\n",
    "    )\n",
    "    if step % 500 == 0:\n",
    "        print(\"\\nstep: {step}, cost: {cost},\\nprediction:\\n{prediction}\".format(\n",
    "            step       = step,\n",
    "            cost       = cost_value,\n",
    "            prediction = hy_value\n",
    "        ))\n",
    "\n",
    "print(\"\\npredictions\")\n",
    "test_x_data = [\n",
    "                  [100, 70, 101]\n",
    "              ]\n",
    "result = sess.run(hypothesis, feed_dict = {X: test_x_data})\n",
    "print(\"\\ninput data: {data},\\nscore prediction:\\n{prediction}\".format(\n",
    "    data       = test_x_data,\n",
    "    prediction = result\n",
    "))\n",
    "\n",
    "test_x_data = [\n",
    "                  [60, 70, 110],\n",
    "                  [90, 100, 80]\n",
    "              ]\n",
    "result = sess.run(hypothesis, feed_dict = {X: test_x_data})\n",
    "print(\"\\ninput data: {data},\\nscore prediction:\\n{prediction}\".format(\n",
    "    data       = test_x_data,\n",
    "    prediction = result\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# preprocessing\n",
    "\n",
    "Preprocessing could involve normalization of data, selecting a random slice, adding noise and distortions etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "# threading and queues\n",
    "\n",
    "Queues are a mechanism for asynchronous computation using TensorFlow. Like everything in TensorFlow, a queue is a node in a TensorFlow graph. It is a stateful node, like a variable: other nodes can modify its content. In particular, nodes can enqueue new items to the queue or dequeue existing items from the queue.\n",
    "\n",
    "Queues such as `FIFOQueue` and `RandomShuffleQueue` are important TensorFlow objects for computing tensors asynchronously in a graph. For example, a typical input architecture is to use a RandomShuffleQueue to prepare inputs for training a model:\n",
    "\n",
    "- Multiple threads prepare training examples and push them to the queue.\n",
    "- A training thread executes a training operation that dequeues mini-batches from the queue.\n",
    "\n",
    "The TensorFlow `Session` object is multithreaded, so multiple threads can use the same session and run operations in parallel. However, it is not always easy to implement a Python program that drives threads as described. All threads must be able to stop together, exceptions must be captured and reported and queues should be closed when stopping.\n",
    "\n",
    "TensorFlow provides two classes to help: `tf.Coordinator` and `tf.QueueRunner`. The `Coordinator` class helps multiple threads to stop together and report exceptions to a program that waits for them to stop. The `QueueRunner` class is used to create a number of threads cooperating to enqueue tensors in the same queue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "filename_queue = tf.train.string_input_producer(\n",
    "    [\"data.csv\"],\n",
    "    shuffle = False,\n",
    "    name    = \"filename_queue\")\n",
    "\n",
    "reader     = tf.TextLineReader()\n",
    "key, value = reader.read(filename_queue)\n",
    "\n",
    "# Set default values for empty columns and specify the decoded result type.\n",
    "xy = tf.decode_csv(\n",
    "    value,\n",
    "    record_defaults = [[0.], [0.], [0.], [0.]]\n",
    ")\n",
    "\n",
    "# Collect batches of CSV.\n",
    "train_x_batch, train_y_batch =\\\n",
    "    tf.train.batch(\n",
    "        [xy[0:-1], xy[-1:]],\n",
    "        batch_size = 10\n",
    "    )\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape = [None, 3])\n",
    "Y = tf.placeholder(tf.float32, shape = [None, 1])\n",
    "\n",
    "W = tf.Variable(tf.random_normal([3, 1]), name = \"weight\")\n",
    "b = tf.Variable(tf.random_normal([1]),    name = \"bias\")\n",
    "\n",
    "hypothesis = tf.matmul(X, W) + b\n",
    "cost       = tf.reduce_mean(tf.square(hypothesis - Y))\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=1e-5)\n",
    "train     = optimizer.minimize(cost)\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# Start populating the filename queue.\n",
    "coord   = tf.train.Coordinator()\n",
    "threads = tf.train.start_queue_runners(\n",
    "              sess  = sess,\n",
    "              coord = coord\n",
    "          )\n",
    "\n",
    "for step in range(2001):\n",
    "    x_batch, y_batch = sess.run([train_x_batch, train_y_batch])\n",
    "    cost_value, hy_value, _ = sess.run(\n",
    "        [cost, hypothesis, train],\n",
    "        feed_dict = {X: x_batch, Y: y_batch}\n",
    "    )\n",
    "    if step % 500 == 0:\n",
    "        print(\"\\nstep: {step}, cost: {cost},\\nprediction:\\n{prediction}\".format(\n",
    "            step       = step,\n",
    "            cost       = cost_value,\n",
    "            prediction = hy_value\n",
    "        ))\n",
    "\n",
    "coord.request_stop()\n",
    "coord.join(threads)\n",
    "\n",
    "print(\"\\npredictions\")\n",
    "test_x_data = [\n",
    "                  [100, 70, 101]\n",
    "              ]\n",
    "result = sess.run(hypothesis, feed_dict = {X: test_x_data})\n",
    "print(\"\\ninput data: {data},\\nscore prediction:\\n{prediction}\".format(\n",
    "    data       = test_x_data,\n",
    "    prediction = result\n",
    "))\n",
    "\n",
    "test_x_data = [\n",
    "                  [60, 70, 110],\n",
    "                  [90, 100, 80]\n",
    "              ]\n",
    "result = sess.run(hypothesis, feed_dict = {X: test_x_data})\n",
    "print(\"\\ninput data: {data},\\nscore prediction:\\n{prediction}\".format(\n",
    "    data       = test_x_data,\n",
    "    prediction = result\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "# logistic regression\n",
    "\n",
    "- hypothesis: ${H\\left(X\\right)=\\frac{1}{1+e^{-W^{T}X}}}$\n",
    "- ${\\textrm{cost}\\left(W\\right)=-\\frac{1}{m}\\Sigma y \\log\\left(H\\left(x\\right)\\right)+\\left(1-y\\right)\\left(\\log\\left(1-H\\left(x\\right)\\right)\\right)}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "step: 0, cost: 2.01184892654\n",
      "\n",
      "step: 1000, cost: 0.468353033066\n",
      "\n",
      "step: 2000, cost: 0.393124341965\n",
      "\n",
      "step: 3000, cost: 0.335385560989\n",
      "\n",
      "step: 4000, cost: 0.290337324142\n",
      "\n",
      "step: 5000, cost: 0.254872947931\n",
      "\n",
      "step: 6000, cost: 0.226582095027\n",
      "\n",
      "step: 7000, cost: 0.203673005104\n",
      "\n",
      "step: 8000, cost: 0.184841319919\n",
      "\n",
      "step: 9000, cost: 0.169140636921\n",
      "\n",
      "step: 10000, cost: 0.155879363418\n",
      "\n",
      "accuracy report:\n",
      "\n",
      "hypothesis:\n",
      "\n",
      "[[ 0.03347061]\n",
      " [ 0.16238944]\n",
      " [ 0.31767666]\n",
      " [ 0.7755906 ]\n",
      " [ 0.93586653]\n",
      " [ 0.97893244]]\n",
      "\n",
      "correct (Y):\n",
      "\n",
      "[[ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]]\n",
      "\n",
      "accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "x_data = [\n",
    "             [1, 2],\n",
    "             [2, 3],\n",
    "             [3, 1],\n",
    "             [4, 3],\n",
    "             [5, 3],\n",
    "             [6, 2]\n",
    "         ]\n",
    "y_data = [\n",
    "             [0],\n",
    "             [0],\n",
    "             [0],\n",
    "             [1],\n",
    "             [1],\n",
    "             [1]\n",
    "         ]\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape = [None, 2])\n",
    "Y = tf.placeholder(tf.float32, shape = [None, 1])\n",
    "\n",
    "W = tf.Variable(tf.random_normal([2, 1]), name = \"weight\")\n",
    "b = tf.Variable(tf.random_normal([1   ]), name = \"bias\"  )\n",
    "\n",
    "# hypothesis using sigmoid: tf.div(1., 1. + tf.exp(tf.matmul(X, W)))\n",
    "hypothesis = tf.sigmoid(tf.matmul(X, W) + b)\n",
    "cost       = -tf.reduce_mean(Y * tf.log(hypothesis) + (1 - Y) * tf.log(1 - hypothesis))\n",
    "\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate = 0.01).minimize(cost)\n",
    "\n",
    "# accuracy computation: true if hypothesis > 0.5 else false\n",
    "predicted = tf.cast(hypothesis > 0.5, dtype = tf.float32)\n",
    "accuracy  = tf.reduce_mean(tf.cast(tf.equal(predicted, Y), dtype = tf.float32))\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for step in range(10001):\n",
    "    cost_value, _ = sess.run(\n",
    "        [cost, train],\n",
    "        feed_dict = {X: x_data, Y: y_data}\n",
    "    )\n",
    "    if step % 1000 == 0:\n",
    "        print(\"\\nstep: {step}, cost: {cost}\".format(\n",
    "            step       = step,\n",
    "            cost       = cost_value\n",
    "        ))\n",
    "\n",
    "print(\"\\naccuracy report:\")\n",
    "h, c, a = sess.run(\n",
    "    [hypothesis, predicted, accuracy],\n",
    "    feed_dict = {X: x_data, Y: y_data}\n",
    ")\n",
    "print(\"\\nhypothesis:\\n\\n{hypothesis}\\n\\ncorrect (Y):\\n\\n{correct}\\n\\naccuracy: {accuracy}\".format(\n",
    "    hypothesis = h,\n",
    "    correct    = c,\n",
    "    accuracy   = a\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# example: multiple variables (for classification) with final column as classification\n",
    "\n",
    "The input is a CSV file with the first line of the file containing headers and the rest of the lines containing data. The rightmost column of data is the class (0 or 1) and the other columns are feature values of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features data shape: (2000, 20)\n",
      "class data shape:    (2000, 1)\n",
      "number of features:  20\n",
      "\n",
      "step: 0, cost: 1.09629881382\n",
      "step: 1000, cost: 0.728394389153\n",
      "step: 2000, cost: 0.656880319118\n",
      "step: 3000, cost: 0.618762850761\n",
      "step: 4000, cost: 0.599011480808\n",
      "step: 5000, cost: 0.588777184486\n",
      "\n",
      "accuracy report (testing trained system on training data):\n",
      "\n",
      "hypothesis:\n",
      "\n",
      "[[ 0.56674618]\n",
      " [ 0.91476625]\n",
      " [ 0.66587365]\n",
      " ..., \n",
      " [ 0.42087814]\n",
      " [ 0.14799729]\n",
      " [ 0.40271857]]\n",
      "\n",
      "correct (Y):\n",
      "\n",
      "[[ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " ..., \n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]]\n",
      "\n",
      "accuracy: 0.70450001955\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "xy = np.loadtxt(\n",
    "    \"output_preprocessed.csv\",\n",
    "    skiprows  = 1,\n",
    "    delimiter = \",\",\n",
    "    dtype     = np.float32\n",
    ")\n",
    "x_data = xy[:, 0:-1]\n",
    "y_data = xy[:, [-1]]\n",
    "\n",
    "number_of_features = x_data.shape[1]\n",
    "\n",
    "print(\"features data shape: \" + str(x_data.shape))\n",
    "print(\"class data shape:    \" + str(y_data.shape))\n",
    "print(\"number of features:  \" + str(number_of_features))\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=[None, number_of_features])\n",
    "Y = tf.placeholder(tf.float32, shape=[None, 1])\n",
    "\n",
    "W = tf.Variable(tf.random_normal([number_of_features, 1]), name = \"weight\")\n",
    "b = tf.Variable(tf.random_normal([1]                    ), name = \"bias\"  )\n",
    "\n",
    "hypothesis = tf.sigmoid(tf.matmul(X, W) + b)\n",
    "cost       = -tf.reduce_mean(Y * tf.log(hypothesis) + (1 - Y) * tf.log(1 - hypothesis))\n",
    "train      = tf.train.GradientDescentOptimizer(learning_rate = 0.01).minimize(cost)\n",
    "\n",
    "# accuracy computation: true if hypothesis > 0.5 else false\n",
    "predicted = tf.cast(hypothesis > 0.5, dtype = tf.float32)\n",
    "accuracy  = tf.reduce_mean(tf.cast(tf.equal(predicted, Y), dtype = tf.float32))\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "print(\"\")\n",
    "for step in range(5001):\n",
    "    cost_value, _ = sess.run(\n",
    "        [cost, train],\n",
    "        feed_dict = {X: x_data, Y: y_data})\n",
    "    if step % 1000 == 0:\n",
    "        print(\"step: {step}, cost: {cost}\".format(\n",
    "            step       = step,\n",
    "            cost       = cost_value\n",
    "        ))\n",
    "\n",
    "print(\"\\naccuracy report (testing trained system on training data):\")\n",
    "h, c, a = sess.run(\n",
    "    [hypothesis, predicted, accuracy],\n",
    "    feed_dict = {X: x_data, Y: y_data}\n",
    ")\n",
    "print(\"\\nhypothesis:\\n\\n{hypothesis}\\n\\ncorrect (Y):\\n\\n{correct}\\n\\naccuracy: {accuracy}\".format(\n",
    "    hypothesis = h,\n",
    "    correct    = c,\n",
    "    accuracy   = a\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# one-hot encoding\n",
    "\n",
    "Often features in data are not continuous values, but categorical. For example, a person could be classed as \"male\" or \"female\" or a nationality could be classed as \"French\", \"Swiss\" and \"Irish\". For the example of nationality, the values could be encoded as 0, 1 and 2. One-hot encoding effectively blows up the feature space to three features instead of the one feature of nationality. So, conceptually, the new features could be the booleans \"is French\", \"is Swiss\" and \"is Irish\".\n",
    "\n",
    "So, basically, one-hot encoding involves changing ordinal encoding (assigning a different number to each category) to binary encoding.\n",
    "\n",
    "A motivation for this is for many machine learning algorithms. Some algorithms, such as random forests, handly categorical values natively.\n",
    "\n",
    "example: A sample of creatures could contain humans, penguins, octopuses and aliens. Each type of creature could be labelled with an ordinal number (e.g. 1 for human, 2 for penguin and so on).\n",
    "\n",
    "|**sample**|**category**|**numerical**|\n",
    "|----------|------------|-------------|\n",
    "|1         |human       |1            |\n",
    "|2         |human       |1            |\n",
    "|3         |penguin     |2            |\n",
    "|4         |octopus     |3            |\n",
    "|5         |alien       |4            |\n",
    "|6         |octopus     |3            |\n",
    "|7         |alien       |4            |\n",
    "\n",
    "One-hot encoding this data involves generating ne boolean column for each category.\n",
    "\n",
    "|**sample**|**human**|**penguin**|**octopus**|**alien**|\n",
    "|----------|---------|-----------|-----------|---------|\n",
    "|1         |1        |0          |0          |0        |\n",
    "|2         |1        |0          |0          |0        |\n",
    "|3         |0        |1          |0          |0        |\n",
    "|4         |0        |0          |1          |0        |\n",
    "|5         |0        |0          |0          |1        |\n",
    "|6         |0        |0          |0          |0        |\n",
    "|7         |0        |0          |0          |1        |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "raw data:\n",
      "\n",
      "   A  B\n",
      "0  a  b\n",
      "1  b  a\n",
      "2  a  c\n",
      "\n",
      "data with column B encoded:\n",
      "\n",
      "   A  a  b  c\n",
      "0  a  0  1  0\n",
      "1  b  1  0  0\n",
      "2  a  0  0  1\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.DataFrame({\n",
    "                        \"A\": [\"a\", \"b\", \"a\"],\n",
    "                        \"B\": [\"b\", \"a\", \"c\"]\n",
    "                    })\n",
    "\n",
    "print(\"\\nraw data:\\n\")\n",
    "print(data)\n",
    "\n",
    "# Get one-hot encoding of column B.\n",
    "one_hot = pd.get_dummies(data[\"B\"])\n",
    "\n",
    "# Drop column B as it is now encoded.\n",
    "data = data.drop(\"B\", axis = 1)\n",
    "\n",
    "# Join the B encoding.\n",
    "data = data.join(one_hot)\n",
    "\n",
    "print(\"\\ndata with column B encoded:\\n\")\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "# softmax\n",
    "\n",
    "The softmax function or normalized exponential function is a generalization of the logistic function that 'squashes' a vector of arbitrary real values to the range (0, 1) that sum up to 1. Conceptually, this could mean changing scores to probabilities. Softmax regression models generalize logistic regression to classification problems in which the labels can have more than two possible values.\n",
    "\n",
    "So, the ${K}$-dimensional vector ${\\vec{z}}$ of arbitrary real values is changed to the ${K}$-dimensional vector ${\\sigma\\left(\\vec{z}\\right)}$ of real values in the range (0, 1) that sum up to 1:\n",
    "\n",
    "$${\n",
    "\\sigma\\left(\\vec{z}\\right)_{j}=\\frac{e^{z_{j}}}{\\sum_{k=1}^{K} e^{z_{k}}}\\textrm{ for }j=1,...,K\n",
    "}$$\n",
    "\n",
    "For example, softmax changes vector\n",
    "\n",
    "```Python\n",
    "[1, 2, 3, 4, 1, 2, 3]\n",
    "```\n",
    "\n",
    "to the following vector:\n",
    "\n",
    "```Python\n",
    "[0.024, 0.064, 0.175, 0.475, 0.024, 0.064, 0.175]\n",
    "```\n",
    "\n",
    "In NumPy, it could be implemented in the following way:\n",
    "\n",
    "```Python\n",
    "def softmax(z):\n",
    "    return np.exp(z) / np.sum(np.exp(z))\n",
    "```\n",
    "\n",
    "Changing from sigmoid activation to softmax activation could involve changing from\n",
    "\n",
    "```Python\n",
    "hypothesis = tf.sigmoid(tf.matmul(X, W) + b)\n",
    "```\n",
    "\n",
    "to the following:\n",
    "\n",
    "```Python\n",
    "hypothesis = tf.nn.softmax(tf.matmul(X, W) + b)\n",
    "```\n",
    "\n",
    "The cost function for softmax is cross-entropy.\n",
    "\n",
    "# cross-entropy\n",
    "\n",
    "Cross-entropy is used commonly to describe the difference between two probability distributions. Often, the \"true\" distribution (the one that the machine learning algorithm is trying to match) is expressed in terms of a one-hot distribution.\n",
    "\n",
    "Consider a specific training instance for which the class label is B, out of the possible class labels A, B and C. The one-hot distribution for this training instance is as follows:\n",
    "\n",
    "|**p(class A)**|**p(class B)**|**p(class C)**|\n",
    "|--------------|--------------|--------------|\n",
    "|0.0           |1.0           |0.0           |\n",
    "\n",
    "This \"true\" distribution can be interpreted to mean that the training instance has 0% probability of being class A, 100% probability of being class B and 0% probability of being class C.\n",
    "\n",
    "Suppose a machine learning algorithm predicts the following probability distribution:\n",
    "\n",
    "|**p(class A)**|**p(class B)**|**p(class C)**|\n",
    "|--------------|--------------|--------------|\n",
    "|0.228         |0.619         |0.153         |\n",
    "\n",
    "How close is the predicted distribution to the \"true\" distribution? This is what cross-entropy loss determines.\n",
    "\n",
    "$${\n",
    "H\\left(q,p\\right)=\\sum_{x}p\\left(x\\right)\\log q\\left(x\\right)\n",
    "}$$\n",
    "\n",
    "The sum is over the three classes A, B and C. The loss is calculated as 0.479. This is a measure of the error of the prediction.\n",
    "\n",
    "Cross-entropy is one of many possible loss functions (another popular one is SVM hinge loss). They are written typically as ${J\\left(\\theta\\right)}$ and can be used within gradient descent, which is an iterative framework of moving the parameters (or coefficients) towards the optimum values. In the following equation ${J\\left(\\theta\\right)}$ is replaced with ${H\\left(p, q\\right)}$. Note that the derivative of ${H\\left(p, q\\right)}$ with respect to the parameters must be computed.\n",
    "\n",
    "$${\n",
    "\\textrm{repeat until convergence: } \\theta_{j}\\leftarrow\\theta_{j}-\\alpha\\frac{\\partial}{\\partial\\theta_{j}}J\\left(\\theta\\right)\n",
    "}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# example: data classified with one-hot encoding\n",
    "\n",
    "In this example, the function `tf.argmax` returns the index with the largest value across axes of a tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 0, cost: 6.15176486969\n",
      "step: 200, cost: 0.693853735924\n",
      "step: 400, cost: 0.584099948406\n",
      "step: 600, cost: 0.49236613512\n",
      "step: 800, cost: 0.403734862804\n",
      "step: 1000, cost: 0.315236628056\n",
      "step: 1200, cost: 0.241405770183\n",
      "step: 1400, cost: 0.218246951699\n",
      "step: 1600, cost: 0.199564412236\n",
      "step: 1800, cost: 0.183692246675\n",
      "step: 2000, cost: 0.170054793358\n",
      "\n",
      "testing\n",
      "\n",
      "------------------------------\n",
      "result:\n",
      "\n",
      "[[  7.33173117e-02   9.26671445e-01   1.12322605e-05]]\n",
      "\n",
      "argmax: [1]\n",
      "------------------------------\n",
      "result:\n",
      "\n",
      "[[ 0.55566663  0.39402479  0.05030856]]\n",
      "\n",
      "argmax: [0]\n",
      "------------------------------\n",
      "result:\n",
      "\n",
      "[[  2.42859510e-08   4.52565990e-04   9.99547422e-01]]\n",
      "\n",
      "argmax: [2]\n",
      "------------------------------\n",
      "result:\n",
      "\n",
      "[[  7.33173117e-02   9.26671445e-01   1.12322605e-05]\n",
      " [  5.55666625e-01   3.94024789e-01   5.03085628e-02]\n",
      " [  2.42859528e-08   4.52565990e-04   9.99547422e-01]]\n",
      "\n",
      "argmax: [1 0 2]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "x_data = [\n",
    "             [1, 2, 1, 1],\n",
    "             [2, 1, 3, 2],\n",
    "             [3, 1, 3, 4],\n",
    "             [4, 1, 5, 5],\n",
    "             [1, 7, 5, 5],\n",
    "             [1, 2, 5, 6],\n",
    "             [1, 6, 6, 6],\n",
    "             [1, 7, 7, 7]\n",
    "         ]\n",
    "y_data = [\n",
    "             [0, 0, 1],\n",
    "             [0, 0, 1],\n",
    "             [0, 0, 1],\n",
    "             [0, 1, 0],\n",
    "             [0, 1, 0],\n",
    "             [0, 1, 0],\n",
    "             [1, 0, 0],\n",
    "             [1, 0, 0]\n",
    "         ]\n",
    "\n",
    "X = tf.placeholder(\"float\", [None, 4])\n",
    "Y = tf.placeholder(\"float\", [None, 3])\n",
    "number_of_classes = 3\n",
    "\n",
    "W = tf.Variable(tf.random_normal([4, number_of_classes]), name = \"weight\")\n",
    "b = tf.Variable(tf.random_normal([number_of_classes]   ), name = \"bias\"  )\n",
    "\n",
    "# tf.nn.softmax computes softmax activations\n",
    "# softmax = exp(logits) / reduce_sum(exp(logits), dim)\n",
    "hypothesis = tf.nn.softmax(tf.matmul(X, W) + b)\n",
    "cost       = tf.reduce_mean(-tf.reduce_sum(Y * tf.log(hypothesis), axis = 1))\n",
    "optimizer  = tf.train.GradientDescentOptimizer(learning_rate = 0.1).minimize(cost)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for step in range(2001):\n",
    "        sess.run(\n",
    "            optimizer,\n",
    "            feed_dict = {X: x_data, Y: y_data}\n",
    "        )\n",
    "        if step % 200 == 0:\n",
    "            print(\"step: {step}, cost: {cost}\".format(\n",
    "                step       = step,\n",
    "                cost       = sess.run(cost, feed_dict = {X: x_data, Y: y_data})\n",
    "            ))\n",
    "\n",
    "    print(\"\\ntesting\\n\")\n",
    "\n",
    "    print(\"------------------------------\")\n",
    "\n",
    "    result = sess.run(\n",
    "                 hypothesis,\n",
    "                 feed_dict = {\n",
    "                                 X: [[1, 11, 7, 9]]\n",
    "                             }\n",
    "             )\n",
    "    print(\"result:\\n\\n{result}\\n\\nargmax: {argmax}\".format(\n",
    "        result = result,\n",
    "        argmax = sess.run(tf.argmax(result, 1))\n",
    "    ))\n",
    "\n",
    "    print(\"------------------------------\")\n",
    "\n",
    "    result = sess.run(\n",
    "                 hypothesis,\n",
    "                 feed_dict = {\n",
    "                                 X: [[1, 3, 4, 3]]\n",
    "                             }\n",
    "             )\n",
    "    print(\"result:\\n\\n{result}\\n\\nargmax: {argmax}\".format(\n",
    "        result = result,\n",
    "        argmax = sess.run(tf.argmax(result, 1))\n",
    "    ))\n",
    "\n",
    "    print(\"------------------------------\")\n",
    "\n",
    "    result = sess.run(\n",
    "                 hypothesis,\n",
    "                 feed_dict = {\n",
    "                                 X: [[1, 1, 0, 1]]\n",
    "                             }\n",
    "             )\n",
    "    print(\"result:\\n\\n{result}\\n\\nargmax: {argmax}\".format(\n",
    "        result = result,\n",
    "        argmax = sess.run(tf.argmax(result, 1))\n",
    "    ))\n",
    "\n",
    "    print(\"------------------------------\")\n",
    "\n",
    "    result = sess.run(\n",
    "                 hypothesis,\n",
    "                 feed_dict = {\n",
    "                                 X: [\n",
    "                                        [1, 11,  7,  9],\n",
    "                                        [1,  3,  4,  3],\n",
    "                                        [1,  1,  0,  1]\n",
    "                                    ]\n",
    "                             }\n",
    "             )\n",
    "    print(\"result:\\n\\n{result}\\n\\nargmax: {argmax}\".format(\n",
    "        result = result,\n",
    "        argmax = sess.run(tf.argmax(result, 1))\n",
    "    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# training and test datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# example: learning rate\n",
    "\n",
    "The learning rate in the following example can be changed. An overly large learning rate means that there is overshooting of minima in the ${J\\left(w\\right)}$ versus ${w}$ graph while an overly small learning rate means that there are many iterations until convergence and there can be trapping in local minima."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "training\n",
      "\n",
      "step: 0,\n",
      "cost: 8.31644821167,\n",
      "W:\n",
      "[[ 0.22195008  0.38584882 -0.53195298]\n",
      " [ 1.21279347 -1.07557487 -0.14719149]\n",
      " [ 0.55527294 -0.24748784  1.24971259]]\n",
      "\n",
      "step: 50,\n",
      "cost: 1.05112886429,\n",
      "W:\n",
      "[[-0.20881306  0.00115683  0.28350213]\n",
      " [ 0.27693221 -0.26255345 -0.02435144]\n",
      " [ 0.49746183  0.57809454  0.48194173]]\n",
      "\n",
      "step: 100,\n",
      "cost: 0.781579613686,\n",
      "W:\n",
      "[[-0.46970949 -0.34654671  0.89210224]\n",
      " [ 0.14641295 -0.16828226  0.01189673]\n",
      " [ 0.73768389  0.6358996   0.18391465]]\n",
      "\n",
      "step: 150,\n",
      "cost: 0.671505689621,\n",
      "W:\n",
      "[[-0.69827598 -0.52357417  1.29769611]\n",
      " [ 0.12296849 -0.11011682 -0.02282415]\n",
      " [ 0.85604739  0.66157854  0.03987237]]\n",
      "\n",
      "step: 200,\n",
      "cost: 0.611969947815,\n",
      "W:\n",
      "[[-0.90268207 -0.61539346  1.59392083]\n",
      " [ 0.12702149 -0.07612278 -0.06087111]\n",
      " [ 0.93543333  0.67505574 -0.05299084]]\n",
      "\n",
      "testing\n",
      "\n",
      "predictions:\n",
      "\n",
      "[2 2 2]\n",
      "\n",
      "accuracy:\n",
      "\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# training dataset\n",
    "\n",
    "x_data = [\n",
    "             [1, 2, 1],\n",
    "             [1, 3, 2],\n",
    "             [1, 3, 4],\n",
    "             [1, 5, 5],\n",
    "             [1, 7, 5],\n",
    "             [1, 2, 5],\n",
    "             [1, 6, 6],\n",
    "             [1, 7, 7]\n",
    "         ]\n",
    "y_data = [\n",
    "             [0, 0, 1],\n",
    "             [0, 0, 1],\n",
    "             [0, 0, 1],\n",
    "             [0, 1, 0],\n",
    "             [0, 1, 0],\n",
    "             [0, 1, 0],\n",
    "             [1, 0, 0],\n",
    "             [1, 0, 0]\n",
    "         ]\n",
    "\n",
    "# test dataset\n",
    "\n",
    "x_test = [\n",
    "             [2, 1, 1],\n",
    "             [3, 1, 2],\n",
    "             [3, 3, 4]\n",
    "         ]\n",
    "y_test = [\n",
    "             [0, 0, 1],\n",
    "             [0, 0, 1],\n",
    "             [0, 0, 1]\n",
    "         ]\n",
    "\n",
    "X = tf.placeholder(\"float\", [None, 3])\n",
    "Y = tf.placeholder(\"float\", [None, 3])\n",
    "\n",
    "W = tf.Variable(tf.random_normal([3, 3]))\n",
    "b = tf.Variable(tf.random_normal([3]   ))\n",
    "\n",
    "hypothesis = tf.nn.softmax(tf.matmul(X, W) + b)\n",
    "cost       = tf.reduce_mean(-tf.reduce_sum(Y * tf.log(hypothesis), axis = 1))\n",
    "\n",
    "#learning_rate = 1.5\n",
    "#learning_rate = 1e-10\n",
    "learning_rate = 0.1\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate = learning_rate).minimize(cost)\n",
    "\n",
    "prediction = tf.argmax(hypothesis, 1)\n",
    "is_correct = tf.equal(prediction, tf.argmax(Y, 1))\n",
    "accuracy   = tf.reduce_mean(tf.cast(is_correct, tf.float32))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    print(\"\\ntraining\")\n",
    "\n",
    "    for step in range(201):\n",
    "        cost_value, W_value, _ = sess.run(\n",
    "            [cost, W, optimizer],\n",
    "            feed_dict = {X: x_data, Y: y_data}\n",
    "        )\n",
    "        if step % 50 == 0:\n",
    "            print(\"\\nstep: {step},\\ncost: {cost},\\nW:\\n{W}\".format(\n",
    "                step = step,\n",
    "                cost = cost_value,\n",
    "                W    = W_value\n",
    "            ))\n",
    "\n",
    "    print(\"\\ntesting\\n\")\n",
    "\n",
    "    result   = sess.run(prediction, feed_dict = {X: x_test}           )\n",
    "    accuracy = sess.run(accuracy,   feed_dict = {X: x_test, Y: y_test})\n",
    "\n",
    "    print(\"predictions:\\n\\n{result}\\n\\naccuracy:\\n\\n{accuracy}\".format(\n",
    "        result   = result,\n",
    "        accuracy = accuracy\n",
    "    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# training epoch/batch\n",
    "\n",
    "- **epoch**: one forward pass and one backward pass of all training examples\n",
    "- **batch size**: the number of training examples in one forward/backward pass -- The greater the batch size, the greater volatile memory is needed.\n",
    "- **iterations**: the number of passes, each pass using the batch size number of examples\n",
    "\n",
    "So, for 1000 training examples with a batch size of 500, it takes 2 iterations to complete 1 epoch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# MNIST database\n",
    "\n",
    "The MNIST (Mixed National Institute of Standards and Technology) dataset is a dataset of handwritten digits, comprising 60,000 training examples and 10,000 test examples.\n",
    "\n",
    "The database can be downloaded in the following way:\n",
    "\n",
    "```Python\n",
    "import tensorflow.examples.tutorials.mnist\n",
    "mnist = tensorflow.examples.tutorials.mnist.input_data.read_data_sets(\n",
    "    \"MNIST_data/\",\n",
    "    one_hot = True\n",
    ")\n",
    "```\n",
    "\n",
    "|**archive**               |**content**        |\n",
    "|--------------------------|-------------------|\n",
    "|t10k-images-idx3-ubyte.gz |training set images|\n",
    "|t10k-labels-idx1-ubyte.gz |training set labels|\n",
    "|train-images-idx3-ubyte.gz|test set images    |\n",
    "|train-labels-idx1-ubyte.gz|test set labels    |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# example: MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "\n",
      "training\n",
      "\n",
      "epoch: 1\tcost: 2.53797165879\n",
      "epoch: 2\tcost: 1.09456407238\n",
      "epoch: 3\tcost: 0.881119381298\n",
      "epoch: 4\tcost: 0.773099724352\n",
      "epoch: 5\tcost: 0.704426293644\n",
      "epoch: 6\tcost: 0.655394366384\n",
      "epoch: 7\tcost: 0.618686331863\n",
      "epoch: 8\tcost: 0.589267465553\n",
      "epoch: 9\tcost: 0.565370515449\n",
      "epoch: 10\tcost: 0.545365656289\n",
      "epoch: 11\tcost: 0.528833894621\n",
      "epoch: 12\tcost: 0.51371610208\n",
      "epoch: 13\tcost: 0.500891014392\n",
      "epoch: 14\tcost: 0.489158772555\n",
      "epoch: 15\tcost: 0.478504846543\n",
      "\n",
      "testing\n",
      "\n",
      "accuracy:\n",
      "\n",
      "0.888100028038\n",
      "\n",
      "label:\n",
      "[2]\n",
      "\n",
      "prediction:\n",
      "[2]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f742721ce10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import tensorflow.examples.tutorials.mnist\n",
    "\n",
    "mnist = tensorflow.examples.tutorials.mnist.input_data.read_data_sets(\n",
    "    \"MNIST_data/\",\n",
    "    one_hot = True\n",
    ")\n",
    "\n",
    "number_of_classes = 10\n",
    "\n",
    "# MNIST data image of shape 28 * 28 = 784\n",
    "X = tf.placeholder(tf.float32, [None, 784])\n",
    "# 10 classes (digits 0 to 9)\n",
    "Y = tf.placeholder(tf.float32, [None, number_of_classes])\n",
    "\n",
    "W = tf.Variable(tf.random_normal([784, number_of_classes]))\n",
    "b = tf.Variable(tf.random_normal([number_of_classes]))\n",
    "\n",
    "# hypothesis (using softmax)\n",
    "hypothesis = tf.nn.softmax(tf.matmul(X, W) + b)\n",
    "cost       = tf.reduce_mean(-tf.reduce_sum(Y * tf.log(hypothesis), axis = 1))\n",
    "optimizer  = tf.train.GradientDescentOptimizer(learning_rate = 0.1).minimize(cost)\n",
    "\n",
    "is_correct = tf.equal(tf.argmax(hypothesis, 1), tf.arg_max(Y, 1))\n",
    "accuracy   = tf.reduce_mean(tf.cast(is_correct, tf.float32))\n",
    "\n",
    "# parameters\n",
    "training_epochs = 15\n",
    "batch_size      = 100\n",
    "\n",
    "with tf.Session() as sess:\n",
    "\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    print(\"\\ntraining\\n\")\n",
    "\n",
    "    for epoch in range(training_epochs):\n",
    "\n",
    "        cost_mean   = 0\n",
    "        total_batch = int(mnist.train.num_examples / batch_size)\n",
    "\n",
    "        for i in range(total_batch):\n",
    "\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "\n",
    "            c, _ = sess.run(\n",
    "                       [cost, optimizer],\n",
    "                       feed_dict={X: batch_xs, Y: batch_ys}\n",
    "                   )\n",
    "            cost_mean += c / total_batch\n",
    "\n",
    "        print(\"epoch: {epoch}\\tcost: {cost}\".format(\n",
    "            epoch = epoch + 1,\n",
    "            cost  = cost_mean\n",
    "        ))\n",
    "\n",
    "    print(\"\\ntesting\")\n",
    "\n",
    "    accuracy = accuracy.eval(\n",
    "                   session   = sess,\n",
    "                   feed_dict = {\n",
    "                                   X: mnist.test.images,\n",
    "                                   Y: mnist.test.labels\n",
    "                               }\n",
    "               )\n",
    "\n",
    "    print(\"\\naccuracy:\\n\\n{accuracy}\".format(\n",
    "        accuracy = accuracy\n",
    "    ))\n",
    "\n",
    "    # select one test example and predict\n",
    "    r = random.randint(0, mnist.test.num_examples - 1)\n",
    "    print(\"\\nlabel:\")\n",
    "    print(sess.run(\n",
    "              tf.argmax(mnist.test.labels[r:r + 1], 1)\n",
    "          )\n",
    "    )\n",
    "    print(\"\\nprediction:\")\n",
    "    print(sess.run(\n",
    "              tf.argmax(hypothesis, 1),\n",
    "              feed_dict = {X: mnist.test.images[r:r + 1]}\n",
    "          )\n",
    "    )\n",
    "\n",
    "    plt.imshow(\n",
    "        mnist.test.images[r:r + 1].reshape(28, 28),\n",
    "        cmap          = \"Greys\",\n",
    "        interpolation = \"nearest\"\n",
    "    )\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# example: simple neural network for XOR truth table\n",
    "\n",
    "|**A**|**B**|**X**|\n",
    "|-----|-----|-----|\n",
    "|0    |0    |0    |\n",
    "|0    |1    |1    |\n",
    "|1    |0    |1    |\n",
    "|1    |1    |0    |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "step: 0\n",
      "cost: 0.753045797348\n",
      "W:\n",
      "[array([[-0.11067381,  0.06392822],\n",
      "       [-0.0321797 , -0.34502214]], dtype=float32), array([[-0.18104731],\n",
      "       [-2.40578866]], dtype=float32)]\n",
      "\n",
      "step: 2000\n",
      "cost: 0.561914384365\n",
      "W:\n",
      "[array([[-0.79464591,  3.22157192],\n",
      "       [ 0.62589425, -2.91539788]], dtype=float32), array([[-0.78444737],\n",
      "       [-3.19287276]], dtype=float32)]\n",
      "\n",
      "step: 4000\n",
      "cost: 0.142582535744\n",
      "W:\n",
      "[array([[-4.08664322,  5.49105358],\n",
      "       [ 4.42439032, -5.37737322]], dtype=float32), array([[-4.98574877],\n",
      "       [-5.03301287]], dtype=float32)]\n",
      "\n",
      "step: 6000\n",
      "cost: 0.0593350082636\n",
      "W:\n",
      "[array([[-5.27438784,  6.26706076],\n",
      "       [ 5.59442902, -6.11997843]], dtype=float32), array([[-6.67433167],\n",
      "       [-6.6047492 ]], dtype=float32)]\n",
      "\n",
      "step: 8000\n",
      "cost: 0.0363751091063\n",
      "W:\n",
      "[array([[-5.80319977,  6.65905046],\n",
      "       [ 6.11412334, -6.48613453]], dtype=float32), array([[-7.58772278],\n",
      "       [-7.52101755]], dtype=float32)]\n",
      "\n",
      "step: 10000\n",
      "cost: 0.0260161906481\n",
      "W:\n",
      "[array([[-6.12727022,  6.9115119 ],\n",
      "       [ 6.43365002, -6.72271347]], dtype=float32), array([[-8.21349049],\n",
      "       [-8.15323925]], dtype=float32)]\n",
      "\n",
      "accuracy report:\n",
      "\n",
      "hypothesis:\n",
      "\n",
      "[[ 0.02304083]\n",
      " [ 0.97056627]\n",
      " [ 0.9699561 ]\n",
      " [ 0.0201681 ]]\n",
      "\n",
      "correct (Y):\n",
      "\n",
      "[[ 0.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 0.]]\n",
      "\n",
      "accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "x_data = np.array([\n",
    "                      [0, 0],\n",
    "                      [0, 1],\n",
    "                      [1, 0],\n",
    "                      [1, 1]\n",
    "                  ],\n",
    "                  dtype=np.float32\n",
    ")\n",
    "y_data = np.array([\n",
    "                      [0],\n",
    "                      [1],\n",
    "                      [1],\n",
    "                      [0]\n",
    "                  ],\n",
    "                  dtype=np.float32\n",
    ")\n",
    "\n",
    "X          = tf.placeholder(tf.float32, [None, 2])\n",
    "Y          = tf.placeholder(tf.float32, [None, 1])\n",
    "\n",
    "W1         = tf.Variable(tf.random_normal([2, 2]), name = \"weight1\")\n",
    "b1         = tf.Variable(tf.random_normal([2]),    name = \"bias1\")\n",
    "layer1     = tf.sigmoid(tf.matmul(X, W1) + b1)\n",
    "\n",
    "W2         = tf.Variable(tf.random_normal([2, 1]), name = \"weight2\")\n",
    "b2         = tf.Variable(tf.random_normal([1]),    name = \"bias2\")\n",
    "hypothesis = tf.sigmoid(tf.matmul(layer1, W2) + b2)\n",
    "\n",
    "cost       = -tf.reduce_mean(Y * tf.log(hypothesis) + (1 - Y) * tf.log(1 - hypothesis))\n",
    "train      = tf.train.GradientDescentOptimizer(learning_rate = 0.1).minimize(cost)\n",
    "\n",
    "# accuracy computation: true if hypothesis > 0.5 else false\n",
    "predicted  = tf.cast(hypothesis > 0.5, dtype = tf.float32)\n",
    "accuracy   = tf.reduce_mean(tf.cast(tf.equal(predicted, Y), dtype = tf.float32))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for step in range(10001):\n",
    "\n",
    "        sess.run(\n",
    "            train,\n",
    "            feed_dict = {\n",
    "                            X: x_data,\n",
    "                            Y: y_data\n",
    "                        }\n",
    "        )\n",
    "        if step % 2000 == 0:\n",
    "            print(\"\\nstep: {step}\\ncost: {cost}\\nW:\\n{W}\".format(\n",
    "                step = step,\n",
    "                cost = sess.run(\n",
    "                           cost,\n",
    "                           feed_dict = {\n",
    "                                           X: x_data,\n",
    "                                           Y: y_data\n",
    "                                       }\n",
    "                       ),\n",
    "                W    = sess.run([W1, W2])\n",
    "            ))\n",
    "\n",
    "    print(\"\\naccuracy report:\")\n",
    "    h, c, a = sess.run(\n",
    "        [hypothesis, predicted, accuracy],\n",
    "        feed_dict = {\n",
    "                        X: x_data,\n",
    "                        Y: y_data\n",
    "                    }\n",
    "    )\n",
    "    print(\"\\nhypothesis:\\n\\n{hypothesis}\\n\\ncorrect (Y):\\n\\n{correct}\\n\\naccuracy: {accuracy}\".format(\n",
    "        hypothesis = h,\n",
    "        correct    = c,\n",
    "        accuracy   = a\n",
    "    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# deep learning\n",
    "\n",
    "Deep learning allows computational models that are composed of multiple processing layers to learn representations of data with multiple levels of abstraction. These methods have dramatically improced the state-of-the-art in speech recognition, visual object recognition, object detection and many other domains such as drug discovery and genomics. Deep learning discovers intricate structure in large data sets by using the backpropagation algorithm to indicate how a machine should change its internal parameters that are used to compute the representation in each layer from the representation in the previous layer. Deep convolutional networks have brought about breakthroughs in processing images, video, speech and sound, whereas recurrent networks have illuminated sequential data such as text and speech.\n",
    "\n",
    "# example: deep neural network for XOR truth table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "step: 0\n",
      "cost: 0.759968400002\n",
      "W:\n",
      "[array([[ 1.17050958,  0.23658907,  0.85170525,  1.88875902,  1.69035661,\n",
      "        -1.20168114,  1.58177483,  0.55276138, -1.39153302, -0.85973883],\n",
      "       [ 0.4605372 ,  0.36892304,  0.42037812,  0.13853477,  0.47574446,\n",
      "        -1.01397741, -2.16375875, -0.52245528,  1.84092593, -0.5710631 ]], dtype=float32), array([[ -6.51916325e-01,  -1.85874426e+00,   4.45647389e-02,\n",
      "         -6.67469561e-01,  -1.54609072e+00,  -9.40441936e-02,\n",
      "         -1.48086023e+00,  -4.72161453e-03,   1.60759103e+00,\n",
      "         -1.39276361e+00],\n",
      "       [ -2.79414564e-01,  -8.82146776e-01,  -1.51190117e-01,\n",
      "         -2.67493904e-01,  -1.13447070e+00,   8.15222740e-01,\n",
      "          7.18632489e-02,   3.05037677e-01,   1.10169828e+00,\n",
      "          9.25161242e-01],\n",
      "       [  4.75105822e-01,  -9.29442406e-01,   9.13582623e-01,\n",
      "         -9.78433847e-01,   4.50041622e-01,  -1.35249472e+00,\n",
      "          4.67909724e-02,  -7.56063581e-01,   9.32560444e-01,\n",
      "          1.57608414e+00],\n",
      "       [  4.74753797e-01,  -3.77498940e-02,   4.18261200e-01,\n",
      "          5.65235853e-01,  -6.51828825e-01,   9.84389305e-01,\n",
      "          1.74106228e+00,   5.27353168e-01,  -1.70698866e-01,\n",
      "          2.33003534e-02],\n",
      "       [ -5.46175502e-02,   6.22723758e-01,  -7.67561257e-01,\n",
      "         -1.13267875e+00,  -1.52870975e-02,   7.69220293e-01,\n",
      "         -2.44073451e-01,  -4.51155782e-01,   1.26717523e-01,\n",
      "          2.82709032e-01],\n",
      "       [ -3.83929610e-01,   2.12787056e+00,  -3.80378634e-01,\n",
      "         -9.56066668e-01,   1.35823059e+00,   2.19387698e+00,\n",
      "          2.98151612e-01,  -3.53446960e-01,   2.16026068e+00,\n",
      "          6.71151102e-01],\n",
      "       [ -8.90956461e-01,  -3.50749820e-01,   1.12407994e+00,\n",
      "         -1.24883640e+00,   7.10271478e-01,  -3.52103829e-01,\n",
      "          9.65147257e-01,   1.42521632e+00,   1.03253496e+00,\n",
      "         -7.08821177e-01],\n",
      "       [  4.75920975e-01,  -5.50910354e-01,   1.76522243e+00,\n",
      "          3.08009207e-01,  -1.27437925e+00,  -2.29747504e-01,\n",
      "          1.10236263e+00,   3.09314132e-01,  -2.15081310e+00,\n",
      "          1.16853654e+00],\n",
      "       [ -2.18560004e+00,  -4.12839986e-02,  -5.68666577e-01,\n",
      "         -2.66190171e-01,  -9.63186085e-01,  -1.21388638e+00,\n",
      "          3.70121777e-01,   6.09177828e-01,   1.01256013e+00,\n",
      "          8.73464286e-01],\n",
      "       [ -5.88184863e-04,   3.18837792e-01,   8.63053262e-01,\n",
      "         -8.58466148e-01,  -1.49794906e-01,  -7.59702444e-01,\n",
      "          1.29285264e+00,  -6.99897408e-01,  -2.02628350e+00,\n",
      "         -3.00212532e-01]], dtype=float32)]\n",
      "\n",
      "step: 2000\n",
      "cost: 0.01114413701\n",
      "W:\n",
      "[array([[ 1.98521769,  0.30403042,  1.32787514,  1.83283591,  1.78198922,\n",
      "        -1.62909162,  3.27980161,  0.49910524, -3.57817411, -0.55922776],\n",
      "       [ 1.75168157,  0.49920267,  1.1829505 ,  0.19544524,  0.2836411 ,\n",
      "        -1.51944184, -4.20622206, -0.2580229 ,  3.23162079, -0.0622626 ]], dtype=float32), array([[-0.37776458, -2.03713012,  0.04895737, -0.71010005, -1.55461216,\n",
      "        -0.12174718, -1.46594691,  0.13099535,  1.67368925, -1.55364335],\n",
      "       [ 0.16004103, -0.69712305, -0.35540289, -0.24590364, -1.0853914 ,\n",
      "         1.03830802,  0.05432311,  0.24380289,  0.85483462,  0.70639956],\n",
      "       [ 0.86299324, -0.89997804,  0.80172426, -0.98486114,  0.4814046 ,\n",
      "        -1.24941957,  0.04234253, -0.71301037,  0.84080839,  1.36699319],\n",
      "       [ 0.7938723 ,  0.19709581,  0.03023471,  0.62575471, -0.66244501,\n",
      "         1.2033397 ,  1.64727712,  0.26879051, -0.55915737, -0.14568838],\n",
      "       [ 0.25958532,  0.83390534, -1.15658045, -1.07560802, -0.0334263 ,\n",
      "         1.00842452, -0.33619571, -0.72956598, -0.27729893,  0.1320466 ],\n",
      "       [-0.18766119,  2.57755065, -0.61852098, -0.87745345,  1.44232202,\n",
      "         2.40515637,  0.25740507, -0.51312041,  1.85743368,  0.54215413],\n",
      "       [-1.89096189, -1.46995986,  2.0109973 , -1.43777013,  0.61770618,\n",
      "        -1.58152425,  1.04666483,  2.29597664,  2.55221367, -0.37994909],\n",
      "       [ 0.56462687, -0.42577374,  1.65682042,  0.33601317, -1.25187159,\n",
      "        -0.21807052,  1.06320179,  0.2772755 , -2.20399094,  1.0963366 ],\n",
      "       [-2.49139977, -1.28742731,  0.45501134, -0.56091487, -1.01522624,\n",
      "        -1.93296278,  0.62528908,  1.59042633,  2.26734567,  1.03851604],\n",
      "       [ 0.44379306,  0.54684651,  0.74534124, -0.83821028, -0.05119529,\n",
      "        -0.61031055,  1.28923488, -0.64227885, -2.1296463 , -0.545066  ]], dtype=float32)]\n",
      "\n",
      "step: 4000\n",
      "cost: 0.0030683837831\n",
      "W:\n",
      "[array([[ 2.06987309,  0.29238752,  1.36369765,  1.82479906,  1.78571844,\n",
      "        -1.69133949,  3.50241017,  0.47419772, -3.79909587, -0.55028951],\n",
      "       [ 1.84372735,  0.49716002,  1.22480059,  0.18024811,  0.2902675 ,\n",
      "        -1.57427073, -4.4084959 , -0.23187041,  3.41623235, -0.02647088]], dtype=float32), array([[-0.39700064, -2.03879881,  0.08418256, -0.7068131 , -1.55986774,\n",
      "        -0.14481136, -1.45166862,  0.15662274,  1.70703411, -1.58542883],\n",
      "       [ 0.19263497, -0.66029841, -0.36129141, -0.23194791, -1.08556306,\n",
      "         1.06441748,  0.06178622,  0.22310978,  0.83199215,  0.67137349],\n",
      "       [ 0.87444288, -0.87758064,  0.81307673, -0.97366869,  0.47881708,\n",
      "        -1.24434745,  0.053905  , -0.7157529 ,  0.84165907,  1.32883716],\n",
      "       [ 0.80360782,  0.24630025,  0.0083555 ,  0.64766264, -0.66640598,\n",
      "         1.21596515,  1.64982641,  0.23593713, -0.58059281, -0.17668022],\n",
      "       [ 0.26878566,  0.88565165, -1.18099034, -1.05411816, -0.03742926,\n",
      "         1.02272832, -0.33554584, -0.76354975, -0.30108178,  0.10540831],\n",
      "       [-0.14117245,  2.60630083, -0.65078712, -0.86601096,  1.44678676,\n",
      "         2.44401503,  0.2557987 , -0.55342007,  1.8127594 ,  0.52815962],\n",
      "       [-2.07264161, -1.63389993,  2.1099813 , -1.4678396 ,  0.60466039,\n",
      "        -1.76514411,  1.06457162,  2.41910958,  2.7342844 , -0.36114782],\n",
      "       [ 0.57098752, -0.4100121 ,  1.64828825,  0.34572282, -1.25294447,\n",
      "        -0.21411738,  1.06619751,  0.26270938, -2.21183562,  1.07881045],\n",
      "       [-2.57746816, -1.42833269,  0.59718454, -0.61157376, -1.01852977,\n",
      "        -2.03334403,  0.65144879,  1.74180627,  2.4123714 ,  1.04945874],\n",
      "       [ 0.48612845,  0.58069319,  0.73849308, -0.82186681, -0.05011338,\n",
      "        -0.58125377,  1.30030692, -0.66793817, -2.15542126, -0.5908556 ]], dtype=float32)]\n",
      "\n",
      "step: 6000\n",
      "cost: 0.0016878277529\n",
      "W:\n",
      "[array([[ 2.10473371,  0.28726593,  1.37793338,  1.82165623,  1.78805029,\n",
      "        -1.71745026,  3.59396052,  0.46256995, -3.88882947, -0.54746324],\n",
      "       [ 1.88168252,  0.49646902,  1.24172914,  0.17239496,  0.29395255,\n",
      "        -1.59743035, -4.49084234, -0.22185875,  3.49313998, -0.01221459]], dtype=float32), array([[ -4.07575130e-01,  -2.03941464e+00,   9.91711393e-02,\n",
      "         -7.05823600e-01,  -1.56206536e+00,  -1.54784217e-01,\n",
      "         -1.44642031e+00,   1.68780372e-01,   1.72188318e+00,\n",
      "         -1.59771848e+00],\n",
      "       [  2.05933303e-01,  -6.45399094e-01,  -3.64031285e-01,\n",
      "         -2.25773960e-01,  -1.08580148e+00,   1.07549596e+00,\n",
      "          6.45613447e-02,   2.14635387e-01,   8.22528005e-01,\n",
      "          6.56991899e-01],\n",
      "       [  8.78056586e-01,  -8.68465662e-01,   8.17599535e-01,\n",
      "         -9.68873143e-01,   4.77587700e-01,  -1.24214292e+00,\n",
      "          5.81872724e-02,  -7.16281474e-01,   8.42290878e-01,\n",
      "          1.31340635e+00],\n",
      "       [  8.06615829e-01,   2.67394692e-01,  -1.53666304e-03,\n",
      "          6.57032430e-01,  -6.68074131e-01,   1.22133529e+00,\n",
      "          1.65075815e+00,   2.22695380e-01,  -5.89089751e-01,\n",
      "         -1.88969329e-01],\n",
      "       [  2.71311820e-01,   9.08151031e-01,  -1.19179749e+00,\n",
      "         -1.04500628e+00,  -3.90701629e-02,   1.02868819e+00,\n",
      "         -3.35373372e-01,  -7.77113378e-01,  -3.10328305e-01,\n",
      "          9.51324105e-02],\n",
      "       [ -1.19598188e-01,   2.61682987e+00,  -6.64776862e-01,\n",
      "         -8.60525846e-01,   1.44833672e+00,   2.46067929e+00,\n",
      "          2.55380124e-01,  -5.71289897e-01,   1.79319930e+00,\n",
      "          5.21377981e-01],\n",
      "       [ -2.15024447e+00,  -1.70363724e+00,   2.15080476e+00,\n",
      "         -1.48187828e+00,   5.99478722e-01,  -1.84190834e+00,\n",
      "          1.07147944e+00,   2.47179389e+00,   2.80945778e+00,\n",
      "         -3.53414804e-01],\n",
      "       [  5.73949397e-01,  -4.03727412e-01,   1.64436710e+00,\n",
      "          3.49937856e-01,  -1.25347781e+00,  -2.12074280e-01,\n",
      "          1.06739116e+00,   2.56669670e-01,  -2.21538568e+00,\n",
      "          1.07144809e+00],\n",
      "       [ -2.61572957e+00,  -1.48817861e+00,   6.58707917e-01,\n",
      "         -6.34265363e-01,  -1.01972091e+00,  -2.07559967e+00,\n",
      "          6.61088467e-01,   1.80674827e+00,   2.47317290e+00,\n",
      "          1.05452681e+00],\n",
      "       [  5.04486144e-01,   5.93457520e-01,   7.35035777e-01,\n",
      "         -8.14387262e-01,  -5.00600114e-02,  -5.68593085e-01,\n",
      "          1.30454707e+00,  -6.78962886e-01,  -2.16681433e+00,\n",
      "         -6.10334992e-01]], dtype=float32)]\n",
      "\n",
      "step: 8000\n",
      "cost: 0.00114469625987\n",
      "W:\n",
      "[array([[  2.12623739e+00,   2.84024686e-01,   1.38657284e+00,\n",
      "          1.81982517e+00,   1.78982985e+00,  -1.73367596e+00,\n",
      "          3.65022731e+00,   4.55022335e-01,  -3.94385529e+00,\n",
      "         -5.45952678e-01],\n",
      "       [  1.90512550e+00,   4.96101886e-01,   1.25208580e+00,\n",
      "          1.67051837e-01,   2.96596855e-01,  -1.61188042e+00,\n",
      "         -4.54140806e+00,  -2.15951473e-01,   3.54071832e+00,\n",
      "         -3.55720916e-03]], dtype=float32), array([[-0.41485974, -2.03972101,  0.10844814, -0.70537728, -1.56342804,\n",
      "        -0.16105978, -1.44348049,  0.17666337,  1.7313664 , -1.60486555],\n",
      "       [ 0.21408351, -0.63623756, -0.36589482, -0.22188419, -1.0860399 ,\n",
      "         1.08240271,  0.06612433,  0.20938629,  0.81671464,  0.64830929],\n",
      "       [ 0.8799566 , -0.86279756,  0.82024044, -0.96591753,  0.47677895,\n",
      "        -1.2407558 ,  0.06059803, -0.71648324,  0.84278399,  1.30417931],\n",
      "       [ 0.80823183,  0.28070003, -0.00799221,  0.66287696, -0.66910481,\n",
      "         1.22473145,  1.65125227,  0.21452603, -0.59425688, -0.19625649],\n",
      "       [ 0.27255338,  0.92243528, -1.19877148, -1.03934073, -0.04007108,\n",
      "         1.03243196, -0.33535069, -0.78543591, -0.31590301,  0.08913336],\n",
      "       [-0.10564253,  2.62293386, -0.67359018, -0.85691917,  1.44917452,\n",
      "         2.47107816,  0.25521576, -0.58262432,  1.78087652,  0.5169456 ],\n",
      "       [-2.19883466, -1.74721992,  2.17607474, -1.49105465,  0.59636658,\n",
      "        -1.88955843,  1.0755527 ,  2.50485039,  2.85584927, -0.3486807 ],\n",
      "       [ 0.57587218, -0.39989311,  1.64181197,  0.3525753 , -1.25382674,\n",
      "        -0.21070856,  1.06808043,  0.25292507, -2.2176187 ,  1.06695259],\n",
      "       [-2.64017153, -1.52556813,  0.69749629, -0.64881825, -1.02037895,\n",
      "        -2.10195494,  0.66661352,  1.84752786,  2.51097727,  1.05781639],\n",
      "       [ 0.51601458,  0.60104728,  0.73264384, -0.80961299, -0.05015899,\n",
      "        -0.56064969,  1.30700314, -0.68589544, -2.17397404, -0.62229699]], dtype=float32)]\n",
      "\n",
      "step: 10000\n",
      "cost: 0.000859200255945\n",
      "W:\n",
      "[array([[  2.14161205e+00,   2.81671345e-01,   1.39269090e+00,\n",
      "          1.81856072e+00,   1.79128873e+00,  -1.74533463e+00,\n",
      "          3.69033599e+00,   4.49463367e-01,  -3.98305607e+00,\n",
      "         -5.44974983e-01],\n",
      "       [  1.92190897e+00,   4.95870918e-01,   1.25945365e+00,\n",
      "          1.62989348e-01,   2.98669934e-01,  -1.62229407e+00,\n",
      "         -4.57746744e+00,  -2.11858407e-01,   3.57478285e+00,\n",
      "          2.57168524e-03]], dtype=float32), array([[-0.42042193, -2.03972101,  0.11507976, -0.7051689 , -1.56441677,\n",
      "        -0.1656107 , -1.44154274,  0.18246652,  1.73829794, -1.60973501],\n",
      "       [ 0.21987981, -0.62970501, -0.36732703, -0.21908149, -1.08627832,\n",
      "         1.0873667 ,  0.06715739,  0.20561644,  0.81257725,  0.64223248],\n",
      "       [ 0.8811577 , -0.85872781,  0.82204235, -0.96382767,  0.47618094,\n",
      "        -1.23975348,  0.06219234, -0.71660244,  0.84319365,  1.29777193],\n",
      "       [ 0.80928737,  0.29034486, -0.01279016,  0.66707408, -0.66985017,\n",
      "         1.227198  ,  1.65149069,  0.20866935, -0.59792155, -0.20129602],\n",
      "       [ 0.27330357,  0.93283755, -1.20391166, -1.03528225, -0.04077977,\n",
      "         1.03514326, -0.33539131, -0.79138011, -0.3198368 ,  0.08503048],\n",
      "       [-0.09537798,  2.62712526, -0.6799795 , -0.85424173,  1.4497155 ,\n",
      "         2.47857213,  0.25514543, -0.59086585,  1.77195966,  0.51368034],\n",
      "       [-2.23389387, -1.77863932,  2.19421172, -1.49785972,  0.59418905,\n",
      "        -1.92376614,  1.07836437,  2.52874732,  2.88902521, -0.34531906],\n",
      "       [ 0.57728857, -0.39717501,  1.63991857,  0.35446966, -1.25406516,\n",
      "        -0.20968813,  1.06855726,  0.25023746, -2.21923661,  1.06378698],\n",
      "       [-2.65804815, -1.55252087,  0.72562265, -0.65949064, -1.02085578,\n",
      "        -2.12094569,  0.67034841,  1.87701845,  2.53815055,  1.06023812],\n",
      "       [ 0.52433282,  0.60633808,  0.73079741, -0.80614823, -0.05029546,\n",
      "        -0.55492693,  1.30866098, -0.69091398, -2.17912292, -0.63076282]], dtype=float32)]\n",
      "\n",
      "accuracy report:\n",
      "\n",
      "hypothesis:\n",
      "\n",
      "[[  8.59555963e-04]\n",
      " [  9.99214053e-01]\n",
      " [  9.99189436e-01]\n",
      " [  9.79224336e-04]]\n",
      "\n",
      "correct (Y):\n",
      "\n",
      "[[ 0.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 0.]]\n",
      "\n",
      "accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "x_data = np.array([\n",
    "                      [0, 0],\n",
    "                      [0, 1],\n",
    "                      [1, 0],\n",
    "                      [1, 1]\n",
    "                  ],\n",
    "                  dtype = np.float32\n",
    ")\n",
    "y_data = np.array([\n",
    "                      [0],\n",
    "                      [1],\n",
    "                      [1],\n",
    "                      [0]\n",
    "                  ],\n",
    "                  dtype = np.float32\n",
    ")\n",
    "\n",
    "X          = tf.placeholder(tf.float32, [None, 2])\n",
    "Y          = tf.placeholder(tf.float32, [None, 1])\n",
    "\n",
    "W1         = tf.Variable(tf.random_normal([2, 10]), name = \"weight1\")\n",
    "b1         = tf.Variable(tf.random_normal([10]),    name = \"bias1\")\n",
    "layer1     = tf.sigmoid(tf.matmul(X, W1) + b1)\n",
    "\n",
    "W2         = tf.Variable(tf.random_normal([10, 10]), name = \"weight2\")\n",
    "b2         = tf.Variable(tf.random_normal([10]),     name = \"bias2\")\n",
    "layer2     = tf.sigmoid(tf.matmul(layer1, W2) + b2)\n",
    "\n",
    "W3         = tf.Variable(tf.random_normal([10, 10]), name = \"weight3\")\n",
    "b3         = tf.Variable(tf.random_normal([10]),     name = \"bias3\")\n",
    "layer3     = tf.sigmoid(tf.matmul(layer2, W3) + b3)\n",
    "\n",
    "W4         = tf.Variable(tf.random_normal([10, 1]),  name = \"weight4\")\n",
    "b4         = tf.Variable(tf.random_normal([1]),      name = \"bias4\")\n",
    "hypothesis = tf.sigmoid(tf.matmul(layer3, W4) + b4)\n",
    "\n",
    "cost       = -tf.reduce_mean(Y * tf.log(hypothesis) + (1 - Y) * tf.log(1 - hypothesis))\n",
    "train      = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n",
    "\n",
    "# accuracy computation: true if hypothesis > 0.5 else false\n",
    "predicted  = tf.cast(hypothesis > 0.5, dtype = tf.float32)\n",
    "accuracy   = tf.reduce_mean(tf.cast(tf.equal(predicted, Y), dtype = tf.float32))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for step in range(10001):\n",
    "\n",
    "        sess.run(\n",
    "            train,\n",
    "            feed_dict = {\n",
    "                            X: x_data,\n",
    "                            Y: y_data\n",
    "                        }\n",
    "        )\n",
    "        if step % 2000 == 0:\n",
    "            print(\"\\nstep: {step}\\ncost: {cost}\\nW:\\n{W}\".format(\n",
    "                step = step,\n",
    "                cost = sess.run(\n",
    "                           cost,\n",
    "                           feed_dict = {\n",
    "                                           X: x_data,\n",
    "                                           Y: y_data\n",
    "                                       }\n",
    "                       ),\n",
    "                W    = sess.run([W1, W2])\n",
    "            ))\n",
    "\n",
    "    print(\"\\naccuracy report:\")\n",
    "    h, c, a = sess.run(\n",
    "        [hypothesis, predicted, accuracy],\n",
    "        feed_dict = {\n",
    "                        X: x_data,\n",
    "                        Y: y_data\n",
    "                    }\n",
    "    )\n",
    "    print(\"\\nhypothesis:\\n\\n{hypothesis}\\n\\ncorrect (Y):\\n\\n{correct}\\n\\naccuracy: {accuracy}\".format(\n",
    "        hypothesis = h,\n",
    "        correct    = c,\n",
    "        accuracy   = a\n",
    "    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# simple neural network versus deep neural network\n",
    "\n",
    "Both the simple and the deep neural networks are successful at modelling the XOR truth table, however, the deep neural network has a far greater accuracy:\n",
    "\n",
    "|**A**|**B**|**X**|**simple NN**|**deep NN**   |**factor of difference in error**|\n",
    "|-----|-----|-----|-------------|--------------|---------------------------------|\n",
    "|0    |0    |0    |0.02304083   |8.59555963e-04|26.8                             |\n",
    "|0    |1    |1    |0.97056627   |9.99214053e-01|37.5                             |\n",
    "|1    |0    |1    |0.9699561    |9.99189436e-01|37.1                             |\n",
    "|1    |1    |0    |0.0201681    |9.79224336e-04|20.6                             |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# TensorBoard\n",
    "\n",
    "Tensors can be logged and viewed in TensorBoard. Images are simply displayed, histograms are multidimensional tensors and scalar tensors are shown as graphs.\n",
    "\n",
    "1. Set the tensors to log.\n",
    "\n",
    "```Python\n",
    "with tf.variable_scope(\"layer1\") as scope:\n",
    "    tf.summary.image(\"input\", x_image, 3)\n",
    "    tf.summary.histogram(\"layer\", L1)\n",
    "    tf.summary.scalar(\"loss\", cost)\n",
    "```\n",
    "\n",
    "2. Merge the summaries.\n",
    "\n",
    "```Python\n",
    "summary = tf.summary.merge_all()\n",
    "```\n",
    "\n",
    "3. Create a summary writer and add the TensorFlow graph.\n",
    "\n",
    "```Python\n",
    "writer = tf.summary.FileWriter(TB_SUMMARY_DIR)\n",
    "writer.add_graph(sess.graph)\n",
    "```\n",
    "\n",
    "4. Run summary merge and add the summary to the writer.\n",
    "\n",
    "```Python\n",
    "s, _ = sess.run([summary, optimizer], feed_dict = feed_dict)\n",
    "writer.add_summary(s, global_step = global_step)\n",
    "```\n",
    "\n",
    "5. Launch TensorBoard.\n",
    "\n",
    "```Bash\n",
    "tensorboard --logdir=/tmp/mnist_logs\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# example: TensorBoard\n",
    "\n",
    "Clear logs and launch TensorBoard:\n",
    "\n",
    "```Bash\n",
    "rm -rf /tmp/mnist\n",
    "tensorboard --logdir=/tmp/mnist\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.examples.tutorials.mnist\n",
    "\n",
    "# reset everything to rerun in jupyter\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# configuration\n",
    "batch_size      = 100\n",
    "learning_rate   = 0.5\n",
    "training_epochs = 500\n",
    "logs_path       = \"/tmp/mnist/1\"\n",
    "\n",
    "# load mnist data set\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = tensorflow.examples.tutorials.mnist.input_data.read_data_sets(\n",
    "    \"MNIST_data/\",\n",
    "    one_hot = True\n",
    ")\n",
    "\n",
    "with tf.name_scope(\"input\"):\n",
    "    # None => batch size can be any size; 784 => flattened image\n",
    "    x  = tf.placeholder(tf.float32, shape = [None, 784], name = \"x-input\") \n",
    "    # target 10 output classes\n",
    "    y_ = tf.placeholder(tf.float32, shape = [None, 10],  name = \"y-input\")\n",
    "\n",
    "with tf.name_scope(\"weights\"):\n",
    "    W = tf.Variable(tf.zeros([784, 10]))\n",
    "\n",
    "with tf.name_scope(\"biases\"):\n",
    "    b = tf.Variable(tf.zeros([10]))\n",
    "\n",
    "with tf.name_scope(\"softmax\"):\n",
    "    y = tf.nn.softmax(tf.matmul(x, W) + b)\n",
    "\n",
    "with tf.name_scope(\"cross-entropy\"):\n",
    "    cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y), axis = 1))\n",
    "\n",
    "# specify optimizer\n",
    "with tf.name_scope(\"train\"):\n",
    "    # optimizer is an \"operation\" which we can execute in a session\n",
    "    train_op = tf.train.GradientDescentOptimizer(learning_rate).minimize(cross_entropy)\n",
    "\n",
    "with tf.name_scope(\"accuracy\"):\n",
    "    correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))\n",
    "    accuracy           = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "tf.summary.scalar(\"cost\", cross_entropy)\n",
    "tf.summary.scalar(\"accuracy\", accuracy)\n",
    "tf.summary.scalar(\"input\", x)\n",
    "\n",
    "summary_operation = tf.summary.merge_all()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    writer = tf.summary.FileWriter(logs_path)\n",
    "\n",
    "    # perform training cycles\n",
    "    for epoch in range(training_epochs):\n",
    "        \n",
    "        # number of batches in one epoch\n",
    "        batch_count = int(mnist.train.num_examples / batch_size)\n",
    "        \n",
    "        for i in range(batch_count):\n",
    "            batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "\n",
    "            _, summary = sess.run(\n",
    "                [train_op, summary_operation],\n",
    "                feed_dict = {x: batch_x, y_: batch_y}\n",
    "            )\n",
    "\n",
    "            writer.add_summary(summary, epoch * batch_count + i)\n",
    "            \n",
    "        if epoch % 100 == 0: \n",
    "            print(\"epoch: {epoch}\".format(epoch = epoch))\n",
    "\n",
    "    print(\"accuracy: {accuracy}\".format(\n",
    "        accuracy = accuracy.eval(feed_dict = {\n",
    "                                                 x:  mnist.test.images,\n",
    "                                                 y_: mnist.test.labels\n",
    "                                             }\n",
    "                                )\n",
    "    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "# example: softmax classifier for MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "epoch: 0, cost: 5.5008147295\n",
      "epoch: 1, cost: 1.82199460918\n",
      "epoch: 2, cost: 1.2013834101\n",
      "epoch: 3, cost: 0.94063898856\n",
      "epoch: 4, cost: 0.794875945937\n",
      "epoch: 5, cost: 0.700277985768\n",
      "epoch: 6, cost: 0.633509654484\n",
      "epoch: 7, cost: 0.583592939865\n",
      "epoch: 8, cost: 0.543543923416\n",
      "epoch: 9, cost: 0.512178804576\n",
      "epoch: 10, cost: 0.486511237662\n",
      "epoch: 11, cost: 0.464348079616\n",
      "epoch: 12, cost: 0.445898542932\n",
      "epoch: 13, cost: 0.430081928332\n",
      "epoch: 14, cost: 0.416244304167\n",
      "accuracy: 0.897599995136\n",
      "\n",
      "label:\n",
      "[5]\n",
      "\n",
      "prediction:\n",
      "[5]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fd2be885b50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot = True)\n",
    "\n",
    "# parameters\n",
    "learning_rate   = 0.001\n",
    "training_epochs = 15\n",
    "batch_size      = 100\n",
    "\n",
    "# input place holders\n",
    "X = tf.placeholder(tf.float32, [None, 784])\n",
    "Y = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "# weights and bias for layers\n",
    "W = tf.Variable(tf.random_normal([784, 10]))\n",
    "b = tf.Variable(tf.random_normal([10]))\n",
    "\n",
    "hypothesis = tf.matmul(X, W) + b\n",
    "\n",
    "# define cost/loss and optimizer\n",
    "cost      = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = hypothesis, labels = Y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(cost)\n",
    "\n",
    "# initialize\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for epoch in range(training_epochs):\n",
    "\n",
    "    avg_cost = 0\n",
    "    total_batch = int(mnist.train.num_examples / batch_size)\n",
    "\n",
    "    for i in range(total_batch):\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "        feed_dict = {X: batch_xs, Y: batch_ys}\n",
    "        c, _ = sess.run([cost, optimizer], feed_dict = feed_dict)\n",
    "        avg_cost += c / total_batch\n",
    "\n",
    "    print(\"epoch: {epoch}, cost: {cost}\".format(\n",
    "        epoch = epoch,\n",
    "        cost  = avg_cost\n",
    "    ))\n",
    "\n",
    "# test accuracy\n",
    "correct_prediction = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "print(\"accuracy: {accuracy}\".format(\n",
    "    accuracy = sess.run(\n",
    "                   accuracy,\n",
    "                   feed_dict = {\n",
    "                                   X: mnist.test.images,\n",
    "                                   Y: mnist.test.labels\n",
    "                               }\n",
    "    )\n",
    "))\n",
    "\n",
    "# select one test example and predict\n",
    "r = random.randint(0, mnist.test.num_examples - 1)\n",
    "print(\"\\nlabel:\")\n",
    "print(sess.run(\n",
    "          tf.argmax(mnist.test.labels[r:r + 1], 1)\n",
    "      )\n",
    ")\n",
    "print(\"\\nprediction:\")\n",
    "print(sess.run(\n",
    "          tf.argmax(hypothesis, 1),\n",
    "          feed_dict = {X: mnist.test.images[r:r + 1]}\n",
    "      )\n",
    ")\n",
    "\n",
    "plt.imshow(\n",
    "    mnist.test.images[r:r + 1].reshape(28, 28),\n",
    "    cmap          = \"Greys\",\n",
    "    interpolation = \"nearest\"\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "# rectifier activation functions\n",
    "\n",
    "A rectifier activation function is defined as\n",
    "\n",
    "$${\n",
    "f\\left(x\\right)=\\textrm{max}\\left(0,x\\right)\n",
    "}$$\n",
    "\n",
    "where ${x}$ is the input to the neuron. It has been used in convolutional networks more effectively than the widely-used logistic sigmoid and the hyperbolic tangent. It was the most popular activation function for deep neural networks in 2015. A unit employing the rectifier is also called a rectified linear unit (ReLU).\n",
    "\n",
    "Rectified linear units, compared to sigmoid function or similar activation functions, allow for faster and effective training of deep neural architectures on large and complex datasets.\n",
    "\n",
    "A smooth approximation to the rectifier is the following analytic function, called the softplus function:\n",
    "\n",
    "$${\n",
    "f\\left(x\\right)=\\ln\\left(1+e^{x}\\right)\n",
    "}$$\n",
    "\n",
    "The derivative of softplus is the logistic function:\n",
    "\n",
    "$${\n",
    "f^{\\prime}\\left(x\\right)=\\frac{e^{x}}{e^{x}+1}=\\frac{1}{1+e^{-x}}\n",
    "}$$\n",
    "\n",
    "# example: neural network classifier for MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "epoch: 0, cost: 173.803806047\n",
      "epoch: 1, cost: 44.0933057898\n",
      "epoch: 2, cost: 27.9979391154\n",
      "epoch: 3, cost: 19.8257013471\n",
      "epoch: 4, cost: 14.5126266484\n",
      "epoch: 5, cost: 10.80432725\n",
      "epoch: 6, cost: 8.09893777095\n",
      "epoch: 7, cost: 6.22545443229\n",
      "epoch: 8, cost: 4.47642658703\n",
      "epoch: 9, cost: 3.42179229905\n",
      "epoch: 10, cost: 2.61225969974\n",
      "epoch: 11, cost: 1.95967011973\n",
      "epoch: 12, cost: 1.47646401341\n",
      "epoch: 13, cost: 1.28384371805\n",
      "epoch: 14, cost: 0.973924499147\n",
      "accuracy: 0.945699989796\n",
      "\n",
      "label:\n",
      "[4]\n",
      "\n",
      "prediction:\n",
      "[4]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot = True)\n",
    "\n",
    "# parameters\n",
    "learning_rate   = 0.001\n",
    "training_epochs = 15\n",
    "batch_size      = 100\n",
    "\n",
    "# input place holders\n",
    "X = tf.placeholder(tf.float32, [None, 784])\n",
    "Y = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "# weights and bias for layers\n",
    "W1 = tf.Variable(tf.random_normal([784, 256]))\n",
    "b1 = tf.Variable(tf.random_normal([256]))\n",
    "L1 = tf.nn.relu(tf.matmul(X, W1) + b1)\n",
    "\n",
    "W2 = tf.Variable(tf.random_normal([256, 256]))\n",
    "b2 = tf.Variable(tf.random_normal([256]))\n",
    "L2 = tf.nn.relu(tf.matmul(L1, W2) + b2)\n",
    "\n",
    "W3 = tf.Variable(tf.random_normal([256, 10]))\n",
    "b3 = tf.Variable(tf.random_normal([10]))\n",
    "hypothesis = tf.matmul(L2, W3) + b3\n",
    "\n",
    "# define cost/loss and optimizer\n",
    "cost      = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = hypothesis, labels = Y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(cost)\n",
    "\n",
    "# initialize\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for epoch in range(training_epochs):\n",
    "\n",
    "    avg_cost = 0\n",
    "    total_batch = int(mnist.train.num_examples / batch_size)\n",
    "\n",
    "    for i in range(total_batch):\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "        feed_dict = {X: batch_xs, Y: batch_ys}\n",
    "        c, _ = sess.run([cost, optimizer], feed_dict = feed_dict)\n",
    "        avg_cost += c / total_batch\n",
    "\n",
    "    print(\"epoch: {epoch}, cost: {cost}\".format(\n",
    "        epoch = epoch,\n",
    "        cost  = avg_cost\n",
    "    ))\n",
    "\n",
    "# test accuracy\n",
    "correct_prediction = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "print(\"accuracy: {accuracy}\".format(\n",
    "    accuracy = sess.run(\n",
    "                   accuracy,\n",
    "                   feed_dict = {\n",
    "                                   X: mnist.test.images,\n",
    "                                   Y: mnist.test.labels\n",
    "                               }\n",
    "    )\n",
    "))\n",
    "\n",
    "# select one test example and predict\n",
    "r = random.randint(0, mnist.test.num_examples - 1)\n",
    "print(\"\\nlabel:\")\n",
    "print(sess.run(\n",
    "          tf.argmax(mnist.test.labels[r:r + 1], 1)\n",
    "      )\n",
    ")\n",
    "print(\"\\nprediction:\")\n",
    "print(sess.run(\n",
    "          tf.argmax(hypothesis, 1),\n",
    "          feed_dict = {X: mnist.test.images[r:r + 1]}\n",
    "      )\n",
    ")\n",
    "\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Xavier initialization\n",
    "\n",
    "Xavier initialization in neural networks involves initialization of the weights of a network so that the neuron activation functions are not starting out in saturated or dead regions. In effect, the weights are initialized with pseudorandom numbers that are not \"too small\" or \"too large\".\n",
    "\n",
    "If the input (from the transfer function) for a neuron is very large or very small, the activation function (hyperbolic tangent, for example) is saturated or stuck at +1 or -1 respectively. Having saturated neurons limits their dynamic range and, so, limits their representational power. How can one avoid getting stuck in such saturated regions? The transfer function is a sum of the products of weights and inputs. To avoid the transfer function being too large or too small, the weights and the input can be kept in some sensible range. The input, from data, can be restricted by normalizing the dataset using z-scaling or other methods (ensuring that the data has zero mean and unit variance). What about the weights?\n",
    "\n",
    "Technically the weights can be set to any pseudorandom values and then can be changed using a learning rile such as stochastic gradient descent to adjust them to minimize error. However, if weights are very large or very small or such that they cause the neuron to be saturated, then it takes gradient descent more iterations to adjust the weights.\n",
    "\n",
    "Xavier initialization suggests initializing the weights with a variance such that the variance of the transfer function is unity. Ensuring that this variance is unity reduces the likelihood of being stuck in saturated regions of the activation function.\n",
    "\n",
    "# example: neural network classifier with Xavier initialization for MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "epoch: 0, cost: 0.331354504322\n",
      "epoch: 1, cost: 0.119444444173\n",
      "epoch: 2, cost: 0.0798904514194\n",
      "epoch: 3, cost: 0.0552678741227\n",
      "epoch: 4, cost: 0.0411215884361\n",
      "epoch: 5, cost: 0.030692967015\n",
      "epoch: 6, cost: 0.0256018338039\n",
      "epoch: 7, cost: 0.0202623238294\n",
      "epoch: 8, cost: 0.0171883650348\n",
      "epoch: 9, cost: 0.0141780922553\n",
      "epoch: 10, cost: 0.0123592672347\n",
      "epoch: 11, cost: 0.011780318041\n",
      "epoch: 12, cost: 0.0112451218761\n",
      "epoch: 13, cost: 0.00916771249669\n",
      "epoch: 14, cost: 0.0129711465011\n",
      "accuracy: 0.977400004864\n",
      "\n",
      "label:\n",
      "[0]\n",
      "\n",
      "prediction:\n",
      "[0]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot = True)\n",
    "\n",
    "# parameters\n",
    "learning_rate   = 0.001\n",
    "training_epochs = 15\n",
    "batch_size      = 100\n",
    "\n",
    "# input place holders\n",
    "X = tf.placeholder(tf.float32, [None, 784])\n",
    "Y = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "# weights and bias for layers\n",
    "W1 = tf.get_variable(\"W1\", shape = [784, 256], initializer = tf.contrib.layers.xavier_initializer())\n",
    "b1 = tf.Variable(tf.random_normal([256]))\n",
    "L1 = tf.nn.relu(tf.matmul(X, W1) + b1)\n",
    "\n",
    "W2 = tf.get_variable(\"W2\", shape = [256, 256], initializer = tf.contrib.layers.xavier_initializer())\n",
    "b2 = tf.Variable(tf.random_normal([256]))\n",
    "L2 = tf.nn.relu(tf.matmul(L1, W2) + b2)\n",
    "\n",
    "W3 = tf.get_variable(\"W3\", shape = [256, 10], initializer = tf.contrib.layers.xavier_initializer())\n",
    "b3 = tf.Variable(tf.random_normal([10]))\n",
    "hypothesis = tf.matmul(L2, W3) + b3\n",
    "\n",
    "# define cost/loss and optimizer\n",
    "cost      = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = hypothesis, labels = Y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(cost)\n",
    "\n",
    "# initialize\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for epoch in range(training_epochs):\n",
    "\n",
    "    avg_cost = 0\n",
    "    total_batch = int(mnist.train.num_examples / batch_size)\n",
    "\n",
    "    for i in range(total_batch):\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "        feed_dict = {X: batch_xs, Y: batch_ys}\n",
    "        c, _ = sess.run([cost, optimizer], feed_dict = feed_dict)\n",
    "        avg_cost += c / total_batch\n",
    "\n",
    "    print(\"epoch: {epoch}, cost: {cost}\".format(\n",
    "        epoch = epoch,\n",
    "        cost  = avg_cost\n",
    "    ))\n",
    "\n",
    "# test accuracy\n",
    "correct_prediction = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "print(\"accuracy: {accuracy}\".format(\n",
    "    accuracy = sess.run(\n",
    "                   accuracy,\n",
    "                   feed_dict = {\n",
    "                                   X: mnist.test.images,\n",
    "                                   Y: mnist.test.labels\n",
    "                   }\n",
    "    )\n",
    "))\n",
    "\n",
    "# select one test example and predict\n",
    "r = random.randint(0, mnist.test.num_examples - 1)\n",
    "print(\"\\nlabel:\")\n",
    "print(sess.run(\n",
    "          tf.argmax(mnist.test.labels[r:r + 1], 1)\n",
    "      )\n",
    ")\n",
    "print(\"\\nprediction:\")\n",
    "print(sess.run(\n",
    "          tf.argmax(hypothesis, 1),\n",
    "          feed_dict = {X: mnist.test.images[r:r + 1]}\n",
    "      )\n",
    ")\n",
    "\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# example: deep neural network classifier with Xavier initialization for MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "epoch: 0, cost: 0.325931779084\n",
      "epoch: 1, cost: 0.107867569024\n",
      "epoch: 2, cost: 0.0705091328517\n",
      "epoch: 3, cost: 0.0552865000247\n",
      "epoch: 4, cost: 0.0430004646313\n",
      "epoch: 5, cost: 0.0329331435739\n",
      "epoch: 6, cost: 0.030525810439\n",
      "epoch: 7, cost: 0.0267394819635\n",
      "epoch: 8, cost: 0.0231419500274\n",
      "epoch: 9, cost: 0.0197627094166\n",
      "epoch: 10, cost: 0.0197169608473\n",
      "epoch: 11, cost: 0.015406726154\n",
      "epoch: 12, cost: 0.0196975519168\n",
      "epoch: 13, cost: 0.0184239531601\n",
      "epoch: 14, cost: 0.0138537911348\n",
      "accuracy: 0.980000019073\n",
      "\n",
      "label:\n",
      "[3]\n",
      "\n",
      "prediction:\n",
      "[3]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot = True)\n",
    "\n",
    "# parameters\n",
    "learning_rate   = 0.001\n",
    "training_epochs = 15\n",
    "batch_size      = 100\n",
    "\n",
    "# input place holders\n",
    "X = tf.placeholder(tf.float32, [None, 784])\n",
    "Y = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "# weights and bias for layers\n",
    "W1 = tf.get_variable(\"W1\", shape = [784, 512], initializer = tf.contrib.layers.xavier_initializer())\n",
    "b1 = tf.Variable(tf.random_normal([512]))\n",
    "L1 = tf.nn.relu(tf.matmul(X, W1) + b1)\n",
    "\n",
    "W2 = tf.get_variable(\"W2\", shape = [512, 512], initializer = tf.contrib.layers.xavier_initializer())\n",
    "b2 = tf.Variable(tf.random_normal([512]))\n",
    "L2 = tf.nn.relu(tf.matmul(L1, W2) + b2)\n",
    "\n",
    "W3 = tf.get_variable(\"W3\", shape = [512, 512], initializer = tf.contrib.layers.xavier_initializer())\n",
    "b3 = tf.Variable(tf.random_normal([512]))\n",
    "L3 = tf.nn.relu(tf.matmul(L2, W3) + b3)\n",
    "\n",
    "W4 = tf.get_variable(\"W4\", shape = [512, 512], initializer = tf.contrib.layers.xavier_initializer())\n",
    "b4 = tf.Variable(tf.random_normal([512]))\n",
    "L4 = tf.nn.relu(tf.matmul(L3, W4) + b4)\n",
    "\n",
    "W5 = tf.get_variable(\"W5\", shape = [512, 10], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b5 = tf.Variable(tf.random_normal([10]))\n",
    "hypothesis = tf.matmul(L4, W5) + b5\n",
    "\n",
    "# define cost/loss and optimizer\n",
    "cost      = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = hypothesis, labels = Y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(cost)\n",
    "\n",
    "# initialize\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for epoch in range(training_epochs):\n",
    "\n",
    "    avg_cost = 0\n",
    "    total_batch = int(mnist.train.num_examples / batch_size)\n",
    "\n",
    "    for i in range(total_batch):\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "        feed_dict = {X: batch_xs, Y: batch_ys}\n",
    "        c, _ = sess.run([cost, optimizer], feed_dict = feed_dict)\n",
    "        avg_cost += c / total_batch\n",
    "\n",
    "    print(\"epoch: {epoch}, cost: {cost}\".format(\n",
    "        epoch = epoch,\n",
    "        cost  = avg_cost\n",
    "    ))\n",
    "\n",
    "# test accuracy\n",
    "correct_prediction = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "print(\"accuracy: {accuracy}\".format(\n",
    "    accuracy = sess.run(\n",
    "                   accuracy,\n",
    "                   feed_dict = {\n",
    "                                   X: mnist.test.images,\n",
    "                                   Y: mnist.test.labels\n",
    "                   }\n",
    "    )\n",
    "))\n",
    "\n",
    "# select one test example and predict\n",
    "r = random.randint(0, mnist.test.num_examples - 1)\n",
    "print(\"\\nlabel:\")\n",
    "print(sess.run(\n",
    "          tf.argmax(mnist.test.labels[r:r + 1], 1)\n",
    "      )\n",
    ")\n",
    "print(\"\\nprediction:\")\n",
    "print(sess.run(\n",
    "          tf.argmax(hypothesis, 1),\n",
    "          feed_dict = {X: mnist.test.images[r:r + 1]}\n",
    "      )\n",
    ")\n",
    "\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# example: deep neural network classifier with Xavier initialization and dropout for MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "epoch: 0, cost: 0.491187336401\n",
      "epoch: 1, cost: 0.174933730807\n",
      "epoch: 2, cost: 0.130297549333\n",
      "epoch: 3, cost: 0.11029827317\n",
      "epoch: 4, cost: 0.0910160335949\n",
      "epoch: 5, cost: 0.0845644689834\n",
      "epoch: 6, cost: 0.0707050579363\n",
      "epoch: 7, cost: 0.0713776325307\n",
      "epoch: 8, cost: 0.0625691854655\n",
      "epoch: 9, cost: 0.05885692129\n",
      "epoch: 10, cost: 0.0574975110159\n",
      "epoch: 11, cost: 0.052429081744\n",
      "epoch: 12, cost: 0.0469085732659\n",
      "epoch: 13, cost: 0.0468979393831\n",
      "epoch: 14, cost: 0.0453263855474\n",
      "accuracy: 0.982699990273\n",
      "\n",
      "label:\n",
      "[4]\n",
      "\n",
      "prediction:\n",
      "[4]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot = True)\n",
    "\n",
    "# parameters\n",
    "learning_rate   = 0.001\n",
    "training_epochs = 15\n",
    "batch_size      = 100\n",
    "\n",
    "# input place holders\n",
    "X = tf.placeholder(tf.float32, [None, 784])\n",
    "Y = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "# dropout (keep_prob) rate  0.7 on training, but should be 1 for testing\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "# weights and bias for layers\n",
    "W1 = tf.get_variable(\"W1\", shape = [784, 512], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b1 = tf.Variable(tf.random_normal([512]))\n",
    "L1 = tf.nn.relu(tf.matmul(X, W1) + b1)\n",
    "L1 = tf.nn.dropout(L1, keep_prob = keep_prob)\n",
    "\n",
    "W2 = tf.get_variable(\"W2\", shape = [512, 512], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b2 = tf.Variable(tf.random_normal([512]))\n",
    "L2 = tf.nn.relu(tf.matmul(L1, W2) + b2)\n",
    "L2 = tf.nn.dropout(L2, keep_prob = keep_prob)\n",
    "\n",
    "W3 = tf.get_variable(\"W3\", shape = [512, 512], initializer = tf.contrib.layers.xavier_initializer())\n",
    "b3 = tf.Variable(tf.random_normal([512]))\n",
    "L3 = tf.nn.relu(tf.matmul(L2, W3) + b3)\n",
    "L3 = tf.nn.dropout(L3, keep_prob = keep_prob)\n",
    "\n",
    "W4 = tf.get_variable(\"W4\", shape = [512, 512], initializer = tf.contrib.layers.xavier_initializer())\n",
    "b4 = tf.Variable(tf.random_normal([512]))\n",
    "L4 = tf.nn.relu(tf.matmul(L3, W4) + b4)\n",
    "L4 = tf.nn.dropout(L4, keep_prob = keep_prob)\n",
    "\n",
    "W5 = tf.get_variable(\"W5\", shape = [512, 10], initializer = tf.contrib.layers.xavier_initializer())\n",
    "b5 = tf.Variable(tf.random_normal([10]))\n",
    "hypothesis = tf.matmul(L4, W5) + b5\n",
    "\n",
    "# define cost/loss and optimizer\n",
    "cost      = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = hypothesis, labels = Y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(cost)\n",
    "\n",
    "# initialize\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for epoch in range(training_epochs):\n",
    "\n",
    "    avg_cost = 0\n",
    "    total_batch = int(mnist.train.num_examples / batch_size)\n",
    "\n",
    "    for i in range(total_batch):\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "        feed_dict = {X: batch_xs, Y: batch_ys, keep_prob: 0.7}\n",
    "        c, _ = sess.run([cost, optimizer], feed_dict = feed_dict)\n",
    "        avg_cost += c / total_batch\n",
    "\n",
    "    print(\"epoch: {epoch}, cost: {cost}\".format(\n",
    "        epoch = epoch,\n",
    "        cost  = avg_cost\n",
    "    ))\n",
    "\n",
    "# test accuracy\n",
    "correct_prediction = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "print(\"accuracy: {accuracy}\".format(\n",
    "    accuracy = sess.run(\n",
    "                   accuracy,\n",
    "                   feed_dict = {\n",
    "                                   X:         mnist.test.images,\n",
    "                                   Y:         mnist.test.labels,\n",
    "                                   keep_prob: 1\n",
    "                   }\n",
    "    )\n",
    "))\n",
    "\n",
    "# select one test example and predict\n",
    "r = random.randint(0, mnist.test.num_examples - 1)\n",
    "print(\"\\nlabel:\")\n",
    "print(sess.run(\n",
    "          tf.argmax(mnist.test.labels[r:r + 1], 1)\n",
    "      )\n",
    ")\n",
    "print(\"\\nprediction:\")\n",
    "print(sess.run(\n",
    "          tf.argmax(hypothesis, 1),\n",
    "          feed_dict = {X: mnist.test.images[r:r + 1], keep_prob: 1}\n",
    "      )\n",
    ")\n",
    "\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST comparison using identical learning rate, training epochs and batch size\n",
    "\n",
    "|**classifier**                                                                 |**accuracy**|\n",
    "|-------------------------------------------------------------------------------|------------|\n",
    "|softmax classifier for MNIST                                                   |89.76%      |\n",
    "|neural network classifier for MNIST                                            |94.57%      |\n",
    "|neural network classifier with Xavier initialization for MNIST                 |97.74%      |\n",
    "|deep neural network classifier with Xavier initialization for MNIST            |98%         |\n",
    "|deep neural network classifier with Xavier initialization and dropout for MNIST|98.26%      |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
