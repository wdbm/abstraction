{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow\n",
    "\n",
    "TensorFlow is an open source software library for numerical computation using data flow graphs. In a data flow graph, nodes represent mathematical operations and edges represent the multidimensional data arrays (tensors) communicated between them.\n",
    "\n",
    "![](https://raw.githubusercontent.com/wdbm/abstraction/master/media/2016-05-14T1754Z.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://unsupervisedmethods.com/cheat-sheet-of-machine-learning-and-python-and-math-cheat-sheets-a4afe4e791b6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.TF_CPP_MIN_LOG_LEVEL = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello world\n"
     ]
    }
   ],
   "source": [
    "# Create a constant operation. This operation is added as a node to the default graph.\n",
    "hello = tf.constant(\"hello world\")\n",
    "\n",
    "# Start a TensorFlow session.\n",
    "sess = tf.Session()\n",
    "\n",
    "# Run the operation and get the result.\n",
    "print(sess.run(hello))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tensors, ranks, shapes and types\n",
    "\n",
    "|**rank**|**mathamatical object**|**shape**  |**example**                       |\n",
    "|--------|-----------------------|-----------|----------------------------------|\n",
    "|0       |scalar                 |`[]`       |`3`                               |\n",
    "|1       |vector                 |`[3]`      |`[1. ,2., 3.]`                    |\n",
    "|2       |matrix                 |`[2, 3]`   |`[[1., 2., 3.], [4., 5., 6.]]`    |\n",
    "|3       |3-tensor               |`[2, 1, 3]`|`[[[1., 2., 3.]], [[7., 8., 9.]]]`|\n",
    "|n       |n-tensor               |...        |...                               |\n",
    "\n",
    "|**data type**|Python type|**description**       |\n",
    "|-------------|-----------|----------------------|\n",
    "|`DT_FLOAT`   |`t.float32`|32 bits floating point|\n",
    "|`DT_DOUBLE`  |`t.float64`|64 bits floating point|\n",
    "|`DT_INT8`    |`t.int8`   |8 bits signed integer |\n",
    "|`DT_INT16`   |`t.int16`  |16 bits signed integer|\n",
    "|`DT_INT32`   |`t.int32`  |32 bits signed integer|\n",
    "|`DT_INT64`   |`t.int64`  |64 bits signed integer|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow mechanics\n",
    "\n",
    "- 1 Build a graph using TensorFlow operations.\n",
    "- 2 Feed data to TensorFlow and run the graph.\n",
    "- 3 Update variables in the graph and return values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "node1: Tensor(\"Const_1:0\", shape=(), dtype=float32)\n",
      "node2: Tensor(\"Const_2:0\", shape=(), dtype=float32)\n",
      "node3: Tensor(\"Add:0\", shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "node1 = tf.constant(3.0, tf.float32)\n",
    "node2 = tf.constant(4.0) # (also tf.float32 by default)\n",
    "node3 = tf.add(node1, node2)\n",
    "\n",
    "print(\"node1: {node}\".format(node = node1))\n",
    "print(\"node2: {node}\".format(node = node2))\n",
    "print(\"node3: {node}\".format(node = node3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sess.run(node1, node2): [3.0, 4.0]\n",
      "sess.run(node3):        7.0\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "\n",
    "print(\"sess.run(node1, node2): {result}\".format(\n",
    "    result = sess.run([node1, node2])\n",
    "))\n",
    "print(\"sess.run(node3):        {result}\".format(\n",
    "    result = sess.run(node3)\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.5\n"
     ]
    }
   ],
   "source": [
    "a = tf.placeholder(tf.float32)\n",
    "b = tf.placeholder(tf.float32)\n",
    "\n",
    "# Create a node that is a shortcut for tf.add(a, b).\n",
    "adder_node = a + b\n",
    "\n",
    "result = sess.run(\n",
    "    adder_node,\n",
    "    feed_dict = {\n",
    "        a: 3,\n",
    "        b: 4.5\n",
    "    }\n",
    ")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 3.  7.]\n"
     ]
    }
   ],
   "source": [
    "result = sess.run(\n",
    "    adder_node,\n",
    "    feed_dict = {\n",
    "        a: [1,3],\n",
    "        b: [2, 4]\n",
    "    }\n",
    ")\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22.5\n"
     ]
    }
   ],
   "source": [
    "add_and_triple = adder_node * 3.\n",
    "\n",
    "result = sess.run(\n",
    "    add_and_triple,\n",
    "    feed_dict = {\n",
    "        a: 3,\n",
    "        b: 4.5\n",
    "    }\n",
    ")\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# session\n",
    "\n",
    "A `Session` is a class for running TensorFlow operations. A session object encapsulates the environment in which operations are executed and tensors are evaluated. For example, `sess.run(c)` evaluates the tensor `c`.\n",
    "\n",
    "A session is run using its `run` method:\n",
    "\n",
    "```Python\n",
    "tf.Session.run(\n",
    "    fetches,\n",
    "    feed_dict    = None,\n",
    "    options      = None,\n",
    "    run_metadata = None\n",
    ")\n",
    "```\n",
    "\n",
    "This method runs operations and evaluates tensors in fetches. It returns one epoch of TensorFlow computation, by running the necessary graph fragment to execute every operation and evaluate every tensor in fetches, substituting the values in `feed_dict` for the corresponding input values. The `fetches` option can be a single graph element, or an arbitrary nested list, tuple, namedtuple, dict or OrderedDict containing graph elements at its leaves. The value returned by `run` has the same shape as the fetches argument, where the leaves are replaced by the corresponding values returned by TensorFlow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10 20]\n",
      "[array([10, 20], dtype=int32), array([ 1.,  2.], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "\n",
    "a = tf.constant([10 ,  20])\n",
    "b = tf.constant([1.0, 2.0])\n",
    "\n",
    "print(sess.run(a))\n",
    "print(sess.run([a, b]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create two variables.\n",
    "weights = tf.Variable(\n",
    "    tf.random_normal(\n",
    "        [784, 200],\n",
    "        stddev = 0.35\n",
    "    ),\n",
    "    name = \"weights\"\n",
    ")\n",
    "biases = tf.Variable(\n",
    "    tf.zeros([200]),\n",
    "    name = \"biases\"\n",
    ")\n",
    "\n",
    "# Create an operation to initialize the variables.\n",
    "init_op = tf.global_variables_initializer()\n",
    "\n",
    "# more code\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init_op)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# single variable linear regression\n",
    "\n",
    "- hypothesis: ${H\\left(x\\right)=Wx+b}$\n",
    "\n",
    "- cost function: ${\\textrm{cost}\\left(W,b\\right)=\\frac{1}{m}\\Sigma_{i=1}^{m}\\left(H\\left(x^{i}\\right)-y^{i}\\right)^{2}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 0, cost: 0.327590852976, W: [ 1.69378853], b: [-1.30574894]\n",
      "step: 500, cost: 0.0235481653363, W: [ 1.17822742], b: [-0.40515277]\n",
      "step: 1000, cost: 0.00212170509622, W: [ 1.05349815], b: [-0.12161381]\n",
      "step: 1500, cost: 0.000191168233869, W: [ 1.01605844], b: [-0.0365047]\n",
      "step: 2000, cost: 1.72240033862e-05, W: [ 1.00482023], b: [-0.01095748]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.set_random_seed(777)\n",
    "\n",
    "# Create some data.\n",
    "x_train = [1, 2, 3]\n",
    "y_train = [1, 2, 3]\n",
    "\n",
    "# Build the graph using TensorFlow operations. With the hypothesis H(x) = Wx + b, the goal is to try to find values for W and b to in order to calculate y_data = x_data * W + b. Analytically, W should be 1 and b should be 0.\n",
    "W = tf.Variable(tf.random_normal([1]), name = \"weight\")\n",
    "b = tf.Variable(tf.random_normal([1]), name = \"bias\")\n",
    "\n",
    "# Define the hypothesis.\n",
    "hypothesis = x_train * W + b\n",
    "\n",
    "# Define the cost function.\n",
    "cost = tf.reduce_mean(tf.square(hypothesis - y_train))\n",
    "\n",
    "# Define a method of minimisation, in this case gradient descent. In gradient descent, steps proportional to the negative of the function gradient at the current point are taken. It is the method of steepest descent to find the local minimum of a function.\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate = 0.01)\n",
    "train = optimizer.minimize(cost)\n",
    "\n",
    "# Launch the graph in a session.\n",
    "sess = tf.Session()\n",
    "\n",
    "# Initialize global variables in the graph.\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# Fit.\n",
    "for step in range(2001):\n",
    "    sess.run(train)\n",
    "    if step % 500 == 0:\n",
    "        print(\"step: {step}, cost: {cost}, W: {W}, b: {b}\".format(\n",
    "            step = step,\n",
    "            cost = sess.run(cost),\n",
    "            W    = sess.run(W),\n",
    "            b    = sess.run(b)\n",
    "        ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# placeholders and variables\n",
    "\n",
    "A variable (`tf.Variable`) is used generally for trainable variables such as weights and biases for a model. A placeholder (`tf.placeholder`) is used to feed actual training examples. A variable is set with an initial value on declaration while a placeholder doesn't require an initial value on declaration, but has its value specified at run time using the session `feed_dict`. In TensorFlow, variables are trained over time while placeholders are input data that doesn't change as the model trains (e.g. input images and class labels for the images).\n",
    "\n",
    "A placeholder is a value that is input when TensorFlow is set to run a computation. A variable is a modifiable tensor that exists in TensorFlow's graph of interacting operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 0, cost: 19.0289134979, W: [-0.81955934], b: [ 0.05452838]\n",
      "step: 500, cost: 0.0066645629704, W: [ 0.90541184], b: [ 0.21502104]\n",
      "step: 1000, cost: 0.000600479834247, W: [ 0.97160774], b: [ 0.06454235]\n",
      "step: 1500, cost: 5.41025183338e-05, W: [ 0.99147761], b: [ 0.01937346]\n",
      "step: 2000, cost: 4.87493980472e-06, W: [ 0.99744177], b: [ 0.00581537]\n",
      "[ 1.00325716  2.0006988   2.99814057]\n",
      "[ 1.00325716  2.0006988   2.99814057]\n",
      "[ 1.00325716  2.0006988   2.99814057]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "W = tf.Variable(tf.random_normal([1]), name = \"weight\")\n",
    "b = tf.Variable(tf.random_normal([1]), name = \"bias\")\n",
    "\n",
    "# Create placeholders for tensors for x and y data.\n",
    "X = tf.placeholder(tf.float32, shape = [None])\n",
    "Y = tf.placeholder(tf.float32, shape = [None])\n",
    "\n",
    "hypothesis = x_train * W + b\n",
    "cost = tf.reduce_mean(tf.square(hypothesis - y_train))\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate = 0.01)\n",
    "train = optimizer.minimize(cost)\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# Fit.\n",
    "for step in range(2001):\n",
    "    cost_value, W_value, b_value, _ = sess.run(\n",
    "        [cost, W, b, train],\n",
    "        feed_dict = {\n",
    "            X: [1, 2, 3],\n",
    "            Y: [1, 2, 3]\n",
    "        }\n",
    "    )\n",
    "    if step % 500 == 0:\n",
    "        print(\"step: {step}, cost: {cost}, W: {W}, b: {b}\".format(\n",
    "            step = step,\n",
    "            cost = cost_value,\n",
    "            W    = W_value,\n",
    "            b    = b_value\n",
    "        ))\n",
    "\n",
    "# Test the trained model.\n",
    "print(sess.run(hypothesis, feed_dict={X: [5]}))\n",
    "print(sess.run(hypothesis, feed_dict={X: [2.5]}))\n",
    "print(sess.run(hypothesis, feed_dict={X: [1.5, 3.5]}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# cost minimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- simplified hypothesis: ${H\\left(x\\right)=Wx}$\n",
    "\n",
    "- cost function: ${\\textrm{cost}\\left(W\\right)=\\frac{1}{m}\\Sigma_{i=1}^{m}\\left(H\\left(x^{i}\\right)-y^{i}\\right)^{2}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQwAAAEKCAYAAADn1WuOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fdd98041490>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "X = [1, 2, 3]\n",
    "Y = [1, 2, 3]\n",
    "\n",
    "W = tf.placeholder(tf.float32)\n",
    "\n",
    "hypothesis = X * W\n",
    "cost = tf.reduce_mean(tf.square(hypothesis - Y))\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# Variables for plotting cost function\n",
    "W_value    = []\n",
    "cost_value = []\n",
    "\n",
    "for i in range(-30, 50):\n",
    "\n",
    "    feed_W = i * 0.1\n",
    "    cost_current, W_current = sess.run(\n",
    "        [cost, W],\n",
    "        feed_dict = {W: feed_W}\n",
    "    )\n",
    "\n",
    "    W_value.append(W_current)\n",
    "    cost_value.append(cost_current)\n",
    "\n",
    "plt.xlabel(\"W\"); plt.ylabel(\"cost(W)\")\n",
    "plt.plot(W_value, cost_value)\n",
    "plt.axes().set_aspect(1 / plt.axes().get_data_ratio())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# multivariable linear regression\n",
    "\n",
    "- hypothesis defined without using a matrix: ${H\\left(x_{1},x_{2},x_{3}\\right)=x_{1}w_{1}+x_{2}w_{2}+x_{3}w_{3}}$\n",
    "\n",
    "test scores for general psychology:\n",
    "\n",
    "|**${x_{1}}$**|**${x_{2}}$**|**${x_{3}}$**|**${Y}$**|\n",
    "|-------------|-------------|-------------|---------|\n",
    "|73           |80           |75           |152      |\n",
    "|93           |88           |93           |185      |\n",
    "|89           |91           |90           |180      |\n",
    "|96           |98           |100          |196      |\n",
    "|73           |66           |70           |142      |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "step: 0, cost: 1221.08886719,\n",
      "prediction: [ 123.09977722  145.56219482  144.64117432  158.4463501   109.52912903]\n",
      "\n",
      "step: 500, cost: 5.31126022339,\n",
      "prediction: [ 153.60321045  183.01623535  181.1355896   198.21069336  138.27522278]\n",
      "\n",
      "step: 1000, cost: 4.28688764572,\n",
      "prediction: [ 153.2394104   183.26794434  181.02700806  198.10955811  138.62460327]\n",
      "\n",
      "step: 1500, cost: 3.50106287003,\n",
      "prediction: [ 152.92288208  183.48718262  180.93289185  198.01914978  138.9311676 ]\n",
      "\n",
      "step: 2000, cost: 2.89726567268,\n",
      "prediction: [ 152.6476593   183.67808533  180.85136414  197.93815613  139.20033264]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "x1_data = [ 73.,  93.,  89.,  96.,  73.]\n",
    "x2_data = [ 80.,  88.,  91.,  98.,  66.]\n",
    "x3_data = [ 75.,  93.,  90., 100.,  70.]\n",
    "\n",
    "y_data  = [152., 185., 180., 196., 142.]\n",
    "\n",
    "x1 = tf.placeholder(tf.float32)\n",
    "x2 = tf.placeholder(tf.float32)\n",
    "x3 = tf.placeholder(tf.float32)\n",
    "Y  = tf.placeholder(tf.float32)\n",
    "\n",
    "w1 = tf.Variable(tf.random_normal([1]), name = \"weight1\")\n",
    "w2 = tf.Variable(tf.random_normal([1]), name = \"weight2\")\n",
    "w3 = tf.Variable(tf.random_normal([1]), name = \"weight3\")\n",
    "b  = tf.Variable(tf.random_normal([1]), name = \"bias\"   )\n",
    "\n",
    "hypothesis = x1 * w1 + x2 * w2 + x3 * w3 + b\n",
    "cost       = tf.reduce_mean(tf.square(hypothesis - Y))\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate = 1e-5)\n",
    "train     = optimizer.minimize(cost)\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for step in range(2001):\n",
    "    cost_value, hy_value, _ = sess.run(\n",
    "        [cost, hypothesis, train],\n",
    "        feed_dict = {\n",
    "            x1: x1_data,\n",
    "            x2: x2_data,\n",
    "            x3: x3_data,\n",
    "            Y:  y_data\n",
    "        }\n",
    "    )\n",
    "\n",
    "    if step % 500 == 0:\n",
    "        print(\"\\nstep: {step}, cost: {cost},\\nprediction: {prediction}\".format(\n",
    "            step       = step,\n",
    "            cost       = cost_value,\n",
    "            prediction = hy_value\n",
    "        ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- hypothesis defined using a matrix:\n",
    "\n",
    "$${\n",
    "\\begin{pmatrix}\n",
    "x_{1} & x_{2} & x_{3}\\\\\n",
    "\\end{pmatrix}\n",
    "\\cdot\n",
    "\\begin{pmatrix}\n",
    "w_{1} \\\\\n",
    "w_{2} \\\\\n",
    "w_{3} \\\\\n",
    "\\end{pmatrix}\n",
    "=\n",
    "\\left(x_{1}w_{1}+x_{2}w_{2}+x_{3}w_{3}\\right)\n",
    "}$$\n",
    "\n",
    "$${\n",
    "H\\left(X\\right)=XW\n",
    "}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "step: 0, cost: 65613.796875,\n",
      "prediction:\n",
      "[[-70.3801651 ]\n",
      " [-92.29891968]\n",
      " [-86.97892761]\n",
      " [-94.72986603]\n",
      " [-72.28895569]]\n",
      "\n",
      "step: 500, cost: 13.3241577148,\n",
      "prediction:\n",
      "[[ 156.13461304]\n",
      " [ 181.35145569]\n",
      " [ 181.92817688]\n",
      " [ 198.17074585]\n",
      " [ 136.72895813]]\n",
      "\n",
      "step: 1000, cost: 10.2774734497,\n",
      "prediction:\n",
      "[[ 155.48991394]\n",
      " [ 181.79486084]\n",
      " [ 181.73246765]\n",
      " [ 198.01477051]\n",
      " [ 137.32301331]]\n",
      "\n",
      "step: 1500, cost: 7.95214223862,\n",
      "prediction:\n",
      "[[ 154.92767334]\n",
      " [ 182.18174744]\n",
      " [ 181.56201172]\n",
      " [ 197.8772583 ]\n",
      " [ 137.84272766]]\n",
      "\n",
      "step: 2000, cost: 6.17698717117,\n",
      "prediction:\n",
      "[[ 154.43740845]\n",
      " [ 182.51925659]\n",
      " [ 181.41358948]\n",
      " [ 197.75588989]\n",
      " [ 138.29750061]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "x_data = [\n",
    "             [ 73.,  80.,  75.],\n",
    "             [ 93.,  88.,  93.],\n",
    "             [ 89.,  91.,  90.],\n",
    "             [ 96.,  98., 100.],\n",
    "             [ 73.,  66.,  70.]\n",
    "         ]\n",
    "y_data = [\n",
    "             [152.],\n",
    "             [185.],\n",
    "             [180.],\n",
    "             [196.],\n",
    "             [142.]\n",
    "         ]\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=[None, 3])\n",
    "Y = tf.placeholder(tf.float32, shape=[None, 1])\n",
    "\n",
    "W = tf.Variable(tf.random_normal([3, 1]), name = \"weight\")\n",
    "b = tf.Variable(tf.random_normal([1]),    name = \"bias\"  )\n",
    "\n",
    "hypothesis = tf.matmul(X, W) + b\n",
    "cost       = tf.reduce_mean(tf.square(hypothesis - Y))\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate = 1e-5)\n",
    "train     = optimizer.minimize(cost)\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for step in range(2001):\n",
    "    cost_value, hy_value, _ = sess.run(\n",
    "        [cost, hypothesis, train], feed_dict={X: x_data, Y: y_data})\n",
    "    if step % 500 == 0:\n",
    "        print(\"\\nstep: {step}, cost: {cost},\\nprediction:\\n{prediction}\".format(\n",
    "            step       = step,\n",
    "            cost       = cost_value,\n",
    "            prediction = hy_value\n",
    "        ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# qualifying accuracy, ROC curves\n",
    "\n",
    "A receiver operating characteristic curve is a graphical plot that illustrates the diagnostic ability of a binary classifier system as its discrimination threshold is varied. The true-positive rate is plotted against the false-positive rate at various threshold settings. The true-positive rate is also known as sensitivity or probability of detection. The false-positive rate is also known as the fall-out or probability of false alarm and can be calculated as (1- specificity), where specificity is the true-negative rate.\n",
    "\n",
    "- sensitivity/true-positive rate: proportion of positives that are correctly identified as such\n",
    "- specificity/true-negative rate: proportion of negatives that are correctly identified as such\n",
    "- false-positive rate: proportion of positives that are incorrectly identified as such\n",
    "\n",
    "So, the ROC curve is the true-positive rate as a function of the false-positive rate. Usually the true-positive rate is on the Y-axis and the false-positive rate is on the X-axis. This means that the top left corner of the plot is the \"ideal\" point, a false-positive rate of zero and a true-positive rate of one. This is not very realistic, but it does mean that a larger area under the curve (AUC) is usually better. The \"steepness\" of the ROC curve is also important, since it is ideal to maximize the true-positive rate while minimizing the false-positive rate.\n",
    "\n",
    "To make the plot, consider creating two distributions for the two classes being classified by the model. Across the X-axis of the distributions is the probability assigned by the classifier. So, for each probability value, there are a certain number of the class objects that were classified by it. Consider these two distributions overlapping a bit. A threshold might be set at this point, wherein you say that everything above 0.5 probability is one class and everything below 0.5 probability is the other class.\n",
    "\n",
    "The ROC curve has the true-positive rate (TPR) on the Y-axis and the false-positive rate (FPR) on the X-axis for every possible classification threshold.\n",
    "\n",
    "An ROC curve visualizes all possible classification thresholds while a misclassification rate is the error rate for a single threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Reordering is not turned on, and the x array is not increasing: [1 1 0 1 1 1 0 0 1 0 1 0 1 0 0 0 1 0 1 0]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-90f54d4f49db>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mfpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthresholds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroc_curve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_probabilities\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mroc_auc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_probabilities\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'ROC curve (area = %0.3f)'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mroc_auc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/sklearn/metrics/ranking.py\u001b[0m in \u001b[0;36mauc\u001b[0;34m(x, y, reorder)\u001b[0m\n\u001b[1;32m     99\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m                 raise ValueError(\"Reordering is not turned on, and \"\n\u001b[0;32m--> 101\u001b[0;31m                                  \"the x array is not increasing: %s\" % x)\n\u001b[0m\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[0marea\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdirection\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrapz\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Reordering is not turned on, and the x array is not increasing: [1 1 0 1 1 1 0 0 1 0 1 0 1 0 0 0 1 0 1 0]"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sklearn.metrics\n",
    "\n",
    "y_true          = np.array([  1,   1,   0,   1,    1,    1,    0,    0,    1,     0,   1,    0,    1,    0,    0,    0,    1,    0,    1,   0])\n",
    "y_probabilities = np.array([0.9, 0.8, 0.7, 0.6, 0.55, 0.54, 0.53, 0.52, 0.51, 0.505, 0.4, 0.39, 0.38, 0.37, 0.36, 0.35, 0.34, 0.33, 0.30, 0.1])\n",
    "\n",
    "# get false-positive rate, true-postive rate and thresholds\n",
    "FPR, TPR, thresholds = sklearn.metrics.roc_curve(y_true, y_probabilities)\n",
    "\n",
    "ROC_AUC = sklearn.metrics.auc(y_true, y_probabilities)\n",
    "\n",
    "plt.plot(FPR, TPR, label = \"ROC curve (area = {area})\".format(area = ROC_AUC))\n",
    "plt.plot([0, 1], [0, 1], \"k--\")  # random predictions curve\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.0])\n",
    "plt.xlabel(\"false positive rate\")\n",
    "plt.ylabel(\"true positive rate\"\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.legend(loc=\"lower right\")\n",
    "\n",
    "plt.xlabel(\"W\"); plt.ylabel(\"cost(W)\")\n",
    "plt.plot(W_value, cost_value)\n",
    "plt.axes().set_aspect(1 / plt.axes().get_data_ratio())\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# reading data\n",
    "\n",
    "There are three main methods to get data into a TensorFlow program:\n",
    "\n",
    "- Feeding: Python code provides the data when running each step.\n",
    "- Reading from files: an input pipeline reads the data from files at the beginning of a graph.\n",
    "- Preloaded data: a constant or variable in the graph holds all the data (for small datasets).\n",
    "\n",
    "## feeding\n",
    "\n",
    "The TensorFlow feed mechanism enables injection of data into any tensor in a computation graph. Thus, a Python computation can feed data directly into the graph. While a tensor can be replaced with feed data, including variables and constants, good practice is to use a placeholder operation node. A `placeholder` exists solely to serve as the target of feeds. It is not initialized and contains no data and it generates an error if it is executed without a feed.\n",
    "\n",
    "## reading from files\n",
    "\n",
    "A typical pipeline for reading records from files has the following stages:\n",
    "\n",
    "- the list of filenames,\n",
    "- optional filename shuffling,\n",
    "- optional epoch limit,\n",
    "- filename queue,\n",
    "- a reader for the file format,\n",
    "- a decoder for a record read by the reader,\n",
    "- optional preprocessing and\n",
    "- an example queue.\n",
    "\n",
    "### file formats\n",
    "\n",
    "Select the reader that matches the input file format and pass the filename queue to the reader's read method. The read method outputs a key identifying the file and record and a scalar string value. One or more of the decoder and conversion operations are used to decode this string into the tensors that make up an example.\n",
    "\n",
    "For CSV files, the `TextLineReader` is available.\n",
    "\n",
    "A recommended format for TensorFlow is a `TFRecords file`, for which the `TFRecordReader` is available.\n",
    "\n",
    "### loading CSV data from file\n",
    "\n",
    "Consider data in an ASCII file of the following CSV form:\n",
    "\n",
    "```\n",
    "73,80,75,152\n",
    "93,88,93,185\n",
    "89,91,90,180\n",
    "96,98,100,196\n",
    "73,66,70,142\n",
    "```\n",
    "\n",
    "This can be loaded na√Øvely into volatile memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "step: 0, cost: 51609.5585938,\n",
      "prediction:\n",
      "[[-63.79487228]\n",
      " [-67.67445374]\n",
      " [-71.40155029]\n",
      " [-78.36078644]\n",
      " [-48.58982849]\n",
      " [-35.12105179]\n",
      " [-61.62991333]\n",
      " [-50.24700165]\n",
      " [-61.59102249]\n",
      " [-57.17690277]\n",
      " [-56.07018661]\n",
      " [-51.35820007]\n",
      " [-73.14998627]\n",
      " [-59.25958252]\n",
      " [-60.58699036]\n",
      " [-70.00971222]\n",
      " [-52.90825653]\n",
      " [-76.68890381]\n",
      " [-71.02648163]\n",
      " [-64.50036621]\n",
      " [-71.00350952]\n",
      " [-64.46847534]\n",
      " [-68.2816925 ]\n",
      " [-62.86103439]\n",
      " [-71.41112518]]\n",
      "\n",
      "step: 500, cost: 26.2186908722,\n",
      "prediction:\n",
      "[[ 150.07359314]\n",
      " [ 188.46737671]\n",
      " [ 181.42796326]\n",
      " [ 197.22511292]\n",
      " [ 146.33998108]\n",
      " [ 108.84999084]\n",
      " [ 145.26107788]\n",
      " [ 104.00246429]\n",
      " [ 177.86880493]\n",
      " [ 165.06048584]\n",
      " [ 142.74584961]\n",
      " [ 144.36811829]\n",
      " [ 187.79936218]\n",
      " [ 157.26612854]\n",
      " [ 147.14537048]\n",
      " [ 190.22279358]\n",
      " [ 152.73558044]\n",
      " [ 171.96018982]\n",
      " [ 177.85375977]\n",
      " [ 158.32931519]\n",
      " [ 171.39477539]\n",
      " [ 176.11552429]\n",
      " [ 162.83569336]\n",
      " [ 152.17649841]\n",
      " [ 194.08108521]]\n",
      "\n",
      "step: 1000, cost: 21.7858181,\n",
      "prediction:\n",
      "[[ 150.3780365 ]\n",
      " [ 188.01315308]\n",
      " [ 181.38641357]\n",
      " [ 197.38243103]\n",
      " [ 145.62249756]\n",
      " [ 108.53780365]\n",
      " [ 145.91044617]\n",
      " [ 105.25130463]\n",
      " [ 177.50161743]\n",
      " [ 165.09494019]\n",
      " [ 142.91879272]\n",
      " [ 144.25445557]\n",
      " [ 187.5333252 ]\n",
      " [ 156.69847107]\n",
      " [ 147.69764709]\n",
      " [ 190.03111267]\n",
      " [ 151.69384766]\n",
      " [ 173.05796814]\n",
      " [ 177.69413757]\n",
      " [ 158.31214905]\n",
      " [ 171.96643066]\n",
      " [ 175.94018555]\n",
      " [ 163.39233398]\n",
      " [ 151.97184753]\n",
      " [ 193.6428833 ]]\n",
      "\n",
      "step: 1500, cost: 18.3485603333,\n",
      "prediction:\n",
      "[[ 150.65576172]\n",
      " [ 187.60858154]\n",
      " [ 181.35401917]\n",
      " [ 197.52157593]\n",
      " [ 144.98640442]\n",
      " [ 108.25173187]\n",
      " [ 146.48138428]\n",
      " [ 106.34899139]\n",
      " [ 177.16543579]\n",
      " [ 165.10324097]\n",
      " [ 143.06945801]\n",
      " [ 144.14411926]\n",
      " [ 187.30708313]\n",
      " [ 156.20980835]\n",
      " [ 148.18023682]\n",
      " [ 189.85543823]\n",
      " [ 150.78691101]\n",
      " [ 174.02487183]\n",
      " [ 177.56436157]\n",
      " [ 158.30769348]\n",
      " [ 172.46786499]\n",
      " [ 175.77868652]\n",
      " [ 163.88313293]\n",
      " [ 151.81015015]\n",
      " [ 193.25665283]]\n",
      "\n",
      "step: 2000, cost: 15.6812143326,\n",
      "prediction:\n",
      "[[ 150.90811157]\n",
      " [ 187.24864197]\n",
      " [ 181.32884216]\n",
      " [ 197.64463806]\n",
      " [ 144.42271423]\n",
      " [ 107.99082947]\n",
      " [ 146.98353577]\n",
      " [ 107.31414032]\n",
      " [ 176.85906982]\n",
      " [ 165.09289551]\n",
      " [ 143.20083618]\n",
      " [ 144.03869629]\n",
      " [ 187.11430359]\n",
      " [ 155.78840637]\n",
      " [ 148.60227966]\n",
      " [ 189.69520569]\n",
      " [ 149.99650574]\n",
      " [ 174.87670898]\n",
      " [ 177.45872498]\n",
      " [ 158.31233215]\n",
      " [ 172.9079895 ]\n",
      " [ 175.63076782]\n",
      " [ 164.31593323]\n",
      " [ 151.68257141]\n",
      " [ 192.91625977]]\n",
      "\n",
      "predictions\n",
      "\n",
      "input data: [[100, 70, 101]],\n",
      "score prediction:\n",
      "[[ 197.1804657]]\n",
      "\n",
      "input data: [[60, 70, 110], [90, 100, 80]],\n",
      "score prediction:\n",
      "[[ 161.40119934]\n",
      " [ 176.64434814]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "xy = np.loadtxt(\n",
    "    \"data.csv\",\n",
    "    delimiter = \",\",\n",
    "    dtype     = np.float32\n",
    ")\n",
    "x_data = xy[:, 0:-1]\n",
    "y_data = xy[:, [-1]]\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=[None, 3])\n",
    "Y = tf.placeholder(tf.float32, shape=[None, 1])\n",
    "\n",
    "W = tf.Variable(tf.random_normal([3, 1]), name = \"weight\")\n",
    "b = tf.Variable(tf.random_normal([1]),    name = \"bias\"  )\n",
    "\n",
    "hypothesis = tf.matmul(X, W) + b\n",
    "cost       = tf.reduce_mean(tf.square(hypothesis - Y))\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=1e-5)\n",
    "train     = optimizer.minimize(cost)\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for step in range(2001):\n",
    "    cost_value, hy_value, _ = sess.run(\n",
    "        [cost, hypothesis, train],\n",
    "        feed_dict = {X: x_data, Y: y_data}\n",
    "    )\n",
    "    if step % 500 == 0:\n",
    "        print(\"\\nstep: {step}, cost: {cost},\\nprediction:\\n{prediction}\".format(\n",
    "            step       = step,\n",
    "            cost       = cost_value,\n",
    "            prediction = hy_value\n",
    "        ))\n",
    "\n",
    "print(\"\\npredictions\")\n",
    "test_x_data = [\n",
    "                  [100, 70, 101]\n",
    "              ]\n",
    "result = sess.run(hypothesis, feed_dict = {X: test_x_data})\n",
    "print(\"\\ninput data: {data},\\nscore prediction:\\n{prediction}\".format(\n",
    "    data       = test_x_data,\n",
    "    prediction = result\n",
    "))\n",
    "\n",
    "test_x_data = [\n",
    "                  [60, 70, 110],\n",
    "                  [90, 100, 80]\n",
    "              ]\n",
    "result = sess.run(hypothesis, feed_dict = {X: test_x_data})\n",
    "print(\"\\ninput data: {data},\\nscore prediction:\\n{prediction}\".format(\n",
    "    data       = test_x_data,\n",
    "    prediction = result\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# preprocessing\n",
    "\n",
    "Preprocessing could involve normalization of data, selecting a random slice, adding noise and distortions etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# threading and queues\n",
    "\n",
    "Queues are a mechanism for asynchronous computation using TensorFlow. Like everything in TensorFlow, a queue is a node in a TensorFlow graph. It is a stateful node, like a variable: other nodes can modify its content. In particular, nodes can enqueue new items to the queue or dequeue existing items from the queue.\n",
    "\n",
    "Queues such as `FIFOQueue` and `RandomShuffleQueue` are important TensorFlow objects for computing tensors asynchronously in a graph. For example, a typical input architecture is to use a RandomShuffleQueue to prepare inputs for training a model:\n",
    "\n",
    "- Multiple threads prepare training examples and push them to the queue.\n",
    "- A training thread executes a training operation that dequeues mini-batches from the queue.\n",
    "\n",
    "The TensorFlow `Session` object is multithreaded, so multiple threads can use the same session and run operations in parallel. However, it is not always easy to implement a Python program that drives threads as described. All threads must be able to stop together, exceptions must be captured and reported and queues should be closed when stopping.\n",
    "\n",
    "TensorFlow provides two classes to help: `tf.Coordinator` and `tf.QueueRunner`. The `Coordinator` class helps multiple threads to stop together and report exceptions to a program that waits for them to stop. The `QueueRunner` class is used to create a number of threads cooperating to enqueue tensors in the same queue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "filename_queue = tf.train.string_input_producer(\n",
    "    [\"data.csv\"],\n",
    "    shuffle = False,\n",
    "    name    = \"filename_queue\")\n",
    "\n",
    "reader     = tf.TextLineReader()\n",
    "key, value = reader.read(filename_queue)\n",
    "\n",
    "# Set default values for empty columns and specify the decoded result type.\n",
    "xy = tf.decode_csv(\n",
    "    value,\n",
    "    record_defaults = [[0.], [0.], [0.], [0.]]\n",
    ")\n",
    "\n",
    "# Collect batches of CSV.\n",
    "train_x_batch, train_y_batch =\\\n",
    "    tf.train.batch(\n",
    "        [xy[0:-1], xy[-1:]],\n",
    "        batch_size = 10\n",
    "    )\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape = [None, 3])\n",
    "Y = tf.placeholder(tf.float32, shape = [None, 1])\n",
    "\n",
    "W = tf.Variable(tf.random_normal([3, 1]), name = \"weight\")\n",
    "b = tf.Variable(tf.random_normal([1]),    name = \"bias\")\n",
    "\n",
    "hypothesis = tf.matmul(X, W) + b\n",
    "cost       = tf.reduce_mean(tf.square(hypothesis - Y))\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=1e-5)\n",
    "train     = optimizer.minimize(cost)\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# Start populating the filename queue.\n",
    "coord   = tf.train.Coordinator()\n",
    "threads = tf.train.start_queue_runners(\n",
    "              sess  = sess,\n",
    "              coord = coord\n",
    "          )\n",
    "\n",
    "for step in range(2001):\n",
    "    x_batch, y_batch = sess.run([train_x_batch, train_y_batch])\n",
    "    cost_value, hy_value, _ = sess.run(\n",
    "        [cost, hypothesis, train],\n",
    "        feed_dict = {X: x_batch, Y: y_batch}\n",
    "    )\n",
    "    if step % 500 == 0:\n",
    "        print(\"\\nstep: {step}, cost: {cost},\\nprediction:\\n{prediction}\".format(\n",
    "            step       = step,\n",
    "            cost       = cost_value,\n",
    "            prediction = hy_value\n",
    "        ))\n",
    "\n",
    "coord.request_stop()\n",
    "coord.join(threads)\n",
    "\n",
    "print(\"\\npredictions\")\n",
    "test_x_data = [\n",
    "                  [100, 70, 101]\n",
    "              ]\n",
    "result = sess.run(hypothesis, feed_dict = {X: test_x_data})\n",
    "print(\"\\ninput data: {data},\\nscore prediction:\\n{prediction}\".format(\n",
    "    data       = test_x_data,\n",
    "    prediction = result\n",
    "))\n",
    "\n",
    "test_x_data = [\n",
    "                  [60, 70, 110],\n",
    "                  [90, 100, 80]\n",
    "              ]\n",
    "result = sess.run(hypothesis, feed_dict = {X: test_x_data})\n",
    "print(\"\\ninput data: {data},\\nscore prediction:\\n{prediction}\".format(\n",
    "    data       = test_x_data,\n",
    "    prediction = result\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# logistic regression\n",
    "\n",
    "- hypothesis: ${H\\left(X\\right)=\\frac{1}{1+e^{-W^{T}X}}}$\n",
    "- ${\\textrm{cost}\\left(W\\right)=-\\frac{1}{m}\\Sigma y \\log\\left(H\\left(x\\right)\\right)+\\left(1-y\\right)\\left(\\log\\left(1-H\\left(x\\right)\\right)\\right)}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "step: 0, cost: 2.01184892654\n",
      "\n",
      "step: 1000, cost: 0.468353033066\n",
      "\n",
      "step: 2000, cost: 0.393124341965\n",
      "\n",
      "step: 3000, cost: 0.335385560989\n",
      "\n",
      "step: 4000, cost: 0.290337324142\n",
      "\n",
      "step: 5000, cost: 0.254872947931\n",
      "\n",
      "step: 6000, cost: 0.226582095027\n",
      "\n",
      "step: 7000, cost: 0.203673005104\n",
      "\n",
      "step: 8000, cost: 0.184841319919\n",
      "\n",
      "step: 9000, cost: 0.169140636921\n",
      "\n",
      "step: 10000, cost: 0.155879363418\n",
      "\n",
      "accuracy report:\n",
      "\n",
      "hypothesis:\n",
      "\n",
      "[[ 0.03347061]\n",
      " [ 0.16238944]\n",
      " [ 0.31767666]\n",
      " [ 0.7755906 ]\n",
      " [ 0.93586653]\n",
      " [ 0.97893244]]\n",
      "\n",
      "correct (Y):\n",
      "\n",
      "[[ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]]\n",
      "\n",
      "accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "x_data = [\n",
    "             [1, 2],\n",
    "             [2, 3],\n",
    "             [3, 1],\n",
    "             [4, 3],\n",
    "             [5, 3],\n",
    "             [6, 2]\n",
    "         ]\n",
    "y_data = [\n",
    "             [0],\n",
    "             [0],\n",
    "             [0],\n",
    "             [1],\n",
    "             [1],\n",
    "             [1]\n",
    "         ]\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape = [None, 2])\n",
    "Y = tf.placeholder(tf.float32, shape = [None, 1])\n",
    "\n",
    "W = tf.Variable(tf.random_normal([2, 1]), name = \"weight\")\n",
    "b = tf.Variable(tf.random_normal([1   ]), name = \"bias\"  )\n",
    "\n",
    "# hypothesis using sigmoid: tf.div(1., 1. + tf.exp(tf.matmul(X, W)))\n",
    "hypothesis = tf.sigmoid(tf.matmul(X, W) + b)\n",
    "cost       = -tf.reduce_mean(Y * tf.log(hypothesis) + (1 - Y) * tf.log(1 - hypothesis))\n",
    "\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate = 0.01).minimize(cost)\n",
    "\n",
    "# accuracy computation: true if hypothesis > 0.5 else false\n",
    "predicted = tf.cast(hypothesis > 0.5, dtype = tf.float32)\n",
    "accuracy  = tf.reduce_mean(tf.cast(tf.equal(predicted, Y), dtype = tf.float32))\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for step in range(10001):\n",
    "    cost_value, _ = sess.run(\n",
    "        [cost, train],\n",
    "        feed_dict = {X: x_data, Y: y_data}\n",
    "    )\n",
    "    if step % 1000 == 0:\n",
    "        print(\"\\nstep: {step}, cost: {cost}\".format(\n",
    "            step       = step,\n",
    "            cost       = cost_value\n",
    "        ))\n",
    "\n",
    "print(\"\\naccuracy report:\")\n",
    "h, c, a = sess.run(\n",
    "    [hypothesis, predicted, accuracy],\n",
    "    feed_dict = {X: x_data, Y: y_data}\n",
    ")\n",
    "print(\"\\nhypothesis:\\n\\n{hypothesis}\\n\\ncorrect (Y):\\n\\n{correct}\\n\\naccuracy: {accuracy}\".format(\n",
    "    hypothesis = h,\n",
    "    correct    = c,\n",
    "    accuracy   = a\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# example: multiple variables (for classification) with final column as classification\n",
    "\n",
    "The input is a CSV file with the first line of the file containing headers and the rest of the lines containing data. The rightmost column of data is the class (0 or 1) and the other columns are feature values of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features data shape: (2000, 20)\n",
      "class data shape:    (2000, 1)\n",
      "number of features:  20\n",
      "\n",
      "step: 0, cost: 1.09629881382\n",
      "step: 1000, cost: 0.728394389153\n",
      "step: 2000, cost: 0.656880319118\n",
      "step: 3000, cost: 0.618762850761\n",
      "step: 4000, cost: 0.599011480808\n",
      "step: 5000, cost: 0.588777184486\n",
      "\n",
      "accuracy report (testing trained system on training data):\n",
      "\n",
      "hypothesis:\n",
      "\n",
      "[[ 0.56674618]\n",
      " [ 0.91476625]\n",
      " [ 0.66587365]\n",
      " ..., \n",
      " [ 0.42087814]\n",
      " [ 0.14799729]\n",
      " [ 0.40271857]]\n",
      "\n",
      "correct (Y):\n",
      "\n",
      "[[ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " ..., \n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]]\n",
      "\n",
      "accuracy: 0.70450001955\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "xy = np.loadtxt(\n",
    "    \"output_preprocessed.csv\",\n",
    "    skiprows  = 1,\n",
    "    delimiter = \",\",\n",
    "    dtype     = np.float32\n",
    ")\n",
    "x_data = xy[:, 0:-1]\n",
    "y_data = xy[:, [-1]]\n",
    "\n",
    "number_of_features = x_data.shape[1]\n",
    "\n",
    "print(\"features data shape: \" + str(x_data.shape))\n",
    "print(\"class data shape:    \" + str(y_data.shape))\n",
    "print(\"number of features:  \" + str(number_of_features))\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=[None, number_of_features])\n",
    "Y = tf.placeholder(tf.float32, shape=[None, 1])\n",
    "\n",
    "W = tf.Variable(tf.random_normal([number_of_features, 1]), name = \"weight\")\n",
    "b = tf.Variable(tf.random_normal([1]                    ), name = \"bias\"  )\n",
    "\n",
    "hypothesis = tf.sigmoid(tf.matmul(X, W) + b)\n",
    "cost       = -tf.reduce_mean(Y * tf.log(hypothesis) + (1 - Y) * tf.log(1 - hypothesis))\n",
    "train      = tf.train.GradientDescentOptimizer(learning_rate = 0.01).minimize(cost)\n",
    "\n",
    "# accuracy computation: true if hypothesis > 0.5 else false\n",
    "predicted = tf.cast(hypothesis > 0.5, dtype = tf.float32)\n",
    "accuracy  = tf.reduce_mean(tf.cast(tf.equal(predicted, Y), dtype = tf.float32))\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "print(\"\")\n",
    "for step in range(5001):\n",
    "    cost_value, _ = sess.run(\n",
    "        [cost, train],\n",
    "        feed_dict = {X: x_data, Y: y_data})\n",
    "    if step % 1000 == 0:\n",
    "        print(\"step: {step}, cost: {cost}\".format(\n",
    "            step       = step,\n",
    "            cost       = cost_value\n",
    "        ))\n",
    "\n",
    "print(\"\\naccuracy report (testing trained system on training data):\")\n",
    "h, c, a = sess.run(\n",
    "    [hypothesis, predicted, accuracy],\n",
    "    feed_dict = {X: x_data, Y: y_data}\n",
    ")\n",
    "print(\"\\nhypothesis:\\n\\n{hypothesis}\\n\\ncorrect (Y):\\n\\n{correct}\\n\\naccuracy: {accuracy}\".format(\n",
    "    hypothesis = h,\n",
    "    correct    = c,\n",
    "    accuracy   = a\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# one-hot encoding\n",
    "\n",
    "Often features in data are not continuous values, but categorical. For example, a person could be classed as \"male\" or \"female\" or a nationality could be classed as \"French\", \"Swiss\" and \"Irish\". For the example of nationality, the values could be encoded as 0, 1 and 2. One-hot encoding effectively blows up the feature space to three features instead of the one feature of nationality. So, conceptually, the new features could be the booleans \"is French\", \"is Swiss\" and \"is Irish\".\n",
    "\n",
    "So, basically, one-hot encoding involves changing ordinal encoding (assigning a different number to each category) to binary encoding.\n",
    "\n",
    "A motivation for this is for many machine learning algorithms. Some algorithms, such as random forests, handly categorical values natively.\n",
    "\n",
    "example: A sample of creatures could contain humans, penguins, octopuses and aliens. Each type of creature could be labelled with an ordinal number (e.g. 1 for human, 2 for penguin and so on).\n",
    "\n",
    "|**sample**|**category**|**numerical**|\n",
    "|----------|------------|-------------|\n",
    "|1         |human       |1            |\n",
    "|2         |human       |1            |\n",
    "|3         |penguin     |2            |\n",
    "|4         |octopus     |3            |\n",
    "|5         |alien       |4            |\n",
    "|6         |octopus     |3            |\n",
    "|7         |alien       |4            |\n",
    "\n",
    "One-hot encoding this data involves generating ne boolean column for each category.\n",
    "\n",
    "|**sample**|**human**|**penguin**|**octopus**|**alien**|\n",
    "|----------|---------|-----------|-----------|---------|\n",
    "|1         |1        |0          |0          |0        |\n",
    "|2         |1        |0          |0          |0        |\n",
    "|3         |0        |1          |0          |0        |\n",
    "|4         |0        |0          |1          |0        |\n",
    "|5         |0        |0          |0          |1        |\n",
    "|6         |0        |0          |0          |0        |\n",
    "|7         |0        |0          |0          |1        |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "raw data:\n",
      "\n",
      "   A  B\n",
      "0  a  b\n",
      "1  b  a\n",
      "2  a  c\n",
      "\n",
      "data with column B encoded:\n",
      "\n",
      "   A  a  b  c\n",
      "0  a  0  1  0\n",
      "1  b  1  0  0\n",
      "2  a  0  0  1\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.DataFrame({\n",
    "                        \"A\": [\"a\", \"b\", \"a\"],\n",
    "                        \"B\": [\"b\", \"a\", \"c\"]\n",
    "                    })\n",
    "\n",
    "print(\"\\nraw data:\\n\")\n",
    "print(data)\n",
    "\n",
    "# Get one-hot encoding of column B.\n",
    "one_hot = pd.get_dummies(data[\"B\"])\n",
    "\n",
    "# Drop column B as it is now encoded.\n",
    "data = data.drop(\"B\", axis = 1)\n",
    "\n",
    "# Join the B encoding.\n",
    "data = data.join(one_hot)\n",
    "\n",
    "print(\"\\ndata with column B encoded:\\n\")\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# softmax\n",
    "\n",
    "The softmax function or normalized exponential function is a generalization of the logistic function that 'squashes' a vector of arbitrary real values to the range (0, 1) that sum up to 1. Conceptually, this could mean changing scores to probabilities. Softmax regression models generalize logistic regression to classification problems in which the labels can have more than two possible values.\n",
    "\n",
    "So, the ${K}$-dimensional vector ${\\vec{z}}$ of arbitrary real values is changed to the ${K}$-dimensional vector ${\\sigma\\left(\\vec{z}\\right)}$ of real values in the range (0, 1) that sum up to 1:\n",
    "\n",
    "$${\n",
    "\\sigma\\left(\\vec{z}\\right)_{j}=\\frac{e^{z_{j}}}{\\sum_{k=1}^{K} e^{z_{k}}}\\textrm{ for }j=1,...,K\n",
    "}$$\n",
    "\n",
    "For example, softmax changes vector\n",
    "\n",
    "```Python\n",
    "[1, 2, 3, 4, 1, 2, 3]\n",
    "```\n",
    "\n",
    "to the following vector:\n",
    "\n",
    "```Python\n",
    "[0.024, 0.064, 0.175, 0.475, 0.024, 0.064, 0.175]\n",
    "```\n",
    "\n",
    "In NumPy, it could be implemented in the following way:\n",
    "\n",
    "```Python\n",
    "def softmax(z):\n",
    "    return np.exp(z) / np.sum(np.exp(z))\n",
    "```\n",
    "\n",
    "Changing from sigmoid activation to softmax activation could involve changing from\n",
    "\n",
    "```Python\n",
    "hypothesis = tf.sigmoid(tf.matmul(X, W) + b)\n",
    "```\n",
    "\n",
    "to the following:\n",
    "\n",
    "```Python\n",
    "hypothesis = tf.nn.softmax(tf.matmul(X, W) + b)\n",
    "```\n",
    "\n",
    "The cost function for softmax is cross-entropy.\n",
    "\n",
    "# cross-entropy\n",
    "\n",
    "Cross-entropy is used commonly to describe the difference between two probability distributions. Often, the \"true\" distribution (the one that the machine learning algorithm is trying to match) is expressed in terms of a one-hot distribution.\n",
    "\n",
    "Consider a specific training instance for which the class label is B, out of the possible class labels A, B and C. The one-hot distribution for this training instance is as follows:\n",
    "\n",
    "|**p(class A)**|**p(class B)**|**p(class C)**|\n",
    "|--------------|--------------|--------------|\n",
    "|0.0           |1.0           |0.0           |\n",
    "\n",
    "This \"true\" distribution can be interpreted to mean that the training instance has 0% probability of being class A, 100% probability of being class B and 0% probability of being class C.\n",
    "\n",
    "Suppose a machine learning algorithm predicts the following probability distribution:\n",
    "\n",
    "|**p(class A)**|**p(class B)**|**p(class C)**|\n",
    "|--------------|--------------|--------------|\n",
    "|0.228         |0.619         |0.153         |\n",
    "\n",
    "How close is the predicted distribution to the \"true\" distribution? This is what cross-entropy loss determines.\n",
    "\n",
    "$${\n",
    "H\\left(q,p\\right)=\\sum_{x}p\\left(x\\right)\\log q\\left(x\\right)\n",
    "}$$\n",
    "\n",
    "The sum is over the three classes A, B and C. The loss is calculated as 0.479. This is a measure of the error of the prediction.\n",
    "\n",
    "Cross-entropy is one of many possible loss functions (another popular one is SVM hinge loss). They are written typically as ${J\\left(\\theta\\right)}$ and can be used within gradient descent, which is an iterative framework of moving the parameters (or coefficients) towards the optimum values. In the following equation ${J\\left(\\theta\\right)}$ is replaced with ${H\\left(p, q\\right)}$. Note that the derivative of ${H\\left(p, q\\right)}$ with respect to the parameters must be computed.\n",
    "\n",
    "$${\n",
    "\\textrm{repeat until convergence: } \\theta_{j}\\leftarrow\\theta_{j}-\\alpha\\frac{\\partial}{\\partial\\theta_{j}}J\\left(\\theta\\right)\n",
    "}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# example: data classified with one-hot encoding\n",
    "\n",
    "In this example, the function `tf.argmax` returns the index with the largest value across axes of a tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 0, cost: 6.15176486969\n",
      "step: 200, cost: 0.693853735924\n",
      "step: 400, cost: 0.584099948406\n",
      "step: 600, cost: 0.49236613512\n",
      "step: 800, cost: 0.403734862804\n",
      "step: 1000, cost: 0.315236628056\n",
      "step: 1200, cost: 0.241405770183\n",
      "step: 1400, cost: 0.218246951699\n",
      "step: 1600, cost: 0.199564412236\n",
      "step: 1800, cost: 0.183692246675\n",
      "step: 2000, cost: 0.170054793358\n",
      "\n",
      "testing\n",
      "\n",
      "------------------------------\n",
      "result:\n",
      "\n",
      "[[  7.33173117e-02   9.26671445e-01   1.12322605e-05]]\n",
      "\n",
      "argmax: [1]\n",
      "------------------------------\n",
      "result:\n",
      "\n",
      "[[ 0.55566663  0.39402479  0.05030856]]\n",
      "\n",
      "argmax: [0]\n",
      "------------------------------\n",
      "result:\n",
      "\n",
      "[[  2.42859510e-08   4.52565990e-04   9.99547422e-01]]\n",
      "\n",
      "argmax: [2]\n",
      "------------------------------\n",
      "result:\n",
      "\n",
      "[[  7.33173117e-02   9.26671445e-01   1.12322605e-05]\n",
      " [  5.55666625e-01   3.94024789e-01   5.03085628e-02]\n",
      " [  2.42859528e-08   4.52565990e-04   9.99547422e-01]]\n",
      "\n",
      "argmax: [1 0 2]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "x_data = [\n",
    "             [1, 2, 1, 1],\n",
    "             [2, 1, 3, 2],\n",
    "             [3, 1, 3, 4],\n",
    "             [4, 1, 5, 5],\n",
    "             [1, 7, 5, 5],\n",
    "             [1, 2, 5, 6],\n",
    "             [1, 6, 6, 6],\n",
    "             [1, 7, 7, 7]\n",
    "         ]\n",
    "y_data = [\n",
    "             [0, 0, 1],\n",
    "             [0, 0, 1],\n",
    "             [0, 0, 1],\n",
    "             [0, 1, 0],\n",
    "             [0, 1, 0],\n",
    "             [0, 1, 0],\n",
    "             [1, 0, 0],\n",
    "             [1, 0, 0]\n",
    "         ]\n",
    "\n",
    "X = tf.placeholder(\"float\", [None, 4])\n",
    "Y = tf.placeholder(\"float\", [None, 3])\n",
    "number_of_classes = 3\n",
    "\n",
    "W = tf.Variable(tf.random_normal([4, number_of_classes]), name = \"weight\")\n",
    "b = tf.Variable(tf.random_normal([number_of_classes]   ), name = \"bias\"  )\n",
    "\n",
    "# tf.nn.softmax computes softmax activations\n",
    "# softmax = exp(logits) / reduce_sum(exp(logits), dim)\n",
    "hypothesis = tf.nn.softmax(tf.matmul(X, W) + b)\n",
    "cost       = tf.reduce_mean(-tf.reduce_sum(Y * tf.log(hypothesis), axis = 1))\n",
    "optimizer  = tf.train.GradientDescentOptimizer(learning_rate = 0.1).minimize(cost)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for step in range(2001):\n",
    "        sess.run(\n",
    "            optimizer,\n",
    "            feed_dict = {X: x_data, Y: y_data}\n",
    "        )\n",
    "        if step % 200 == 0:\n",
    "            print(\"step: {step}, cost: {cost}\".format(\n",
    "                step       = step,\n",
    "                cost       = sess.run(cost, feed_dict = {X: x_data, Y: y_data})\n",
    "            ))\n",
    "\n",
    "    print(\"\\ntesting\\n\")\n",
    "\n",
    "    print(\"------------------------------\")\n",
    "\n",
    "    result = sess.run(\n",
    "                 hypothesis,\n",
    "                 feed_dict = {\n",
    "                                 X: [[1, 11, 7, 9]]\n",
    "                             }\n",
    "             )\n",
    "    print(\"result:\\n\\n{result}\\n\\nargmax: {argmax}\".format(\n",
    "        result = result,\n",
    "        argmax = sess.run(tf.argmax(result, 1))\n",
    "    ))\n",
    "\n",
    "    print(\"------------------------------\")\n",
    "\n",
    "    result = sess.run(\n",
    "                 hypothesis,\n",
    "                 feed_dict = {\n",
    "                                 X: [[1, 3, 4, 3]]\n",
    "                             }\n",
    "             )\n",
    "    print(\"result:\\n\\n{result}\\n\\nargmax: {argmax}\".format(\n",
    "        result = result,\n",
    "        argmax = sess.run(tf.argmax(result, 1))\n",
    "    ))\n",
    "\n",
    "    print(\"------------------------------\")\n",
    "\n",
    "    result = sess.run(\n",
    "                 hypothesis,\n",
    "                 feed_dict = {\n",
    "                                 X: [[1, 1, 0, 1]]\n",
    "                             }\n",
    "             )\n",
    "    print(\"result:\\n\\n{result}\\n\\nargmax: {argmax}\".format(\n",
    "        result = result,\n",
    "        argmax = sess.run(tf.argmax(result, 1))\n",
    "    ))\n",
    "\n",
    "    print(\"------------------------------\")\n",
    "\n",
    "    result = sess.run(\n",
    "                 hypothesis,\n",
    "                 feed_dict = {\n",
    "                                 X: [\n",
    "                                        [1, 11,  7,  9],\n",
    "                                        [1,  3,  4,  3],\n",
    "                                        [1,  1,  0,  1]\n",
    "                                    ]\n",
    "                             }\n",
    "             )\n",
    "    print(\"result:\\n\\n{result}\\n\\nargmax: {argmax}\".format(\n",
    "        result = result,\n",
    "        argmax = sess.run(tf.argmax(result, 1))\n",
    "    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# training and test datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# example: learning rate\n",
    "\n",
    "The learning rate in the following example can be changed. An overly large learning rate means that there is overshooting of minima in the ${J\\left(w\\right)}$ versus ${w}$ graph while an overly small learning rate means that there are many iterations until convergence and there can be trapping in local minima."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "training\n",
      "\n",
      "step: 0,\n",
      "cost: 8.31644821167,\n",
      "W:\n",
      "[[ 0.22195008  0.38584882 -0.53195298]\n",
      " [ 1.21279347 -1.07557487 -0.14719149]\n",
      " [ 0.55527294 -0.24748784  1.24971259]]\n",
      "\n",
      "step: 50,\n",
      "cost: 1.05112886429,\n",
      "W:\n",
      "[[-0.20881306  0.00115683  0.28350213]\n",
      " [ 0.27693221 -0.26255345 -0.02435144]\n",
      " [ 0.49746183  0.57809454  0.48194173]]\n",
      "\n",
      "step: 100,\n",
      "cost: 0.781579613686,\n",
      "W:\n",
      "[[-0.46970949 -0.34654671  0.89210224]\n",
      " [ 0.14641295 -0.16828226  0.01189673]\n",
      " [ 0.73768389  0.6358996   0.18391465]]\n",
      "\n",
      "step: 150,\n",
      "cost: 0.671505689621,\n",
      "W:\n",
      "[[-0.69827598 -0.52357417  1.29769611]\n",
      " [ 0.12296849 -0.11011682 -0.02282415]\n",
      " [ 0.85604739  0.66157854  0.03987237]]\n",
      "\n",
      "step: 200,\n",
      "cost: 0.611969947815,\n",
      "W:\n",
      "[[-0.90268207 -0.61539346  1.59392083]\n",
      " [ 0.12702149 -0.07612278 -0.06087111]\n",
      " [ 0.93543333  0.67505574 -0.05299084]]\n",
      "\n",
      "testing\n",
      "\n",
      "predictions:\n",
      "\n",
      "[2 2 2]\n",
      "\n",
      "accuracy:\n",
      "\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# training dataset\n",
    "\n",
    "x_data = [\n",
    "             [1, 2, 1],\n",
    "             [1, 3, 2],\n",
    "             [1, 3, 4],\n",
    "             [1, 5, 5],\n",
    "             [1, 7, 5],\n",
    "             [1, 2, 5],\n",
    "             [1, 6, 6],\n",
    "             [1, 7, 7]\n",
    "         ]\n",
    "y_data = [\n",
    "             [0, 0, 1],\n",
    "             [0, 0, 1],\n",
    "             [0, 0, 1],\n",
    "             [0, 1, 0],\n",
    "             [0, 1, 0],\n",
    "             [0, 1, 0],\n",
    "             [1, 0, 0],\n",
    "             [1, 0, 0]\n",
    "         ]\n",
    "\n",
    "# test dataset\n",
    "\n",
    "x_test = [\n",
    "             [2, 1, 1],\n",
    "             [3, 1, 2],\n",
    "             [3, 3, 4]\n",
    "         ]\n",
    "y_test = [\n",
    "             [0, 0, 1],\n",
    "             [0, 0, 1],\n",
    "             [0, 0, 1]\n",
    "         ]\n",
    "\n",
    "X = tf.placeholder(\"float\", [None, 3])\n",
    "Y = tf.placeholder(\"float\", [None, 3])\n",
    "\n",
    "W = tf.Variable(tf.random_normal([3, 3]))\n",
    "b = tf.Variable(tf.random_normal([3]   ))\n",
    "\n",
    "hypothesis = tf.nn.softmax(tf.matmul(X, W) + b)\n",
    "cost       = tf.reduce_mean(-tf.reduce_sum(Y * tf.log(hypothesis), axis = 1))\n",
    "\n",
    "#learning_rate = 1.5\n",
    "#learning_rate = 1e-10\n",
    "learning_rate = 0.1\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate = learning_rate).minimize(cost)\n",
    "\n",
    "prediction = tf.argmax(hypothesis, 1)\n",
    "is_correct = tf.equal(prediction, tf.argmax(Y, 1))\n",
    "accuracy   = tf.reduce_mean(tf.cast(is_correct, tf.float32))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    print(\"\\ntraining\")\n",
    "\n",
    "    for step in range(201):\n",
    "        cost_value, W_value, _ = sess.run(\n",
    "            [cost, W, optimizer],\n",
    "            feed_dict = {X: x_data, Y: y_data}\n",
    "        )\n",
    "        if step % 50 == 0:\n",
    "            print(\"\\nstep: {step},\\ncost: {cost},\\nW:\\n{W}\".format(\n",
    "                step = step,\n",
    "                cost = cost_value,\n",
    "                W    = W_value\n",
    "            ))\n",
    "\n",
    "    print(\"\\ntesting\\n\")\n",
    "\n",
    "    result   = sess.run(prediction, feed_dict = {X: x_test}           )\n",
    "    accuracy = sess.run(accuracy,   feed_dict = {X: x_test, Y: y_test})\n",
    "\n",
    "    print(\"predictions:\\n\\n{result}\\n\\naccuracy:\\n\\n{accuracy}\".format(\n",
    "        result   = result,\n",
    "        accuracy = accuracy\n",
    "    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# training epoch/batch\n",
    "\n",
    "- **epoch**: one forward pass and one backward pass of all training examples\n",
    "- **batch size**: the number of training examples in one forward/backward pass -- The greater the batch size, the greater volatile memory is needed.\n",
    "- **iterations**: the number of passes, each pass using the batch size number of examples\n",
    "\n",
    "So, for 1000 training examples with a batch size of 500, it takes 2 iterations to complete 1 epoch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST database\n",
    "\n",
    "The MNIST (Mixed National Institute of Standards and Technology) dataset is a dataset of handwritten digits, comprising 60,000 training examples and 10,000 test examples.\n",
    "\n",
    "The database can be downloaded in the following way:\n",
    "\n",
    "```Python\n",
    "import tensorflow.examples.tutorials.mnist\n",
    "mnist = tensorflow.examples.tutorials.mnist.input_data.read_data_sets(\n",
    "    \"MNIST_data/\",\n",
    "    one_hot = True\n",
    ")\n",
    "```\n",
    "\n",
    "|**archive**               |**content**        |\n",
    "|--------------------------|-------------------|\n",
    "|t10k-images-idx3-ubyte.gz |training set images|\n",
    "|t10k-labels-idx1-ubyte.gz |training set labels|\n",
    "|train-images-idx3-ubyte.gz|test set images    |\n",
    "|train-labels-idx1-ubyte.gz|test set labels    |\n",
    "\n",
    "The data can be made accessable in a way like the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQFCEHwiK8ANB/T9ZoGRpZTRCfQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f1d5125ec10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label (extracted by NumPy):      [5]\n",
      "label (extracted by TensorFlow): [5]\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import tensorflow.examples.tutorials.mnist\n",
    "import numpy as np\n",
    "\n",
    "mnist = tensorflow.examples.tutorials.mnist.input_data.read_data_sets(\n",
    "    \"MNIST_data/\",\n",
    "    one_hot = True\n",
    ")\n",
    "\n",
    "# access some image (of some index number)\n",
    "# access the (one-hot) class label of the image\n",
    "index = 15\n",
    "image = mnist.test.images[index].reshape(28, 28)\n",
    "label = mnist.test.labels[index:index + 1]\n",
    "\n",
    "plt.imshow(\n",
    "    image,\n",
    "    cmap          = \"Greys\",\n",
    "    interpolation = \"nearest\"\n",
    ")\n",
    "plt.show()\n",
    "\n",
    "print(\"label (extracted by NumPy):      {label}\".format(\n",
    "      label = np.where(label[0] == 1)[0]))\n",
    "\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "print(\"label (extracted by TensorFlow): {label}\".format(\n",
    "      label = sess.run(tf.argmax(label, 1))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, the form of the data is a simple NumPy array that van be represented as an image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQFCEHwiK8ANB/T9ZoGRpZTRCfQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f1d846c9350>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "image_array =\\\n",
    "np.array(\n",
    "      [[ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
    "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
    "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
    "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
    "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
    "         0.        ,  0.        ,  0.        ],\n",
    "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
    "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
    "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
    "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
    "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
    "         0.        ,  0.        ,  0.        ],\n",
    "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
    "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
    "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
    "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
    "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
    "         0.        ,  0.        ,  0.        ],\n",
    "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
    "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
    "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
    "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
    "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
    "         0.        ,  0.        ,  0.        ],\n",
    "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
    "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
    "         0.        ,  0.        ,  0.20000002,  0.51764709,  0.83921576,\n",
    "         0.99215692,  0.99607849,  0.99215692,  0.7960785 ,  0.63529414,\n",
    "         0.16078432,  0.        ,  0.        ,  0.        ,  0.        ,\n",
    "         0.        ,  0.        ,  0.        ],\n",
    "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
    "         0.        ,  0.        ,  0.        ,  0.40000004,  0.55686277,\n",
    "         0.7960785 ,  0.7960785 ,  0.99215692,  0.98823535,  0.99215692,\n",
    "         0.98823535,  0.59215689,  0.27450982,  0.        ,  0.        ,\n",
    "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
    "         0.        ,  0.        ,  0.        ],\n",
    "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
    "         0.        ,  0.        ,  0.        ,  0.99607849,  0.99215692,\n",
    "         0.95686281,  0.7960785 ,  0.55686277,  0.40000004,  0.32156864,\n",
    "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
    "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
    "         0.        ,  0.        ,  0.        ],\n",
    "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
    "         0.        ,  0.        ,  0.        ,  0.67450982,  0.98823535,\n",
    "         0.7960785 ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
    "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
    "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
    "         0.        ,  0.        ,  0.        ],\n",
    "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
    "         0.        ,  0.        ,  0.        ,  0.08235294,  0.87450987,\n",
    "         0.91764712,  0.11764707,  0.        ,  0.        ,  0.        ,\n",
    "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
    "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
    "         0.        ,  0.        ,  0.        ],\n",
    "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
    "         0.        ,  0.        ,  0.        ,  0.        ,  0.4784314 ,\n",
    "         0.99215692,  0.19607845,  0.        ,  0.        ,  0.        ,\n",
    "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
    "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
    "         0.        ,  0.        ,  0.        ],\n",
    "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
    "         0.        ,  0.        ,  0.        ,  0.        ,  0.48235297,\n",
    "         0.99607849,  0.35686275,  0.20000002,  0.20000002,  0.20000002,\n",
    "         0.03921569,  0.        ,  0.        ,  0.        ,  0.        ,\n",
    "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
    "         0.        ,  0.        ,  0.        ],\n",
    "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
    "         0.        ,  0.        ,  0.        ,  0.08235294,  0.87450987,\n",
    "         0.99215692,  0.98823535,  0.99215692,  0.98823535,  0.99215692,\n",
    "         0.67450982,  0.32156864,  0.        ,  0.        ,  0.        ,\n",
    "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
    "         0.        ,  0.        ,  0.        ],\n",
    "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
    "         0.        ,  0.        ,  0.08235294,  0.83921576,  0.99215692,\n",
    "         0.7960785 ,  0.63529414,  0.40000004,  0.40000004,  0.7960785 ,\n",
    "         0.87450987,  0.99607849,  0.99215692,  0.20000002,  0.03921569,\n",
    "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
    "         0.        ,  0.        ,  0.        ],\n",
    "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
    "         0.        ,  0.        ,  0.2392157 ,  0.99215692,  0.67058825,\n",
    "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
    "         0.07843138,  0.43921572,  0.75294125,  0.99215692,  0.83137262,\n",
    "         0.16078432,  0.        ,  0.        ,  0.        ,  0.        ,\n",
    "         0.        ,  0.        ,  0.        ],\n",
    "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
    "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
    "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
    "         0.        ,  0.        ,  0.        ,  0.40000004,  0.7960785 ,\n",
    "         0.91764712,  0.20000002,  0.        ,  0.        ,  0.        ,\n",
    "         0.        ,  0.        ,  0.        ],\n",
    "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
    "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
    "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
    "         0.        ,  0.        ,  0.        ,  0.        ,  0.07843138,\n",
    "         0.83529419,  0.90980399,  0.32156864,  0.        ,  0.        ,\n",
    "         0.        ,  0.        ,  0.        ],\n",
    "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
    "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
    "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
    "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
    "         0.24313727,  0.7960785 ,  0.91764712,  0.43921572,  0.        ,\n",
    "         0.        ,  0.        ,  0.        ],\n",
    "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
    "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
    "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
    "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
    "         0.        ,  0.07843138,  0.83529419,  0.98823535,  0.        ,\n",
    "         0.        ,  0.        ,  0.        ],\n",
    "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
    "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
    "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
    "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
    "         0.        ,  0.        ,  0.60000002,  0.99215692,  0.        ,\n",
    "         0.        ,  0.        ,  0.        ],\n",
    "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
    "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
    "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
    "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
    "         0.        ,  0.16078432,  0.91372555,  0.83137262,  0.        ,\n",
    "         0.        ,  0.        ,  0.        ],\n",
    "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
    "         0.        ,  0.        ,  0.        ,  0.44313729,  0.36078432,\n",
    "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
    "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
    "         0.12156864,  0.67843139,  0.95686281,  0.15686275,  0.        ,\n",
    "         0.        ,  0.        ,  0.        ],\n",
    "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
    "         0.        ,  0.        ,  0.32156864,  0.99215692,  0.59215689,\n",
    "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
    "         0.        ,  0.08235294,  0.40000004,  0.40000004,  0.71764708,\n",
    "         0.91372555,  0.83137262,  0.31764707,  0.        ,  0.        ,\n",
    "         0.        ,  0.        ,  0.        ],\n",
    "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
    "         0.        ,  0.        ,  0.32156864,  1.        ,  0.99215692,\n",
    "         0.91764712,  0.59607846,  0.60000002,  0.75686282,  0.67843139,\n",
    "         0.99215692,  0.99607849,  0.99215692,  0.99607849,  0.83529419,\n",
    "         0.55686277,  0.07843138,  0.        ,  0.        ,  0.        ,\n",
    "         0.        ,  0.        ,  0.        ],\n",
    "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
    "         0.        ,  0.        ,  0.        ,  0.27843139,  0.59215689,\n",
    "         0.59215689,  0.90980399,  0.99215692,  0.83137262,  0.75294125,\n",
    "         0.59215689,  0.51372552,  0.19607845,  0.19607845,  0.03921569,\n",
    "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
    "         0.        ,  0.        ,  0.        ],\n",
    "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
    "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
    "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
    "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
    "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
    "         0.        ,  0.        ,  0.        ],\n",
    "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
    "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
    "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
    "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
    "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
    "         0.        ,  0.        ,  0.        ],\n",
    "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
    "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
    "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
    "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
    "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
    "         0.        ,  0.        ,  0.        ],\n",
    "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
    "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
    "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
    "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
    "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
    "         0.        ,  0.        ,  0.        ]], dtype = np.float32)\n",
    "\n",
    "image = image_array.reshape(28, 28)\n",
    "\n",
    "plt.imshow(\n",
    "    image,\n",
    "    cmap          = \"Greys\",\n",
    "    interpolation = \"nearest\"\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An array can be represented as an image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.5  0.   0. ]\n",
      " [ 0.   1.   1. ]\n",
      " [ 0.5  0.   0. ]]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQ8AAAD8CAYAAABpXiE9AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f1d44a12d90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "image_array =\\\n",
    "np.array([\n",
    "    0.5,\n",
    "    0.0,\n",
    "    0.0,\n",
    "    0.0,\n",
    "    1.0,\n",
    "    1.0,\n",
    "    0.5,\n",
    "    0.0,\n",
    "    0.0\n",
    "])\n",
    "\n",
    "image = image_array.reshape(3, 3)\n",
    "\n",
    "print(image)\n",
    "\n",
    "plt.imshow(\n",
    "    image,\n",
    "    cmap          = \"Greys\",\n",
    "    interpolation = \"nearest\"\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An array can be represented as an image using padding as necessary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQ8AAAD8CAYAAABpXiE9AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f1d379e8550>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def numpy_array_pad_square_shape(\n",
    "    array     = None,\n",
    "    pad_value = 0\n",
    "    ):\n",
    "\n",
    "    width_padded = int(math.ceil(math.sqrt(len(array))))\n",
    "    padding      = (width_padded ** 2 - len(array)) * [pad_value]\n",
    "    array        = np.append(array, padding)\n",
    "    array        = array.reshape(width_padded, width_padded)\n",
    "\n",
    "    return array\n",
    "\n",
    "data  = np.array([0.5, 0.0, 0.0, 0.0, 1.0, 1.0, 0.5, 0.0])\n",
    "image = numpy_array_pad_square_shape(array = data)\n",
    "\n",
    "plt.imshow(\n",
    "    image,\n",
    "    cmap          = \"Greys\",\n",
    "    interpolation = \"nearest\"\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# example: MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "\n",
      "training\n",
      "\n",
      "epoch: 1\tcost: 2.53797165879\n",
      "epoch: 2\tcost: 1.09456407238\n",
      "epoch: 3\tcost: 0.881119381298\n",
      "epoch: 4\tcost: 0.773099724352\n",
      "epoch: 5\tcost: 0.704426293644\n",
      "epoch: 6\tcost: 0.655394366384\n",
      "epoch: 7\tcost: 0.618686331863\n",
      "epoch: 8\tcost: 0.589267465553\n",
      "epoch: 9\tcost: 0.565370515449\n",
      "epoch: 10\tcost: 0.545365656289\n",
      "epoch: 11\tcost: 0.528833894621\n",
      "epoch: 12\tcost: 0.51371610208\n",
      "epoch: 13\tcost: 0.500891014392\n",
      "epoch: 14\tcost: 0.489158772555\n",
      "epoch: 15\tcost: 0.478504846543\n",
      "\n",
      "testing\n",
      "\n",
      "accuracy:\n",
      "\n",
      "0.888100028038\n",
      "\n",
      "label:\n",
      "[2]\n",
      "\n",
      "prediction:\n",
      "[2]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f742721ce10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import tensorflow.examples.tutorials.mnist\n",
    "\n",
    "mnist = tensorflow.examples.tutorials.mnist.input_data.read_data_sets(\n",
    "    \"MNIST_data/\",\n",
    "    one_hot = True\n",
    ")\n",
    "\n",
    "number_of_classes = 10\n",
    "\n",
    "# MNIST data image of shape 28 * 28 = 784\n",
    "X = tf.placeholder(tf.float32, [None, 784])\n",
    "# 10 classes (digits 0 to 9)\n",
    "Y = tf.placeholder(tf.float32, [None, number_of_classes])\n",
    "\n",
    "W = tf.Variable(tf.random_normal([784, number_of_classes]))\n",
    "b = tf.Variable(tf.random_normal([number_of_classes]))\n",
    "\n",
    "# hypothesis (using softmax)\n",
    "hypothesis = tf.nn.softmax(tf.matmul(X, W) + b)\n",
    "cost       = tf.reduce_mean(-tf.reduce_sum(Y * tf.log(hypothesis), axis = 1))\n",
    "optimizer  = tf.train.GradientDescentOptimizer(learning_rate = 0.1).minimize(cost)\n",
    "\n",
    "is_correct = tf.equal(tf.argmax(hypothesis, 1), tf.arg_max(Y, 1))\n",
    "accuracy   = tf.reduce_mean(tf.cast(is_correct, tf.float32))\n",
    "\n",
    "# parameters\n",
    "training_epochs = 15\n",
    "batch_size      = 100\n",
    "\n",
    "with tf.Session() as sess:\n",
    "\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    print(\"\\ntraining\\n\")\n",
    "\n",
    "    for epoch in range(training_epochs):\n",
    "\n",
    "        cost_mean   = 0\n",
    "        total_batch = int(mnist.train.num_examples / batch_size)\n",
    "\n",
    "        for i in range(total_batch):\n",
    "\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "\n",
    "            c, _ = sess.run(\n",
    "                       [cost, optimizer],\n",
    "                       feed_dict={X: batch_xs, Y: batch_ys}\n",
    "                   )\n",
    "            cost_mean += c / total_batch\n",
    "\n",
    "        print(\"epoch: {epoch}\\tcost: {cost}\".format(\n",
    "            epoch = epoch + 1,\n",
    "            cost  = cost_mean\n",
    "        ))\n",
    "\n",
    "    print(\"\\ntesting\")\n",
    "\n",
    "    accuracy = accuracy.eval(\n",
    "                   session   = sess,\n",
    "                   feed_dict = {\n",
    "                                   X: mnist.test.images,\n",
    "                                   Y: mnist.test.labels\n",
    "                               }\n",
    "               )\n",
    "\n",
    "    print(\"\\naccuracy:\\n\\n{accuracy}\".format(\n",
    "        accuracy = accuracy\n",
    "    ))\n",
    "\n",
    "    # select one test example and predict\n",
    "    r = random.randint(0, mnist.test.num_examples - 1)\n",
    "    print(\"\\nlabel:\")\n",
    "    print(sess.run(\n",
    "              tf.argmax(mnist.test.labels[r:r + 1], 1)\n",
    "          )\n",
    "    )\n",
    "    print(\"\\nprediction:\")\n",
    "    print(sess.run(\n",
    "              tf.argmax(hypothesis, 1),\n",
    "              feed_dict = {X: mnist.test.images[r:r + 1]}\n",
    "          )\n",
    "    )\n",
    "\n",
    "    plt.imshow(\n",
    "        mnist.test.images[r:r + 1].reshape(28, 28),\n",
    "        cmap          = \"Greys\",\n",
    "        interpolation = \"nearest\"\n",
    "    )\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# example: simple neural network for XOR truth table\n",
    "\n",
    "|**A**|**B**|**X**|\n",
    "|-----|-----|-----|\n",
    "|0    |0    |0    |\n",
    "|0    |1    |1    |\n",
    "|1    |0    |1    |\n",
    "|1    |1    |0    |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "step: 0\n",
      "cost: 0.753045797348\n",
      "W:\n",
      "[array([[-0.11067381,  0.06392822],\n",
      "       [-0.0321797 , -0.34502214]], dtype=float32), array([[-0.18104731],\n",
      "       [-2.40578866]], dtype=float32)]\n",
      "\n",
      "step: 2000\n",
      "cost: 0.561914384365\n",
      "W:\n",
      "[array([[-0.79464591,  3.22157192],\n",
      "       [ 0.62589425, -2.91539788]], dtype=float32), array([[-0.78444737],\n",
      "       [-3.19287276]], dtype=float32)]\n",
      "\n",
      "step: 4000\n",
      "cost: 0.142582535744\n",
      "W:\n",
      "[array([[-4.08664322,  5.49105358],\n",
      "       [ 4.42439032, -5.37737322]], dtype=float32), array([[-4.98574877],\n",
      "       [-5.03301287]], dtype=float32)]\n",
      "\n",
      "step: 6000\n",
      "cost: 0.0593350082636\n",
      "W:\n",
      "[array([[-5.27438784,  6.26706076],\n",
      "       [ 5.59442902, -6.11997843]], dtype=float32), array([[-6.67433167],\n",
      "       [-6.6047492 ]], dtype=float32)]\n",
      "\n",
      "step: 8000\n",
      "cost: 0.0363751091063\n",
      "W:\n",
      "[array([[-5.80319977,  6.65905046],\n",
      "       [ 6.11412334, -6.48613453]], dtype=float32), array([[-7.58772278],\n",
      "       [-7.52101755]], dtype=float32)]\n",
      "\n",
      "step: 10000\n",
      "cost: 0.0260161906481\n",
      "W:\n",
      "[array([[-6.12727022,  6.9115119 ],\n",
      "       [ 6.43365002, -6.72271347]], dtype=float32), array([[-8.21349049],\n",
      "       [-8.15323925]], dtype=float32)]\n",
      "\n",
      "accuracy report:\n",
      "\n",
      "hypothesis:\n",
      "\n",
      "[[ 0.02304083]\n",
      " [ 0.97056627]\n",
      " [ 0.9699561 ]\n",
      " [ 0.0201681 ]]\n",
      "\n",
      "correct (Y):\n",
      "\n",
      "[[ 0.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 0.]]\n",
      "\n",
      "accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "x_data = np.array([\n",
    "                      [0, 0],\n",
    "                      [0, 1],\n",
    "                      [1, 0],\n",
    "                      [1, 1]\n",
    "                  ],\n",
    "                  dtype=np.float32\n",
    ")\n",
    "y_data = np.array([\n",
    "                      [0],\n",
    "                      [1],\n",
    "                      [1],\n",
    "                      [0]\n",
    "                  ],\n",
    "                  dtype=np.float32\n",
    ")\n",
    "\n",
    "X          = tf.placeholder(tf.float32, [None, 2])\n",
    "Y          = tf.placeholder(tf.float32, [None, 1])\n",
    "\n",
    "W1         = tf.Variable(tf.random_normal([2, 2]), name = \"weight1\")\n",
    "b1         = tf.Variable(tf.random_normal([2]),    name = \"bias1\")\n",
    "layer1     = tf.sigmoid(tf.matmul(X, W1) + b1)\n",
    "\n",
    "W2         = tf.Variable(tf.random_normal([2, 1]), name = \"weight2\")\n",
    "b2         = tf.Variable(tf.random_normal([1]),    name = \"bias2\")\n",
    "hypothesis = tf.sigmoid(tf.matmul(layer1, W2) + b2)\n",
    "\n",
    "cost       = -tf.reduce_mean(Y * tf.log(hypothesis) + (1 - Y) * tf.log(1 - hypothesis))\n",
    "train      = tf.train.GradientDescentOptimizer(learning_rate = 0.1).minimize(cost)\n",
    "\n",
    "# accuracy computation: true if hypothesis > 0.5 else false\n",
    "predicted  = tf.cast(hypothesis > 0.5, dtype = tf.float32)\n",
    "accuracy   = tf.reduce_mean(tf.cast(tf.equal(predicted, Y), dtype = tf.float32))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for step in range(10001):\n",
    "\n",
    "        sess.run(\n",
    "            train,\n",
    "            feed_dict = {\n",
    "                            X: x_data,\n",
    "                            Y: y_data\n",
    "                        }\n",
    "        )\n",
    "        if step % 2000 == 0:\n",
    "            print(\"\\nstep: {step}\\ncost: {cost}\\nW:\\n{W}\".format(\n",
    "                step = step,\n",
    "                cost = sess.run(\n",
    "                           cost,\n",
    "                           feed_dict = {\n",
    "                                           X: x_data,\n",
    "                                           Y: y_data\n",
    "                                       }\n",
    "                       ),\n",
    "                W    = sess.run([W1, W2])\n",
    "            ))\n",
    "\n",
    "    print(\"\\naccuracy report:\")\n",
    "    h, c, a = sess.run(\n",
    "        [hypothesis, predicted, accuracy],\n",
    "        feed_dict = {\n",
    "                        X: x_data,\n",
    "                        Y: y_data\n",
    "                    }\n",
    "    )\n",
    "    print(\"\\nhypothesis:\\n\\n{hypothesis}\\n\\ncorrect (Y):\\n\\n{correct}\\n\\naccuracy: {accuracy}\".format(\n",
    "        hypothesis = h,\n",
    "        correct    = c,\n",
    "        accuracy   = a\n",
    "    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# deep learning\n",
    "\n",
    "Deep learning allows computational models that are composed of multiple processing layers to learn representations of data with multiple levels of abstraction. These methods have dramatically improced the state-of-the-art in speech recognition, visual object recognition, object detection and many other domains such as drug discovery and genomics. Deep learning discovers intricate structure in large data sets by using the backpropagation algorithm to indicate how a machine should change its internal parameters that are used to compute the representation in each layer from the representation in the previous layer. Deep convolutional networks have brought about breakthroughs in processing images, video, speech and sound, whereas recurrent networks have illuminated sequential data such as text and speech.\n",
    "\n",
    "# example: deep neural network for XOR truth table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "step: 0\n",
      "cost: 0.759968400002\n",
      "W:\n",
      "[array([[ 1.17050958,  0.23658907,  0.85170525,  1.88875902,  1.69035661,\n",
      "        -1.20168114,  1.58177483,  0.55276138, -1.39153302, -0.85973883],\n",
      "       [ 0.4605372 ,  0.36892304,  0.42037812,  0.13853477,  0.47574446,\n",
      "        -1.01397741, -2.16375875, -0.52245528,  1.84092593, -0.5710631 ]], dtype=float32), array([[ -6.51916325e-01,  -1.85874426e+00,   4.45647389e-02,\n",
      "         -6.67469561e-01,  -1.54609072e+00,  -9.40441936e-02,\n",
      "         -1.48086023e+00,  -4.72161453e-03,   1.60759103e+00,\n",
      "         -1.39276361e+00],\n",
      "       [ -2.79414564e-01,  -8.82146776e-01,  -1.51190117e-01,\n",
      "         -2.67493904e-01,  -1.13447070e+00,   8.15222740e-01,\n",
      "          7.18632489e-02,   3.05037677e-01,   1.10169828e+00,\n",
      "          9.25161242e-01],\n",
      "       [  4.75105822e-01,  -9.29442406e-01,   9.13582623e-01,\n",
      "         -9.78433847e-01,   4.50041622e-01,  -1.35249472e+00,\n",
      "          4.67909724e-02,  -7.56063581e-01,   9.32560444e-01,\n",
      "          1.57608414e+00],\n",
      "       [  4.74753797e-01,  -3.77498940e-02,   4.18261200e-01,\n",
      "          5.65235853e-01,  -6.51828825e-01,   9.84389305e-01,\n",
      "          1.74106228e+00,   5.27353168e-01,  -1.70698866e-01,\n",
      "          2.33003534e-02],\n",
      "       [ -5.46175502e-02,   6.22723758e-01,  -7.67561257e-01,\n",
      "         -1.13267875e+00,  -1.52870975e-02,   7.69220293e-01,\n",
      "         -2.44073451e-01,  -4.51155782e-01,   1.26717523e-01,\n",
      "          2.82709032e-01],\n",
      "       [ -3.83929610e-01,   2.12787056e+00,  -3.80378634e-01,\n",
      "         -9.56066668e-01,   1.35823059e+00,   2.19387698e+00,\n",
      "          2.98151612e-01,  -3.53446960e-01,   2.16026068e+00,\n",
      "          6.71151102e-01],\n",
      "       [ -8.90956461e-01,  -3.50749820e-01,   1.12407994e+00,\n",
      "         -1.24883640e+00,   7.10271478e-01,  -3.52103829e-01,\n",
      "          9.65147257e-01,   1.42521632e+00,   1.03253496e+00,\n",
      "         -7.08821177e-01],\n",
      "       [  4.75920975e-01,  -5.50910354e-01,   1.76522243e+00,\n",
      "          3.08009207e-01,  -1.27437925e+00,  -2.29747504e-01,\n",
      "          1.10236263e+00,   3.09314132e-01,  -2.15081310e+00,\n",
      "          1.16853654e+00],\n",
      "       [ -2.18560004e+00,  -4.12839986e-02,  -5.68666577e-01,\n",
      "         -2.66190171e-01,  -9.63186085e-01,  -1.21388638e+00,\n",
      "          3.70121777e-01,   6.09177828e-01,   1.01256013e+00,\n",
      "          8.73464286e-01],\n",
      "       [ -5.88184863e-04,   3.18837792e-01,   8.63053262e-01,\n",
      "         -8.58466148e-01,  -1.49794906e-01,  -7.59702444e-01,\n",
      "          1.29285264e+00,  -6.99897408e-01,  -2.02628350e+00,\n",
      "         -3.00212532e-01]], dtype=float32)]\n",
      "\n",
      "step: 2000\n",
      "cost: 0.01114413701\n",
      "W:\n",
      "[array([[ 1.98521769,  0.30403042,  1.32787514,  1.83283591,  1.78198922,\n",
      "        -1.62909162,  3.27980161,  0.49910524, -3.57817411, -0.55922776],\n",
      "       [ 1.75168157,  0.49920267,  1.1829505 ,  0.19544524,  0.2836411 ,\n",
      "        -1.51944184, -4.20622206, -0.2580229 ,  3.23162079, -0.0622626 ]], dtype=float32), array([[-0.37776458, -2.03713012,  0.04895737, -0.71010005, -1.55461216,\n",
      "        -0.12174718, -1.46594691,  0.13099535,  1.67368925, -1.55364335],\n",
      "       [ 0.16004103, -0.69712305, -0.35540289, -0.24590364, -1.0853914 ,\n",
      "         1.03830802,  0.05432311,  0.24380289,  0.85483462,  0.70639956],\n",
      "       [ 0.86299324, -0.89997804,  0.80172426, -0.98486114,  0.4814046 ,\n",
      "        -1.24941957,  0.04234253, -0.71301037,  0.84080839,  1.36699319],\n",
      "       [ 0.7938723 ,  0.19709581,  0.03023471,  0.62575471, -0.66244501,\n",
      "         1.2033397 ,  1.64727712,  0.26879051, -0.55915737, -0.14568838],\n",
      "       [ 0.25958532,  0.83390534, -1.15658045, -1.07560802, -0.0334263 ,\n",
      "         1.00842452, -0.33619571, -0.72956598, -0.27729893,  0.1320466 ],\n",
      "       [-0.18766119,  2.57755065, -0.61852098, -0.87745345,  1.44232202,\n",
      "         2.40515637,  0.25740507, -0.51312041,  1.85743368,  0.54215413],\n",
      "       [-1.89096189, -1.46995986,  2.0109973 , -1.43777013,  0.61770618,\n",
      "        -1.58152425,  1.04666483,  2.29597664,  2.55221367, -0.37994909],\n",
      "       [ 0.56462687, -0.42577374,  1.65682042,  0.33601317, -1.25187159,\n",
      "        -0.21807052,  1.06320179,  0.2772755 , -2.20399094,  1.0963366 ],\n",
      "       [-2.49139977, -1.28742731,  0.45501134, -0.56091487, -1.01522624,\n",
      "        -1.93296278,  0.62528908,  1.59042633,  2.26734567,  1.03851604],\n",
      "       [ 0.44379306,  0.54684651,  0.74534124, -0.83821028, -0.05119529,\n",
      "        -0.61031055,  1.28923488, -0.64227885, -2.1296463 , -0.545066  ]], dtype=float32)]\n",
      "\n",
      "step: 4000\n",
      "cost: 0.0030683837831\n",
      "W:\n",
      "[array([[ 2.06987309,  0.29238752,  1.36369765,  1.82479906,  1.78571844,\n",
      "        -1.69133949,  3.50241017,  0.47419772, -3.79909587, -0.55028951],\n",
      "       [ 1.84372735,  0.49716002,  1.22480059,  0.18024811,  0.2902675 ,\n",
      "        -1.57427073, -4.4084959 , -0.23187041,  3.41623235, -0.02647088]], dtype=float32), array([[-0.39700064, -2.03879881,  0.08418256, -0.7068131 , -1.55986774,\n",
      "        -0.14481136, -1.45166862,  0.15662274,  1.70703411, -1.58542883],\n",
      "       [ 0.19263497, -0.66029841, -0.36129141, -0.23194791, -1.08556306,\n",
      "         1.06441748,  0.06178622,  0.22310978,  0.83199215,  0.67137349],\n",
      "       [ 0.87444288, -0.87758064,  0.81307673, -0.97366869,  0.47881708,\n",
      "        -1.24434745,  0.053905  , -0.7157529 ,  0.84165907,  1.32883716],\n",
      "       [ 0.80360782,  0.24630025,  0.0083555 ,  0.64766264, -0.66640598,\n",
      "         1.21596515,  1.64982641,  0.23593713, -0.58059281, -0.17668022],\n",
      "       [ 0.26878566,  0.88565165, -1.18099034, -1.05411816, -0.03742926,\n",
      "         1.02272832, -0.33554584, -0.76354975, -0.30108178,  0.10540831],\n",
      "       [-0.14117245,  2.60630083, -0.65078712, -0.86601096,  1.44678676,\n",
      "         2.44401503,  0.2557987 , -0.55342007,  1.8127594 ,  0.52815962],\n",
      "       [-2.07264161, -1.63389993,  2.1099813 , -1.4678396 ,  0.60466039,\n",
      "        -1.76514411,  1.06457162,  2.41910958,  2.7342844 , -0.36114782],\n",
      "       [ 0.57098752, -0.4100121 ,  1.64828825,  0.34572282, -1.25294447,\n",
      "        -0.21411738,  1.06619751,  0.26270938, -2.21183562,  1.07881045],\n",
      "       [-2.57746816, -1.42833269,  0.59718454, -0.61157376, -1.01852977,\n",
      "        -2.03334403,  0.65144879,  1.74180627,  2.4123714 ,  1.04945874],\n",
      "       [ 0.48612845,  0.58069319,  0.73849308, -0.82186681, -0.05011338,\n",
      "        -0.58125377,  1.30030692, -0.66793817, -2.15542126, -0.5908556 ]], dtype=float32)]\n",
      "\n",
      "step: 6000\n",
      "cost: 0.0016878277529\n",
      "W:\n",
      "[array([[ 2.10473371,  0.28726593,  1.37793338,  1.82165623,  1.78805029,\n",
      "        -1.71745026,  3.59396052,  0.46256995, -3.88882947, -0.54746324],\n",
      "       [ 1.88168252,  0.49646902,  1.24172914,  0.17239496,  0.29395255,\n",
      "        -1.59743035, -4.49084234, -0.22185875,  3.49313998, -0.01221459]], dtype=float32), array([[ -4.07575130e-01,  -2.03941464e+00,   9.91711393e-02,\n",
      "         -7.05823600e-01,  -1.56206536e+00,  -1.54784217e-01,\n",
      "         -1.44642031e+00,   1.68780372e-01,   1.72188318e+00,\n",
      "         -1.59771848e+00],\n",
      "       [  2.05933303e-01,  -6.45399094e-01,  -3.64031285e-01,\n",
      "         -2.25773960e-01,  -1.08580148e+00,   1.07549596e+00,\n",
      "          6.45613447e-02,   2.14635387e-01,   8.22528005e-01,\n",
      "          6.56991899e-01],\n",
      "       [  8.78056586e-01,  -8.68465662e-01,   8.17599535e-01,\n",
      "         -9.68873143e-01,   4.77587700e-01,  -1.24214292e+00,\n",
      "          5.81872724e-02,  -7.16281474e-01,   8.42290878e-01,\n",
      "          1.31340635e+00],\n",
      "       [  8.06615829e-01,   2.67394692e-01,  -1.53666304e-03,\n",
      "          6.57032430e-01,  -6.68074131e-01,   1.22133529e+00,\n",
      "          1.65075815e+00,   2.22695380e-01,  -5.89089751e-01,\n",
      "         -1.88969329e-01],\n",
      "       [  2.71311820e-01,   9.08151031e-01,  -1.19179749e+00,\n",
      "         -1.04500628e+00,  -3.90701629e-02,   1.02868819e+00,\n",
      "         -3.35373372e-01,  -7.77113378e-01,  -3.10328305e-01,\n",
      "          9.51324105e-02],\n",
      "       [ -1.19598188e-01,   2.61682987e+00,  -6.64776862e-01,\n",
      "         -8.60525846e-01,   1.44833672e+00,   2.46067929e+00,\n",
      "          2.55380124e-01,  -5.71289897e-01,   1.79319930e+00,\n",
      "          5.21377981e-01],\n",
      "       [ -2.15024447e+00,  -1.70363724e+00,   2.15080476e+00,\n",
      "         -1.48187828e+00,   5.99478722e-01,  -1.84190834e+00,\n",
      "          1.07147944e+00,   2.47179389e+00,   2.80945778e+00,\n",
      "         -3.53414804e-01],\n",
      "       [  5.73949397e-01,  -4.03727412e-01,   1.64436710e+00,\n",
      "          3.49937856e-01,  -1.25347781e+00,  -2.12074280e-01,\n",
      "          1.06739116e+00,   2.56669670e-01,  -2.21538568e+00,\n",
      "          1.07144809e+00],\n",
      "       [ -2.61572957e+00,  -1.48817861e+00,   6.58707917e-01,\n",
      "         -6.34265363e-01,  -1.01972091e+00,  -2.07559967e+00,\n",
      "          6.61088467e-01,   1.80674827e+00,   2.47317290e+00,\n",
      "          1.05452681e+00],\n",
      "       [  5.04486144e-01,   5.93457520e-01,   7.35035777e-01,\n",
      "         -8.14387262e-01,  -5.00600114e-02,  -5.68593085e-01,\n",
      "          1.30454707e+00,  -6.78962886e-01,  -2.16681433e+00,\n",
      "         -6.10334992e-01]], dtype=float32)]\n",
      "\n",
      "step: 8000\n",
      "cost: 0.00114469625987\n",
      "W:\n",
      "[array([[  2.12623739e+00,   2.84024686e-01,   1.38657284e+00,\n",
      "          1.81982517e+00,   1.78982985e+00,  -1.73367596e+00,\n",
      "          3.65022731e+00,   4.55022335e-01,  -3.94385529e+00,\n",
      "         -5.45952678e-01],\n",
      "       [  1.90512550e+00,   4.96101886e-01,   1.25208580e+00,\n",
      "          1.67051837e-01,   2.96596855e-01,  -1.61188042e+00,\n",
      "         -4.54140806e+00,  -2.15951473e-01,   3.54071832e+00,\n",
      "         -3.55720916e-03]], dtype=float32), array([[-0.41485974, -2.03972101,  0.10844814, -0.70537728, -1.56342804,\n",
      "        -0.16105978, -1.44348049,  0.17666337,  1.7313664 , -1.60486555],\n",
      "       [ 0.21408351, -0.63623756, -0.36589482, -0.22188419, -1.0860399 ,\n",
      "         1.08240271,  0.06612433,  0.20938629,  0.81671464,  0.64830929],\n",
      "       [ 0.8799566 , -0.86279756,  0.82024044, -0.96591753,  0.47677895,\n",
      "        -1.2407558 ,  0.06059803, -0.71648324,  0.84278399,  1.30417931],\n",
      "       [ 0.80823183,  0.28070003, -0.00799221,  0.66287696, -0.66910481,\n",
      "         1.22473145,  1.65125227,  0.21452603, -0.59425688, -0.19625649],\n",
      "       [ 0.27255338,  0.92243528, -1.19877148, -1.03934073, -0.04007108,\n",
      "         1.03243196, -0.33535069, -0.78543591, -0.31590301,  0.08913336],\n",
      "       [-0.10564253,  2.62293386, -0.67359018, -0.85691917,  1.44917452,\n",
      "         2.47107816,  0.25521576, -0.58262432,  1.78087652,  0.5169456 ],\n",
      "       [-2.19883466, -1.74721992,  2.17607474, -1.49105465,  0.59636658,\n",
      "        -1.88955843,  1.0755527 ,  2.50485039,  2.85584927, -0.3486807 ],\n",
      "       [ 0.57587218, -0.39989311,  1.64181197,  0.3525753 , -1.25382674,\n",
      "        -0.21070856,  1.06808043,  0.25292507, -2.2176187 ,  1.06695259],\n",
      "       [-2.64017153, -1.52556813,  0.69749629, -0.64881825, -1.02037895,\n",
      "        -2.10195494,  0.66661352,  1.84752786,  2.51097727,  1.05781639],\n",
      "       [ 0.51601458,  0.60104728,  0.73264384, -0.80961299, -0.05015899,\n",
      "        -0.56064969,  1.30700314, -0.68589544, -2.17397404, -0.62229699]], dtype=float32)]\n",
      "\n",
      "step: 10000\n",
      "cost: 0.000859200255945\n",
      "W:\n",
      "[array([[  2.14161205e+00,   2.81671345e-01,   1.39269090e+00,\n",
      "          1.81856072e+00,   1.79128873e+00,  -1.74533463e+00,\n",
      "          3.69033599e+00,   4.49463367e-01,  -3.98305607e+00,\n",
      "         -5.44974983e-01],\n",
      "       [  1.92190897e+00,   4.95870918e-01,   1.25945365e+00,\n",
      "          1.62989348e-01,   2.98669934e-01,  -1.62229407e+00,\n",
      "         -4.57746744e+00,  -2.11858407e-01,   3.57478285e+00,\n",
      "          2.57168524e-03]], dtype=float32), array([[-0.42042193, -2.03972101,  0.11507976, -0.7051689 , -1.56441677,\n",
      "        -0.1656107 , -1.44154274,  0.18246652,  1.73829794, -1.60973501],\n",
      "       [ 0.21987981, -0.62970501, -0.36732703, -0.21908149, -1.08627832,\n",
      "         1.0873667 ,  0.06715739,  0.20561644,  0.81257725,  0.64223248],\n",
      "       [ 0.8811577 , -0.85872781,  0.82204235, -0.96382767,  0.47618094,\n",
      "        -1.23975348,  0.06219234, -0.71660244,  0.84319365,  1.29777193],\n",
      "       [ 0.80928737,  0.29034486, -0.01279016,  0.66707408, -0.66985017,\n",
      "         1.227198  ,  1.65149069,  0.20866935, -0.59792155, -0.20129602],\n",
      "       [ 0.27330357,  0.93283755, -1.20391166, -1.03528225, -0.04077977,\n",
      "         1.03514326, -0.33539131, -0.79138011, -0.3198368 ,  0.08503048],\n",
      "       [-0.09537798,  2.62712526, -0.6799795 , -0.85424173,  1.4497155 ,\n",
      "         2.47857213,  0.25514543, -0.59086585,  1.77195966,  0.51368034],\n",
      "       [-2.23389387, -1.77863932,  2.19421172, -1.49785972,  0.59418905,\n",
      "        -1.92376614,  1.07836437,  2.52874732,  2.88902521, -0.34531906],\n",
      "       [ 0.57728857, -0.39717501,  1.63991857,  0.35446966, -1.25406516,\n",
      "        -0.20968813,  1.06855726,  0.25023746, -2.21923661,  1.06378698],\n",
      "       [-2.65804815, -1.55252087,  0.72562265, -0.65949064, -1.02085578,\n",
      "        -2.12094569,  0.67034841,  1.87701845,  2.53815055,  1.06023812],\n",
      "       [ 0.52433282,  0.60633808,  0.73079741, -0.80614823, -0.05029546,\n",
      "        -0.55492693,  1.30866098, -0.69091398, -2.17912292, -0.63076282]], dtype=float32)]\n",
      "\n",
      "accuracy report:\n",
      "\n",
      "hypothesis:\n",
      "\n",
      "[[  8.59555963e-04]\n",
      " [  9.99214053e-01]\n",
      " [  9.99189436e-01]\n",
      " [  9.79224336e-04]]\n",
      "\n",
      "correct (Y):\n",
      "\n",
      "[[ 0.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 0.]]\n",
      "\n",
      "accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "x_data = np.array([\n",
    "                      [0, 0],\n",
    "                      [0, 1],\n",
    "                      [1, 0],\n",
    "                      [1, 1]\n",
    "                  ],\n",
    "                  dtype = np.float32\n",
    ")\n",
    "y_data = np.array([\n",
    "                      [0],\n",
    "                      [1],\n",
    "                      [1],\n",
    "                      [0]\n",
    "                  ],\n",
    "                  dtype = np.float32\n",
    ")\n",
    "\n",
    "X          = tf.placeholder(tf.float32, [None, 2])\n",
    "Y          = tf.placeholder(tf.float32, [None, 1])\n",
    "\n",
    "W1         = tf.Variable(tf.random_normal([2, 10]), name = \"weight1\")\n",
    "b1         = tf.Variable(tf.random_normal([10]),    name = \"bias1\")\n",
    "layer1     = tf.sigmoid(tf.matmul(X, W1) + b1)\n",
    "\n",
    "W2         = tf.Variable(tf.random_normal([10, 10]), name = \"weight2\")\n",
    "b2         = tf.Variable(tf.random_normal([10]),     name = \"bias2\")\n",
    "layer2     = tf.sigmoid(tf.matmul(layer1, W2) + b2)\n",
    "\n",
    "W3         = tf.Variable(tf.random_normal([10, 10]), name = \"weight3\")\n",
    "b3         = tf.Variable(tf.random_normal([10]),     name = \"bias3\")\n",
    "layer3     = tf.sigmoid(tf.matmul(layer2, W3) + b3)\n",
    "\n",
    "W4         = tf.Variable(tf.random_normal([10, 1]),  name = \"weight4\")\n",
    "b4         = tf.Variable(tf.random_normal([1]),      name = \"bias4\")\n",
    "hypothesis = tf.sigmoid(tf.matmul(layer3, W4) + b4)\n",
    "\n",
    "cost       = -tf.reduce_mean(Y * tf.log(hypothesis) + (1 - Y) * tf.log(1 - hypothesis))\n",
    "train      = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n",
    "\n",
    "# accuracy computation: true if hypothesis > 0.5 else false\n",
    "predicted  = tf.cast(hypothesis > 0.5, dtype = tf.float32)\n",
    "accuracy   = tf.reduce_mean(tf.cast(tf.equal(predicted, Y), dtype = tf.float32))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for step in range(10001):\n",
    "\n",
    "        sess.run(\n",
    "            train,\n",
    "            feed_dict = {\n",
    "                            X: x_data,\n",
    "                            Y: y_data\n",
    "                        }\n",
    "        )\n",
    "        if step % 2000 == 0:\n",
    "            print(\"\\nstep: {step}\\ncost: {cost}\\nW:\\n{W}\".format(\n",
    "                step = step,\n",
    "                cost = sess.run(\n",
    "                           cost,\n",
    "                           feed_dict = {\n",
    "                                           X: x_data,\n",
    "                                           Y: y_data\n",
    "                                       }\n",
    "                       ),\n",
    "                W    = sess.run([W1, W2])\n",
    "            ))\n",
    "\n",
    "    print(\"\\naccuracy report:\")\n",
    "    h, c, a = sess.run(\n",
    "        [hypothesis, predicted, accuracy],\n",
    "        feed_dict = {\n",
    "                        X: x_data,\n",
    "                        Y: y_data\n",
    "                    }\n",
    "    )\n",
    "    print(\"\\nhypothesis:\\n\\n{hypothesis}\\n\\ncorrect (Y):\\n\\n{correct}\\n\\naccuracy: {accuracy}\".format(\n",
    "        hypothesis = h,\n",
    "        correct    = c,\n",
    "        accuracy   = a\n",
    "    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# simple neural network versus deep neural network\n",
    "\n",
    "Both the simple and the deep neural networks are successful at modelling the XOR truth table, however, the deep neural network has a far greater accuracy:\n",
    "\n",
    "|**A**|**B**|**X**|**simple NN**|**deep NN**   |**factor of difference in error**|\n",
    "|-----|-----|-----|-------------|--------------|---------------------------------|\n",
    "|0    |0    |0    |0.02304083   |8.59555963e-04|26.8                             |\n",
    "|0    |1    |1    |0.97056627   |9.99214053e-01|37.5                             |\n",
    "|1    |0    |1    |0.9699561    |9.99189436e-01|37.1                             |\n",
    "|1    |1    |0    |0.0201681    |9.79224336e-04|20.6                             |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorBoard\n",
    "\n",
    "Tensors can be logged and viewed in TensorBoard. Images are simply displayed, histograms are multidimensional tensors and scalar tensors are shown as graphs.\n",
    "\n",
    "1. Set the tensors to log.\n",
    "\n",
    "```Python\n",
    "with tf.variable_scope(\"layer1\") as scope:\n",
    "    tf.summary.image(\"input\", x_image, 3)\n",
    "    tf.summary.histogram(\"layer\", L1)\n",
    "    tf.summary.scalar(\"loss\", cost)\n",
    "```\n",
    "\n",
    "2. Merge the summaries.\n",
    "\n",
    "```Python\n",
    "summary = tf.summary.merge_all()\n",
    "```\n",
    "\n",
    "3. Create a summary writer and add the TensorFlow graph.\n",
    "\n",
    "```Python\n",
    "writer = tf.summary.FileWriter(TB_SUMMARY_DIR)\n",
    "writer.add_graph(sess.graph)\n",
    "```\n",
    "\n",
    "4. Run summary merge and add the summary to the writer.\n",
    "\n",
    "```Python\n",
    "s, _ = sess.run([summary, optimizer], feed_dict = feed_dict)\n",
    "writer.add_summary(s, global_step = global_step)\n",
    "```\n",
    "\n",
    "5. Launch TensorBoard.\n",
    "\n",
    "```Bash\n",
    "tensorboard --logdir=/tmp/mnist_logs\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# example: TensorBoard\n",
    "\n",
    "Clear logs and launch TensorBoard:\n",
    "\n",
    "```Bash\n",
    "rm -rf /tmp/mnist\n",
    "tensorboard --logdir=/tmp/mnist\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.examples.tutorials.mnist\n",
    "\n",
    "# reset everything to rerun in jupyter\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# configuration\n",
    "batch_size      = 100\n",
    "learning_rate   = 0.5\n",
    "training_epochs = 500\n",
    "logs_path       = \"/tmp/mnist/1\"\n",
    "\n",
    "# load mnist data set\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = tensorflow.examples.tutorials.mnist.input_data.read_data_sets(\n",
    "    \"MNIST_data/\",\n",
    "    one_hot = True\n",
    ")\n",
    "\n",
    "with tf.name_scope(\"input\"):\n",
    "    # None => batch size can be any size; 784 => flattened image\n",
    "    x  = tf.placeholder(tf.float32, shape = [None, 784], name = \"x-input\") \n",
    "    # target 10 output classes\n",
    "    y_ = tf.placeholder(tf.float32, shape = [None, 10],  name = \"y-input\")\n",
    "\n",
    "with tf.name_scope(\"weights\"):\n",
    "    W = tf.Variable(tf.zeros([784, 10]))\n",
    "\n",
    "with tf.name_scope(\"biases\"):\n",
    "    b = tf.Variable(tf.zeros([10]))\n",
    "\n",
    "with tf.name_scope(\"softmax\"):\n",
    "    y = tf.nn.softmax(tf.matmul(x, W) + b)\n",
    "\n",
    "with tf.name_scope(\"cross-entropy\"):\n",
    "    cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y), axis = 1))\n",
    "\n",
    "# specify optimizer\n",
    "with tf.name_scope(\"train\"):\n",
    "    # optimizer is an \"operation\" which we can execute in a session\n",
    "    train_op = tf.train.GradientDescentOptimizer(learning_rate).minimize(cross_entropy)\n",
    "\n",
    "with tf.name_scope(\"accuracy\"):\n",
    "    correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))\n",
    "    accuracy           = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "tf.summary.scalar(\"cost\", cross_entropy)\n",
    "tf.summary.scalar(\"accuracy\", accuracy)\n",
    "tf.summary.scalar(\"input\", x)\n",
    "\n",
    "summary_operation = tf.summary.merge_all()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    writer = tf.summary.FileWriter(logs_path)\n",
    "\n",
    "    # perform training cycles\n",
    "    for epoch in range(training_epochs):\n",
    "        \n",
    "        # number of batches in one epoch\n",
    "        batch_count = int(mnist.train.num_examples / batch_size)\n",
    "        \n",
    "        for i in range(batch_count):\n",
    "            batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "\n",
    "            _, summary = sess.run(\n",
    "                [train_op, summary_operation],\n",
    "                feed_dict = {x: batch_x, y_: batch_y}\n",
    "            )\n",
    "\n",
    "            writer.add_summary(summary, epoch * batch_count + i)\n",
    "            \n",
    "        if epoch % 100 == 0: \n",
    "            print(\"epoch: {epoch}\".format(epoch = epoch))\n",
    "\n",
    "    print(\"accuracy: {accuracy}\".format(\n",
    "        accuracy = accuracy.eval(feed_dict = {\n",
    "                                                 x:  mnist.test.images,\n",
    "                                                 y_: mnist.test.labels\n",
    "                                             }\n",
    "                                )\n",
    "    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# example: softmax classifier for MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "epoch: 0, cost: 5.5008147295\n",
      "epoch: 1, cost: 1.82199460918\n",
      "epoch: 2, cost: 1.2013834101\n",
      "epoch: 3, cost: 0.94063898856\n",
      "epoch: 4, cost: 0.794875945937\n",
      "epoch: 5, cost: 0.700277985768\n",
      "epoch: 6, cost: 0.633509654484\n",
      "epoch: 7, cost: 0.583592939865\n",
      "epoch: 8, cost: 0.543543923416\n",
      "epoch: 9, cost: 0.512178804576\n",
      "epoch: 10, cost: 0.486511237662\n",
      "epoch: 11, cost: 0.464348079616\n",
      "epoch: 12, cost: 0.445898542932\n",
      "epoch: 13, cost: 0.430081928332\n",
      "epoch: 14, cost: 0.416244304167\n",
      "accuracy: 0.897599995136\n",
      "\n",
      "label:\n",
      "[5]\n",
      "\n",
      "prediction:\n",
      "[5]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fd2be885b50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot = True)\n",
    "\n",
    "# parameters\n",
    "learning_rate   = 0.001\n",
    "training_epochs = 15\n",
    "batch_size      = 100\n",
    "\n",
    "# input place holders\n",
    "X = tf.placeholder(tf.float32, [None, 784])\n",
    "Y = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "# weights and bias for layers\n",
    "W = tf.Variable(tf.random_normal([784, 10]))\n",
    "b = tf.Variable(tf.random_normal([10]))\n",
    "\n",
    "hypothesis = tf.matmul(X, W) + b\n",
    "\n",
    "# define cost/loss and optimizer\n",
    "cost      = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = hypothesis, labels = Y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(cost)\n",
    "\n",
    "# initialize\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for epoch in range(training_epochs):\n",
    "\n",
    "    avg_cost = 0\n",
    "    total_batch = int(mnist.train.num_examples / batch_size)\n",
    "\n",
    "    for i in range(total_batch):\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "        feed_dict = {X: batch_xs, Y: batch_ys}\n",
    "        c, _ = sess.run([cost, optimizer], feed_dict = feed_dict)\n",
    "        avg_cost += c / total_batch\n",
    "\n",
    "    print(\"epoch: {epoch}, cost: {cost}\".format(\n",
    "        epoch = epoch,\n",
    "        cost  = avg_cost\n",
    "    ))\n",
    "\n",
    "# test accuracy\n",
    "correct_prediction = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "print(\"accuracy: {accuracy}\".format(\n",
    "    accuracy = sess.run(\n",
    "                   accuracy,\n",
    "                   feed_dict = {\n",
    "                                   X: mnist.test.images,\n",
    "                                   Y: mnist.test.labels\n",
    "                               }\n",
    "    )\n",
    "))\n",
    "\n",
    "# select one test example and predict\n",
    "r = random.randint(0, mnist.test.num_examples - 1)\n",
    "print(\"\\nlabel:\")\n",
    "print(sess.run(\n",
    "          tf.argmax(mnist.test.labels[r:r + 1], 1)\n",
    "      )\n",
    ")\n",
    "print(\"\\nprediction:\")\n",
    "print(sess.run(\n",
    "          tf.argmax(hypothesis, 1),\n",
    "          feed_dict = {X: mnist.test.images[r:r + 1]}\n",
    "      )\n",
    ")\n",
    "\n",
    "plt.imshow(\n",
    "    mnist.test.images[r:r + 1].reshape(28, 28),\n",
    "    cmap          = \"Greys\",\n",
    "    interpolation = \"nearest\"\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# rectifier activation functions\n",
    "\n",
    "A rectifier activation function is defined as\n",
    "\n",
    "$${\n",
    "f\\left(x\\right)=\\textrm{max}\\left(0,x\\right)\n",
    "}$$\n",
    "\n",
    "where ${x}$ is the input to the neuron. It has been used in convolutional networks more effectively than the widely-used logistic sigmoid and the hyperbolic tangent. It was the most popular activation function for deep neural networks in 2015. A unit employing the rectifier is also called a rectified linear unit (ReLU).\n",
    "\n",
    "Rectified linear units, compared to sigmoid function or similar activation functions, allow for faster and effective training of deep neural architectures on large and complex datasets.\n",
    "\n",
    "A smooth approximation to the rectifier is the following analytic function, called the softplus function:\n",
    "\n",
    "$${\n",
    "f\\left(x\\right)=\\ln\\left(1+e^{x}\\right)\n",
    "}$$\n",
    "\n",
    "The derivative of softplus is the logistic function:\n",
    "\n",
    "$${\n",
    "f^{\\prime}\\left(x\\right)=\\frac{e^{x}}{e^{x}+1}=\\frac{1}{1+e^{-x}}\n",
    "}$$\n",
    "\n",
    "# example: neural network classifier for MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "epoch: 0, cost: 173.803806047\n",
      "epoch: 1, cost: 44.0933057898\n",
      "epoch: 2, cost: 27.9979391154\n",
      "epoch: 3, cost: 19.8257013471\n",
      "epoch: 4, cost: 14.5126266484\n",
      "epoch: 5, cost: 10.80432725\n",
      "epoch: 6, cost: 8.09893777095\n",
      "epoch: 7, cost: 6.22545443229\n",
      "epoch: 8, cost: 4.47642658703\n",
      "epoch: 9, cost: 3.42179229905\n",
      "epoch: 10, cost: 2.61225969974\n",
      "epoch: 11, cost: 1.95967011973\n",
      "epoch: 12, cost: 1.47646401341\n",
      "epoch: 13, cost: 1.28384371805\n",
      "epoch: 14, cost: 0.973924499147\n",
      "accuracy: 0.945699989796\n",
      "\n",
      "label:\n",
      "[4]\n",
      "\n",
      "prediction:\n",
      "[4]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot = True)\n",
    "\n",
    "# parameters\n",
    "learning_rate   = 0.001\n",
    "training_epochs = 15\n",
    "batch_size      = 100\n",
    "\n",
    "# input place holders\n",
    "X = tf.placeholder(tf.float32, [None, 784])\n",
    "Y = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "# weights and bias for layers\n",
    "W1 = tf.Variable(tf.random_normal([784, 256]))\n",
    "b1 = tf.Variable(tf.random_normal([256]))\n",
    "L1 = tf.nn.relu(tf.matmul(X, W1) + b1)\n",
    "\n",
    "W2 = tf.Variable(tf.random_normal([256, 256]))\n",
    "b2 = tf.Variable(tf.random_normal([256]))\n",
    "L2 = tf.nn.relu(tf.matmul(L1, W2) + b2)\n",
    "\n",
    "W3 = tf.Variable(tf.random_normal([256, 10]))\n",
    "b3 = tf.Variable(tf.random_normal([10]))\n",
    "hypothesis = tf.matmul(L2, W3) + b3\n",
    "\n",
    "# define cost/loss and optimizer\n",
    "cost      = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = hypothesis, labels = Y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(cost)\n",
    "\n",
    "# initialize\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for epoch in range(training_epochs):\n",
    "\n",
    "    avg_cost = 0\n",
    "    total_batch = int(mnist.train.num_examples / batch_size)\n",
    "\n",
    "    for i in range(total_batch):\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "        feed_dict = {X: batch_xs, Y: batch_ys}\n",
    "        c, _ = sess.run([cost, optimizer], feed_dict = feed_dict)\n",
    "        avg_cost += c / total_batch\n",
    "\n",
    "    print(\"epoch: {epoch}, cost: {cost}\".format(\n",
    "        epoch = epoch,\n",
    "        cost  = avg_cost\n",
    "    ))\n",
    "\n",
    "# test accuracy\n",
    "correct_prediction = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "print(\"accuracy: {accuracy}\".format(\n",
    "    accuracy = sess.run(\n",
    "                   accuracy,\n",
    "                   feed_dict = {\n",
    "                                   X: mnist.test.images,\n",
    "                                   Y: mnist.test.labels\n",
    "                               }\n",
    "    )\n",
    "))\n",
    "\n",
    "# select one test example and predict\n",
    "r = random.randint(0, mnist.test.num_examples - 1)\n",
    "print(\"\\nlabel:\")\n",
    "print(sess.run(\n",
    "          tf.argmax(mnist.test.labels[r:r + 1], 1)\n",
    "      )\n",
    ")\n",
    "print(\"\\nprediction:\")\n",
    "print(sess.run(\n",
    "          tf.argmax(hypothesis, 1),\n",
    "          feed_dict = {X: mnist.test.images[r:r + 1]}\n",
    "      )\n",
    ")\n",
    "\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Xavier initialization\n",
    "\n",
    "Xavier initialization in neural networks involves initialization of the weights of a network so that the neuron activation functions are not starting out in saturated or dead regions. In effect, the weights are initialized with pseudorandom numbers that are not \"too small\" or \"too large\".\n",
    "\n",
    "If the input (from the transfer function) for a neuron is very large or very small, the activation function (hyperbolic tangent, for example) is saturated or stuck at +1 or -1 respectively. Having saturated neurons limits their dynamic range and, so, limits their representational power. How can one avoid getting stuck in such saturated regions? The transfer function is a sum of the products of weights and inputs. To avoid the transfer function being too large or too small, the weights and the input can be kept in some sensible range. The input, from data, can be restricted by normalizing the dataset using z-scaling or other methods (ensuring that the data has zero mean and unit variance). What about the weights?\n",
    "\n",
    "Technically the weights can be set to any pseudorandom values and then can be changed using a learning rile such as stochastic gradient descent to adjust them to minimize error. However, if weights are very large or very small or such that they cause the neuron to be saturated, then it takes gradient descent more iterations to adjust the weights.\n",
    "\n",
    "Xavier initialization suggests initializing the weights with a variance such that the variance of the transfer function is unity. Ensuring that this variance is unity reduces the likelihood of being stuck in saturated regions of the activation function.\n",
    "\n",
    "# example: neural network classifier with Xavier initialization for MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "epoch: 0, cost: 0.331354504322\n",
      "epoch: 1, cost: 0.119444444173\n",
      "epoch: 2, cost: 0.0798904514194\n",
      "epoch: 3, cost: 0.0552678741227\n",
      "epoch: 4, cost: 0.0411215884361\n",
      "epoch: 5, cost: 0.030692967015\n",
      "epoch: 6, cost: 0.0256018338039\n",
      "epoch: 7, cost: 0.0202623238294\n",
      "epoch: 8, cost: 0.0171883650348\n",
      "epoch: 9, cost: 0.0141780922553\n",
      "epoch: 10, cost: 0.0123592672347\n",
      "epoch: 11, cost: 0.011780318041\n",
      "epoch: 12, cost: 0.0112451218761\n",
      "epoch: 13, cost: 0.00916771249669\n",
      "epoch: 14, cost: 0.0129711465011\n",
      "accuracy: 0.977400004864\n",
      "\n",
      "label:\n",
      "[0]\n",
      "\n",
      "prediction:\n",
      "[0]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot = True)\n",
    "\n",
    "# parameters\n",
    "learning_rate   = 0.001\n",
    "training_epochs = 15\n",
    "batch_size      = 100\n",
    "\n",
    "# input place holders\n",
    "X = tf.placeholder(tf.float32, [None, 784])\n",
    "Y = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "# weights and bias for layers\n",
    "W1 = tf.get_variable(\"W1\", shape = [784, 256], initializer = tf.contrib.layers.xavier_initializer())\n",
    "b1 = tf.Variable(tf.random_normal([256]))\n",
    "L1 = tf.nn.relu(tf.matmul(X, W1) + b1)\n",
    "\n",
    "W2 = tf.get_variable(\"W2\", shape = [256, 256], initializer = tf.contrib.layers.xavier_initializer())\n",
    "b2 = tf.Variable(tf.random_normal([256]))\n",
    "L2 = tf.nn.relu(tf.matmul(L1, W2) + b2)\n",
    "\n",
    "W3 = tf.get_variable(\"W3\", shape = [256, 10], initializer = tf.contrib.layers.xavier_initializer())\n",
    "b3 = tf.Variable(tf.random_normal([10]))\n",
    "hypothesis = tf.matmul(L2, W3) + b3\n",
    "\n",
    "# define cost/loss and optimizer\n",
    "cost      = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = hypothesis, labels = Y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(cost)\n",
    "\n",
    "# initialize\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for epoch in range(training_epochs):\n",
    "\n",
    "    avg_cost = 0\n",
    "    total_batch = int(mnist.train.num_examples / batch_size)\n",
    "\n",
    "    for i in range(total_batch):\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "        feed_dict = {X: batch_xs, Y: batch_ys}\n",
    "        c, _ = sess.run([cost, optimizer], feed_dict = feed_dict)\n",
    "        avg_cost += c / total_batch\n",
    "\n",
    "    print(\"epoch: {epoch}, cost: {cost}\".format(\n",
    "        epoch = epoch,\n",
    "        cost  = avg_cost\n",
    "    ))\n",
    "\n",
    "# test accuracy\n",
    "correct_prediction = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "print(\"accuracy: {accuracy}\".format(\n",
    "    accuracy = sess.run(\n",
    "                   accuracy,\n",
    "                   feed_dict = {\n",
    "                                   X: mnist.test.images,\n",
    "                                   Y: mnist.test.labels\n",
    "                   }\n",
    "    )\n",
    "))\n",
    "\n",
    "# select one test example and predict\n",
    "r = random.randint(0, mnist.test.num_examples - 1)\n",
    "print(\"\\nlabel:\")\n",
    "print(sess.run(\n",
    "          tf.argmax(mnist.test.labels[r:r + 1], 1)\n",
    "      )\n",
    ")\n",
    "print(\"\\nprediction:\")\n",
    "print(sess.run(\n",
    "          tf.argmax(hypothesis, 1),\n",
    "          feed_dict = {X: mnist.test.images[r:r + 1]}\n",
    "      )\n",
    ")\n",
    "\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# example: deep neural network classifier with Xavier initialization for MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "epoch: 0, cost: 0.325931779084\n",
      "epoch: 1, cost: 0.107867569024\n",
      "epoch: 2, cost: 0.0705091328517\n",
      "epoch: 3, cost: 0.0552865000247\n",
      "epoch: 4, cost: 0.0430004646313\n",
      "epoch: 5, cost: 0.0329331435739\n",
      "epoch: 6, cost: 0.030525810439\n",
      "epoch: 7, cost: 0.0267394819635\n",
      "epoch: 8, cost: 0.0231419500274\n",
      "epoch: 9, cost: 0.0197627094166\n",
      "epoch: 10, cost: 0.0197169608473\n",
      "epoch: 11, cost: 0.015406726154\n",
      "epoch: 12, cost: 0.0196975519168\n",
      "epoch: 13, cost: 0.0184239531601\n",
      "epoch: 14, cost: 0.0138537911348\n",
      "accuracy: 0.980000019073\n",
      "\n",
      "label:\n",
      "[3]\n",
      "\n",
      "prediction:\n",
      "[3]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot = True)\n",
    "\n",
    "# parameters\n",
    "learning_rate   = 0.001\n",
    "training_epochs = 15\n",
    "batch_size      = 100\n",
    "\n",
    "# input place holders\n",
    "X = tf.placeholder(tf.float32, [None, 784])\n",
    "Y = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "# weights and bias for layers\n",
    "W1 = tf.get_variable(\"W1\", shape = [784, 512], initializer = tf.contrib.layers.xavier_initializer())\n",
    "b1 = tf.Variable(tf.random_normal([512]))\n",
    "L1 = tf.nn.relu(tf.matmul(X, W1) + b1)\n",
    "\n",
    "W2 = tf.get_variable(\"W2\", shape = [512, 512], initializer = tf.contrib.layers.xavier_initializer())\n",
    "b2 = tf.Variable(tf.random_normal([512]))\n",
    "L2 = tf.nn.relu(tf.matmul(L1, W2) + b2)\n",
    "\n",
    "W3 = tf.get_variable(\"W3\", shape = [512, 512], initializer = tf.contrib.layers.xavier_initializer())\n",
    "b3 = tf.Variable(tf.random_normal([512]))\n",
    "L3 = tf.nn.relu(tf.matmul(L2, W3) + b3)\n",
    "\n",
    "W4 = tf.get_variable(\"W4\", shape = [512, 512], initializer = tf.contrib.layers.xavier_initializer())\n",
    "b4 = tf.Variable(tf.random_normal([512]))\n",
    "L4 = tf.nn.relu(tf.matmul(L3, W4) + b4)\n",
    "\n",
    "W5 = tf.get_variable(\"W5\", shape = [512, 10], initializer = tf.contrib.layers.xavier_initializer())\n",
    "b5 = tf.Variable(tf.random_normal([10]))\n",
    "hypothesis = tf.matmul(L4, W5) + b5\n",
    "\n",
    "# define cost/loss and optimizer\n",
    "cost      = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = hypothesis, labels = Y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(cost)\n",
    "\n",
    "# initialize\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for epoch in range(training_epochs):\n",
    "\n",
    "    avg_cost = 0\n",
    "    total_batch = int(mnist.train.num_examples / batch_size)\n",
    "\n",
    "    for i in range(total_batch):\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "        feed_dict = {X: batch_xs, Y: batch_ys}\n",
    "        c, _ = sess.run([cost, optimizer], feed_dict = feed_dict)\n",
    "        avg_cost += c / total_batch\n",
    "\n",
    "    print(\"epoch: {epoch}, cost: {cost}\".format(\n",
    "        epoch = epoch,\n",
    "        cost  = avg_cost\n",
    "    ))\n",
    "\n",
    "# test accuracy\n",
    "correct_prediction = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "print(\"accuracy: {accuracy}\".format(\n",
    "    accuracy = sess.run(\n",
    "                   accuracy,\n",
    "                   feed_dict = {\n",
    "                                   X: mnist.test.images,\n",
    "                                   Y: mnist.test.labels\n",
    "                   }\n",
    "    )\n",
    "))\n",
    "\n",
    "# select one test example and predict\n",
    "r = random.randint(0, mnist.test.num_examples - 1)\n",
    "print(\"\\nlabel:\")\n",
    "print(sess.run(\n",
    "          tf.argmax(mnist.test.labels[r:r + 1], 1)\n",
    "      )\n",
    ")\n",
    "print(\"\\nprediction:\")\n",
    "print(sess.run(\n",
    "          tf.argmax(hypothesis, 1),\n",
    "          feed_dict = {X: mnist.test.images[r:r + 1]}\n",
    "      )\n",
    ")\n",
    "\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# example: deep neural network classifier with Xavier initialization and dropout for MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "epoch: 0, cost: 0.491187336401\n",
      "epoch: 1, cost: 0.174933730807\n",
      "epoch: 2, cost: 0.130297549333\n",
      "epoch: 3, cost: 0.11029827317\n",
      "epoch: 4, cost: 0.0910160335949\n",
      "epoch: 5, cost: 0.0845644689834\n",
      "epoch: 6, cost: 0.0707050579363\n",
      "epoch: 7, cost: 0.0713776325307\n",
      "epoch: 8, cost: 0.0625691854655\n",
      "epoch: 9, cost: 0.05885692129\n",
      "epoch: 10, cost: 0.0574975110159\n",
      "epoch: 11, cost: 0.052429081744\n",
      "epoch: 12, cost: 0.0469085732659\n",
      "epoch: 13, cost: 0.0468979393831\n",
      "epoch: 14, cost: 0.0453263855474\n",
      "accuracy: 0.982699990273\n",
      "\n",
      "label:\n",
      "[4]\n",
      "\n",
      "prediction:\n",
      "[4]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot = True)\n",
    "\n",
    "# parameters\n",
    "learning_rate   = 0.001\n",
    "training_epochs = 15\n",
    "batch_size      = 100\n",
    "\n",
    "# input place holders\n",
    "X = tf.placeholder(tf.float32, [None, 784])\n",
    "Y = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "# dropout (keep_prob) rate  0.7 on training, but should be 1 for testing\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "# weights and bias for layers\n",
    "W1 = tf.get_variable(\"W1\", shape = [784, 512], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b1 = tf.Variable(tf.random_normal([512]))\n",
    "L1 = tf.nn.relu(tf.matmul(X, W1) + b1)\n",
    "L1 = tf.nn.dropout(L1, keep_prob = keep_prob)\n",
    "\n",
    "W2 = tf.get_variable(\"W2\", shape = [512, 512], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b2 = tf.Variable(tf.random_normal([512]))\n",
    "L2 = tf.nn.relu(tf.matmul(L1, W2) + b2)\n",
    "L2 = tf.nn.dropout(L2, keep_prob = keep_prob)\n",
    "\n",
    "W3 = tf.get_variable(\"W3\", shape = [512, 512], initializer = tf.contrib.layers.xavier_initializer())\n",
    "b3 = tf.Variable(tf.random_normal([512]))\n",
    "L3 = tf.nn.relu(tf.matmul(L2, W3) + b3)\n",
    "L3 = tf.nn.dropout(L3, keep_prob = keep_prob)\n",
    "\n",
    "W4 = tf.get_variable(\"W4\", shape = [512, 512], initializer = tf.contrib.layers.xavier_initializer())\n",
    "b4 = tf.Variable(tf.random_normal([512]))\n",
    "L4 = tf.nn.relu(tf.matmul(L3, W4) + b4)\n",
    "L4 = tf.nn.dropout(L4, keep_prob = keep_prob)\n",
    "\n",
    "W5 = tf.get_variable(\"W5\", shape = [512, 10], initializer = tf.contrib.layers.xavier_initializer())\n",
    "b5 = tf.Variable(tf.random_normal([10]))\n",
    "hypothesis = tf.matmul(L4, W5) + b5\n",
    "\n",
    "# define cost/loss and optimizer\n",
    "cost      = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = hypothesis, labels = Y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(cost)\n",
    "\n",
    "# initialize\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for epoch in range(training_epochs):\n",
    "\n",
    "    avg_cost = 0\n",
    "    total_batch = int(mnist.train.num_examples / batch_size)\n",
    "\n",
    "    for i in range(total_batch):\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "        feed_dict = {X: batch_xs, Y: batch_ys, keep_prob: 0.7}\n",
    "        c, _ = sess.run([cost, optimizer], feed_dict = feed_dict)\n",
    "        avg_cost += c / total_batch\n",
    "\n",
    "    print(\"epoch: {epoch}, cost: {cost}\".format(\n",
    "        epoch = epoch,\n",
    "        cost  = avg_cost\n",
    "    ))\n",
    "\n",
    "# test accuracy\n",
    "correct_prediction = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "print(\"accuracy: {accuracy}\".format(\n",
    "    accuracy = sess.run(\n",
    "                   accuracy,\n",
    "                   feed_dict = {\n",
    "                                   X:         mnist.test.images,\n",
    "                                   Y:         mnist.test.labels,\n",
    "                                   keep_prob: 1\n",
    "                   }\n",
    "    )\n",
    "))\n",
    "\n",
    "# select one test example and predict\n",
    "r = random.randint(0, mnist.test.num_examples - 1)\n",
    "print(\"\\nlabel:\")\n",
    "print(sess.run(\n",
    "          tf.argmax(mnist.test.labels[r:r + 1], 1)\n",
    "      )\n",
    ")\n",
    "print(\"\\nprediction:\")\n",
    "print(sess.run(\n",
    "          tf.argmax(hypothesis, 1),\n",
    "          feed_dict = {X: mnist.test.images[r:r + 1], keep_prob: 1}\n",
    "      )\n",
    ")\n",
    "\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# convolutional neural network\n",
    "\n",
    "A convolutional neural network (CNN, ConvNet) is a type of feed-forward neural network in which the connectivity pattern between its neurons is inspired by the organization of the known animal visual corex. Individual cortical neurons respond to stimuli in a restricted region of space known as teh receptive field. The receptive fields of different neurons overlap partially such that they tile the visual field. The response of an individual neuron to stimuli within its receptive field can be approximated mathematically by a convolution operation. Convolutional networks were inspired by biological processes and are variations of multilayer perceptrons designed to use minimal amounts of preprocessing.\n",
    "\n",
    "## image recognition\n",
    "\n",
    "CNNs consist of multiple layers of receptive fields. These are neuron collections which process portions of the input image. The outputs of these collections are then tiled such that their input regions overlap in order to obtain a higher-resolution representation of the original image; this is repeated for every such layer. CNNs may include local or global pooling layers, which combine the outputs of neuron clusters. They also consist of various combinations of convolutional and fully-connected layers, with pointwise nonlinearity applied at the end of or after each layer. A convolutional operation on small regions of input is introduced to reduce the number of free parameters and to improve generalization. One advantage of networks is the use of shared weight in convolution layers, which means that the same filter (weights bank) is used for each pixel in the layer, which reduces memory footprint and improves performance.\n",
    "\n",
    "## layers\n",
    "\n",
    "A simple CNN is a sequence of layers, and every layer transforms one volume of activations to another through a differentiable function. There are three main types of layer: convolutional, pooling and fully-connected (exactly as seen in regular neural networks). These are stacked to form a full CNN architecture.\n",
    "\n",
    "## example architecture for CIFAR-10 classification: INPUT-CONV-RELU-POOL-FC\n",
    "\n",
    "![](https://raw.githubusercontent.com/wdbm/abstraction/master/media/CNN_1.png)\n",
    "\n",
    "|**layer**      |**description**                                                                                                                                                                                                                                                                                                 |\n",
    "|---------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
    "|INPUT [32x32x3]|Hold the raw pixel values of the image, in this case an image of width 32, height 32, and with three color channels RGB.                                                                                                                                                                                        |\n",
    "|CONV           |Compute the output of neurons that are connected to local regions in the input, each computing a dot product between their weights and a small region they are connected to in the input volume. This may result in a volume such as [32x32x12] when using 12 filters.                                          |\n",
    "|RELU           |Apply an element-wise activation function, such as ${\\textrm{max}\\left(0,x\\right)}$ thresholding at zero. This leaves the size of the volume unchanged ([32x32x12]).                                                                                                                                            |\n",
    "|POOL           |Perform a downsampling operation along the spatial dimensions (width, height) resulting in a volume such as [16x16x12].                                                                                                                                                                                         |\n",
    "|FC             |Compute class scores, resulting in a volume of size [1x1x10], where each of the 10 numbers correspond to a class score, such as the 10 categories of CIFAR-10. As with ordinary neural networks, and as the name implies, each neuron in this layer will be connected to all the numbers in the previous volume.|\n",
    "\n",
    "In this way, CNNs transform the original image layer by layer from the original pixel values to the final class scores. Some layers contain parameters and others don't. In particular, the CONV and FC layers perform transformations that are a function of not only the activations in the input volume but also of the parameters (the weights and biases of the neurons). Otherwise, teh RELU and POOL layers implement a fixed function. Tha parameters in the CONV and FC layers are trained with gradient descent so that the class scores that the CNN computes are consistent with the labels in the training set for each image.\n",
    "\n",
    "In summary, a CNN architecture is, in the simplest case, a list of layers that transform the image volume into an output volume (e.g. holding the class scores). There are a few distinct types of layers (e.g. CONV, FC, RELU, POOL are some of the most popular). Each layer accepts an input 3D volume and transforms it to an output 3D volume through a differentiable function. Each layer may or may not have parameters (e.g. CONV and FC do; RELU and POOL don't). Each layer may or may not have additional hyperparameters (e.g. CONV, FC and POOL do; RELU doesn't).\n",
    "\n",
    "### example representation of CNN architecture\n",
    "\n",
    "The initial volume stores the raw image pixels and the last colume stores the class scores. Each volume of activations along the processing path is shown as a colume. Since it is difficult to visualize 3D volumes, the slices of each volume are laid out in columns. The last layer volume holds the scores for each class, but here is shown only the sorted top 5 scores with the labels for each one displayed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# convolutional neural network architectures\n",
    "\n",
    "## some 2015 design guidelines\n",
    "\n",
    "Inception networks achieve high performance on a constrained parameter budget, so it is a reasonable place to start.\n",
    "\n",
    "- [Rethinking the Inception Architecture for Computer Vision](https://arxiv.org/abs/1512.00567)\n",
    "\n",
    "While increased model size and computational cost tend to translate to immediate quality gains for most tasks as long as enough labelled data is provided for training, computational efficiency and low parameter count are still enabling factors for various use cases such as mobile computers and big-data scenarios. It can be beneficial to explore ways to scale up networks in ways that aim at itilizing the added computation as efficiently as possible by suitable factorized convolutions and aggressive regularization. It can also be the case that gains in classification performance tend to transfer to significant quality gains in a wide variety of application domains.\n",
    "\n",
    "2015 GoogleNet used 5 million parameters and 2012 AlexNet used 60 million parameters.\n",
    "\n",
    "Some principles considered by Google are as follows:\n",
    "\n",
    "- Use stacks of smaller receptive field convolutional layers instead of using a single large receptive field convolutional layer, i.e. 2 stacks of 3x3 conv layers vs a single 7x7 convolutional layer. This approach is motivated by the need to be parameter efficient. It also has the dual effect of more representational capacity as more nonlinearity is introduced with more layers.\n",
    "- Convolutional layers could be factorizing into deep layers. So, instead of having a single 7x7 convolutional layer, there could be a 1x7 convolutional layer and then a 7x1 convolutional layer. This approach adds more depth and is parameter efficient.\n",
    "- Balance the depth and width of the network. Use high dimensional representations. This is one of the principles behind Inception modules, which concatenate multiple convolutional layers together. So even for a small spatial size in a convolutional network, Inception modules provide high dimensional representation via multi-scale convolutional concatenation: 1x1, 3x3, 3x3-3x3 and max pool all put together. These Inception modules have a \"width\" since they can be interpreted as performing multiple operations in parallel. New Inception modules for further by having factorized convolutional sizes: 1x3, 3x1, etc.\n",
    "- Use 1x1 convolutional layers to reduce dimensionality. Many dimensionality reduction techniques can be used to achieve parameter efficiency. Google believes that this is effective because adjacent feature maps have highly-correlated outputs. This is sensibile for natural images because they are known to exhibit some local statistical properties consistent with this. So, reducing dimensionality via 1x1 NIN (Network in Network) layers does not have a disastrous effect on representational power.\n",
    "\n",
    "## Network in Network\n",
    "\n",
    "- [Network in Network](https://arxiv.org/abs/1312.4400)\n",
    "\n",
    "Network in Network is a deep network structure that enhances model discriminability for local patches in the receptive field. 2014 conventional convolutional layers use linear filters followed by a nonlinear activation function to scan the input. For NIN, micro neural networks with more complex structures (such as a multilayer perceptron) are used to abstract the data within the receptive field. Feature maps are obtained by sliding the micro networks over the input in a manner similar to that in a convolutional neural network and are then fed to the next layer. Deep NIN can be implemented by stacking multiples of the structure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# toy image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 2, 2, 1)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAARUAAAD8CAYAAABZ0jAcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nNWWoSGrKUJHUlKEiqan/Ax2NxrvCk/9vAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fe8fc070910>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "image = np.array(\n",
    "            [[\n",
    "                [[1], [2]],\n",
    "                [[2], [3]]\n",
    "            ]],\n",
    "            dtype = np.float32\n",
    "        )\n",
    "\n",
    "print(image.shape)\n",
    "plt.imshow(image.reshape(2, 2), cmap = \"Greys\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# example: convolutional neural network classifier for MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "epoch: 0, cost: 0.408241077072\n",
      "epoch: 1, cost: 0.113866317177\n",
      "epoch: 2, cost: 0.0791251511245\n",
      "epoch: 3, cost: 0.0653506306779\n",
      "epoch: 4, cost: 0.0535522125822\n",
      "epoch: 5, cost: 0.0458742706338\n",
      "epoch: 6, cost: 0.0414381767204\n",
      "epoch: 7, cost: 0.0354744575647\n",
      "epoch: 8, cost: 0.0327315344029\n",
      "epoch: 9, cost: 0.0287840995411\n",
      "epoch: 10, cost: 0.0251736668172\n",
      "epoch: 11, cost: 0.0224041748257\n",
      "epoch: 12, cost: 0.020394609374\n",
      "epoch: 13, cost: 0.0169807871724\n",
      "epoch: 14, cost: 0.0157285714376\n",
      "accuracy: 0.988300025463\n",
      "\n",
      "label:\n",
      "[2]\n",
      "\n",
      "prediction:\n",
      "[2]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n+IGgCD8QFOEHgiL8QFD/ANglSEscWva0AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f594628ad50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot = True)\n",
    "\n",
    "# parameters\n",
    "learning_rate   = 0.001\n",
    "training_epochs = 15\n",
    "batch_size      = 100\n",
    "\n",
    "# input place holders\n",
    "X     = tf.placeholder(tf.float32, [None, 784])\n",
    "X_img = tf.reshape(X, [-1, 28, 28, 1]) # 28x28x1 (black/white)\n",
    "Y     = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "# L1 ImgIn shape = (?, 28, 28, 1)\n",
    "W1 = tf.Variable(tf.random_normal([3, 3, 1, 32], stddev = 0.01))\n",
    "# Conv -> (?, 28, 28, 32)\n",
    "# Pool -> (?, 14, 14, 32)\n",
    "L1 = tf.nn.conv2d(X_img, W1, strides = [1, 1, 1, 1], padding = \"SAME\")\n",
    "L1 = tf.nn.relu(L1)\n",
    "L1 = tf.nn.max_pool(L1, ksize = [1, 2, 2, 1], strides = [1, 2, 2, 1], padding =  \"SAME\")\n",
    "\"\"\"\n",
    "Tensor(\"Conv2D:0\",  shape = (?, 28, 28, 32), dtype = float32)\n",
    "Tensor(\"Relu:0\",    shape = (?, 28, 28, 32), dtype = float32)\n",
    "Tensor(\"MaxPool:0\", shape = (?, 14, 14, 32), dtype = float32)\n",
    "\"\"\"\n",
    "\n",
    "# L2 ImgIn shape=(?, 14, 14, 32)\n",
    "W2 = tf.Variable(tf.random_normal([3, 3, 32, 64], stddev = 0.01))\n",
    "#    Conv      ->(?, 14, 14, 64)\n",
    "#    Pool      ->(?, 7, 7, 64)\n",
    "L2 = tf.nn.conv2d(L1, W2, strides = [1, 1, 1, 1], padding = \"SAME\")\n",
    "L2 = tf.nn.relu(L2)\n",
    "L2 = tf.nn.max_pool(L2, ksize = [1, 2, 2, 1], strides = [1, 2, 2, 1], padding = \"SAME\")\n",
    "L2 = tf.reshape(L2, [-1, 7 * 7 * 64])\n",
    "\"\"\"\n",
    "Tensor(\"Conv2D_1:0\",  shape = (?, 14, 14, 64), dtype = float32)\n",
    "Tensor(\"Relu_1:0\",    shape = (?, 14, 14, 64), dtype = float32)\n",
    "Tensor(\"MaxPool_1:0\", shape = (?, 7, 7, 64),   dtype = float32)\n",
    "Tensor(\"Reshape_1:0\", shape = (?, 3136),       dtype = float32)\n",
    "\"\"\"\n",
    "\n",
    "# final FC 7x7x64 inputs -> 10 outputs\n",
    "W3 = tf.get_variable(\"W3\", shape=[7 * 7 * 64, 10], initializer = tf.contrib.layers.xavier_initializer())\n",
    "b = tf.Variable(tf.random_normal([10]))\n",
    "hypothesis = tf.matmul(L2, W3) + b\n",
    "\n",
    "# define cost/loss and optimizer\n",
    "cost      = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = hypothesis, labels = Y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(cost)\n",
    "\n",
    "# initialize\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for epoch in range(training_epochs):\n",
    "\n",
    "    avg_cost = 0\n",
    "    total_batch = int(mnist.train.num_examples / batch_size)\n",
    "\n",
    "    for i in range(total_batch):\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "        feed_dict = {X: batch_xs, Y: batch_ys}\n",
    "        c, _ = sess.run([cost, optimizer], feed_dict = feed_dict)\n",
    "        avg_cost += c / total_batch\n",
    "\n",
    "    print(\"epoch: {epoch}, cost: {cost}\".format(\n",
    "        epoch = epoch,\n",
    "        cost  = avg_cost\n",
    "    ))\n",
    "\n",
    "# test accuracy\n",
    "correct_prediction = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "print(\"accuracy: {accuracy}\".format(\n",
    "    accuracy = sess.run(\n",
    "                   accuracy,\n",
    "                   feed_dict = {\n",
    "                                   X:         mnist.test.images,\n",
    "                                   Y:         mnist.test.labels\n",
    "                   }\n",
    "    )\n",
    "))\n",
    "\n",
    "# select one test example and predict\n",
    "r = random.randint(0, mnist.test.num_examples - 1)\n",
    "print(\"\\nlabel:\")\n",
    "print(sess.run(\n",
    "          tf.argmax(mnist.test.labels[r:r + 1], 1)\n",
    "      )\n",
    ")\n",
    "print(\"\\nprediction:\")\n",
    "print(sess.run(\n",
    "          tf.argmax(hypothesis, 1),\n",
    "          feed_dict = {X: mnist.test.images[r:r + 1]}\n",
    "      )\n",
    ")\n",
    "\n",
    "sess.close()\n",
    "\n",
    "plt.imshow(\n",
    "    mnist.test.images[r:r + 1].\n",
    "    reshape(28, 28),\n",
    "    cmap = \"Greys\",\n",
    "    interpolation = \"nearest\"\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST comparison using identical learning rate, training epochs and batch size\n",
    "\n",
    "|**classifier**                                                                 |**accuracy**|\n",
    "|-------------------------------------------------------------------------------|------------|\n",
    "|softmax classifier for MNIST                                                   |89.76%      |\n",
    "|neural network classifier for MNIST                                            |94.57%      |\n",
    "|neural network classifier with Xavier initialization for MNIST                 |97.74%      |\n",
    "|deep neural network classifier with Xavier initialization for MNIST            |98%         |\n",
    "|deep neural network classifier with Xavier initialization and dropout for MNIST|98.26%      |\n",
    "|convolutional neural network classifier for MNIST                              |98.83%      |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [dimensionality reduction](https://github.com/wdbm/Psychedelic_Machine_Learning_in_the_Cenozoic_Era/blob/master/dimensionality_reduction.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# hyperparameter optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scikit-Optimize\n",
    "\n",
    "Scikit-Optimize is a library for minimization of expensive and noisy black-box functions. It implements several methods of sequential model-based optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### example: find a minimum of a noisy function ${f(x)}$ over the range ${-2 < x < 2}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/skopt/optimizer/optimizer.py:195: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "/usr/local/lib/python3.5/dist-packages/skopt/optimizer/optimizer.py:195: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "/usr/local/lib/python3.5/dist-packages/skopt/optimizer/optimizer.py:195: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "/usr/local/lib/python3.5/dist-packages/skopt/optimizer/optimizer.py:195: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "/usr/local/lib/python3.5/dist-packages/skopt/optimizer/optimizer.py:195: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "/usr/local/lib/python3.5/dist-packages/skopt/optimizer/optimizer.py:195: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "/usr/local/lib/python3.5/dist-packages/skopt/optimizer/optimizer.py:195: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "/usr/local/lib/python3.5/dist-packages/skopt/optimizer/optimizer.py:195: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "/usr/local/lib/python3.5/dist-packages/skopt/optimizer/optimizer.py:195: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "/usr/local/lib/python3.5/dist-packages/skopt/optimizer/optimizer.py:195: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "/usr/local/lib/python3.5/dist-packages/skopt/optimizer/optimizer.py:195: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "/usr/local/lib/python3.5/dist-packages/skopt/optimizer/optimizer.py:195: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "/usr/local/lib/python3.5/dist-packages/skopt/optimizer/optimizer.py:195: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "/usr/local/lib/python3.5/dist-packages/skopt/optimizer/optimizer.py:195: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "/usr/local/lib/python3.5/dist-packages/skopt/optimizer/optimizer.py:195: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "/usr/local/lib/python3.5/dist-packages/skopt/optimizer/optimizer.py:195: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "/usr/local/lib/python3.5/dist-packages/skopt/optimizer/optimizer.py:195: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "/usr/local/lib/python3.5/dist-packages/skopt/optimizer/optimizer.py:195: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "/usr/local/lib/python3.5/dist-packages/skopt/optimizer/optimizer.py:195: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "/usr/local/lib/python3.5/dist-packages/skopt/optimizer/optimizer.py:195: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "/usr/local/lib/python3.5/dist-packages/skopt/optimizer/optimizer.py:195: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "/usr/local/lib/python3.5/dist-packages/skopt/optimizer/optimizer.py:195: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "/usr/local/lib/python3.5/dist-packages/skopt/optimizer/optimizer.py:195: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "/usr/local/lib/python3.5/dist-packages/skopt/optimizer/optimizer.py:195: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "/usr/local/lib/python3.5/dist-packages/skopt/optimizer/optimizer.py:195: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "/usr/local/lib/python3.5/dist-packages/skopt/optimizer/optimizer.py:195: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "/usr/local/lib/python3.5/dist-packages/skopt/optimizer/optimizer.py:195: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "/usr/local/lib/python3.5/dist-packages/skopt/optimizer/optimizer.py:195: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "          fun: -0.23204481252472509\n",
       "    func_vals: array([  1.44257961e-02,   7.79354128e-03,  -4.70039946e-03,\n",
       "        -3.67973598e-03,  -7.06116640e-04,  -3.17876973e-03,\n",
       "        -1.44235256e-03,   5.39645573e-04,   4.75153843e-04,\n",
       "         4.75428182e-04,   5.27728488e-05,   2.07094186e-05,\n",
       "         8.34867503e-06,  -6.00693286e-05,   6.31535665e-05,\n",
       "         2.30818028e-05,  -1.83723410e-05,   2.52391092e-05,\n",
       "        -1.34220494e-05,  -9.12747410e-05,  -2.29302043e-05,\n",
       "        -9.61903722e-02,   4.18629563e-03,  -3.07422229e-06,\n",
       "         1.66460632e-01,  -5.47278093e-02,  -2.04790248e-05,\n",
       "         4.01509834e-02,  -3.70783191e-02,   6.15397586e-05,\n",
       "        -2.32044813e-01,   3.22385062e-05,   8.29354523e-02,\n",
       "        -1.82539994e-05,   7.53359763e-06,  -1.70819195e-02,\n",
       "         6.39289743e-05,   5.03488878e-06,  -2.82948447e-05,\n",
       "         6.85763351e-02,  -3.42105727e-06,  -2.14759167e-05,\n",
       "        -1.57368452e-05,   2.38449956e-05,   1.86841011e-05,\n",
       "         6.48748142e-02,   7.54032664e-05,   1.66894467e-05,\n",
       "        -4.99040026e-05,   5.81932537e-05,  -3.21077277e-05,\n",
       "         4.33375691e-06,   9.65432736e-06,   4.95051479e-05,\n",
       "         1.86522962e-05,   2.01055127e-05,  -2.52972657e-05,\n",
       "        -8.56401492e-05,  -8.04704032e-05,  -3.80779530e-03,\n",
       "        -8.40108248e-05,   1.01845311e-02,  -3.04986836e-03,\n",
       "         7.23193795e-03,   1.60419970e-02,   4.19465133e-03,\n",
       "         1.24958410e-05,  -3.70643663e-03,   4.71660085e-05,\n",
       "         6.03468410e-02,  -2.76435624e-02,  -1.38403075e-05,\n",
       "        -1.57595378e-01,   6.87661142e-05,  -3.18549641e-02,\n",
       "         7.59396269e-05,  -7.73056702e-07,  -1.59322147e-05,\n",
       "        -6.63808879e-04,  -6.28487623e-05,  -4.84323613e-05,\n",
       "         2.43523379e-03,  -4.67712983e-05,   1.03408529e-01,\n",
       "         1.87549920e-02,  -6.05231414e-05,   7.90750096e-04,\n",
       "        -4.22485582e-04,   2.39123169e-03,  -7.54742993e-06,\n",
       "        -4.31402528e-05,  -5.60291048e-03,  -1.10551388e-05,\n",
       "         8.33402303e-03,   1.39441107e-02,  -2.32482821e-02,\n",
       "         8.45500222e-06,  -3.05160864e-05,  -1.84747777e-04,\n",
       "        -2.51605276e-02])\n",
       "       models: [GaussianProcessRegressor(alpha=0.0, copy_X_train=True,\n",
       "             kernel=1**2 * Matern(length_scale=1, nu=2.5) + WhiteKernel(noise_level=1),\n",
       "             n_restarts_optimizer=2, noise='gaussian', normalize_y=True,\n",
       "             optimizer='fmin_l_bfgs_b',\n",
       "             random_state=<mtrand.RandomState object at 0x7fed55ee5630>), GaussianProcessRegressor(alpha=0.0, copy_X_train=True,\n",
       "             kernel=1**2 * Matern(length_scale=1, nu=2.5) + WhiteKernel(noise_level=1),\n",
       "             n_restarts_optimizer=2, noise='gaussian', normalize_y=True,\n",
       "             optimizer='fmin_l_bfgs_b',\n",
       "             random_state=<mtrand.RandomState object at 0x7fed55ee59d8>), GaussianProcessRegressor(alpha=0.0, copy_X_train=True,\n",
       "             kernel=1**2 * Matern(length_scale=1, nu=2.5) + WhiteKernel(noise_level=1),\n",
       "             n_restarts_optimizer=2, noise='gaussian', normalize_y=True,\n",
       "             optimizer='fmin_l_bfgs_b',\n",
       "             random_state=<mtrand.RandomState object at 0x7fed55ee5870>), GaussianProcessRegressor(alpha=0.0, copy_X_train=True,\n",
       "             kernel=1**2 * Matern(length_scale=1, nu=2.5) + WhiteKernel(noise_level=1),\n",
       "             n_restarts_optimizer=2, noise='gaussian', normalize_y=True,\n",
       "             optimizer='fmin_l_bfgs_b',\n",
       "             random_state=<mtrand.RandomState object at 0x7fed55ee5948>), GaussianProcessRegressor(alpha=0.0, copy_X_train=True,\n",
       "             kernel=1**2 * Matern(length_scale=1, nu=2.5) + WhiteKernel(noise_level=1),\n",
       "             n_restarts_optimizer=2, noise='gaussian', normalize_y=True,\n",
       "             optimizer='fmin_l_bfgs_b',\n",
       "             random_state=<mtrand.RandomState object at 0x7fed55ee57e0>), GaussianProcessRegressor(alpha=0.0, copy_X_train=True,\n",
       "             kernel=1**2 * Matern(length_scale=1, nu=2.5) + WhiteKernel(noise_level=1),\n",
       "             n_restarts_optimizer=2, noise='gaussian', normalize_y=True,\n",
       "             optimizer='fmin_l_bfgs_b',\n",
       "             random_state=<mtrand.RandomState object at 0x7fed55ee5678>), GaussianProcessRegressor(alpha=0.0, copy_X_train=True,\n",
       "             kernel=1**2 * Matern(length_scale=1, nu=2.5) + WhiteKernel(noise_level=1),\n",
       "             n_restarts_optimizer=2, noise='gaussian', normalize_y=True,\n",
       "             optimizer='fmin_l_bfgs_b',\n",
       "             random_state=<mtrand.RandomState object at 0x7fed55ee5900>), GaussianProcessRegressor(alpha=0.0, copy_X_train=True,\n",
       "             kernel=1**2 * Matern(length_scale=1, nu=2.5) + WhiteKernel(noise_level=1),\n",
       "             n_restarts_optimizer=2, noise='gaussian', normalize_y=True,\n",
       "             optimizer='fmin_l_bfgs_b',\n",
       "             random_state=<mtrand.RandomState object at 0x7fed55ee53a8>), GaussianProcessRegressor(alpha=0.0, copy_X_train=True,\n",
       "             kernel=1**2 * Matern(length_scale=1, nu=2.5) + WhiteKernel(noise_level=1),\n",
       "             n_restarts_optimizer=2, noise='gaussian', normalize_y=True,\n",
       "             optimizer='fmin_l_bfgs_b',\n",
       "             random_state=<mtrand.RandomState object at 0x7fed55ee56c0>), GaussianProcessRegressor(alpha=0.0, copy_X_train=True,\n",
       "             kernel=1**2 * Matern(length_scale=1, nu=2.5) + WhiteKernel(noise_level=1),\n",
       "             n_restarts_optimizer=2, noise='gaussian', normalize_y=True,\n",
       "             optimizer='fmin_l_bfgs_b',\n",
       "             random_state=<mtrand.RandomState object at 0x7fed55ee54c8>), GaussianProcessRegressor(alpha=0.0, copy_X_train=True,\n",
       "             kernel=1**2 * Matern(length_scale=1, nu=2.5) + WhiteKernel(noise_level=1),\n",
       "             n_restarts_optimizer=2, noise='gaussian', normalize_y=True,\n",
       "             optimizer='fmin_l_bfgs_b',\n",
       "             random_state=<mtrand.RandomState object at 0x7fed55ee5510>), GaussianProcessRegressor(alpha=0.0, copy_X_train=True,\n",
       "             kernel=1**2 * Matern(length_scale=1, nu=2.5) + WhiteKernel(noise_level=1),\n",
       "             n_restarts_optimizer=2, noise='gaussian', normalize_y=True,\n",
       "             optimizer='fmin_l_bfgs_b',\n",
       "             random_state=<mtrand.RandomState object at 0x7fed55ee55e8>), GaussianProcessRegressor(alpha=0.0, copy_X_train=True,\n",
       "             kernel=1**2 * Matern(length_scale=1, nu=2.5) + WhiteKernel(noise_level=1),\n",
       "             n_restarts_optimizer=2, noise='gaussian', normalize_y=True,\n",
       "             optimizer='fmin_l_bfgs_b',\n",
       "             random_state=<mtrand.RandomState object at 0x7fed55ee5a68>), GaussianProcessRegressor(alpha=0.0, copy_X_train=True,\n",
       "             kernel=1**2 * Matern(length_scale=1, nu=2.5) + WhiteKernel(noise_level=1),\n",
       "             n_restarts_optimizer=2, noise='gaussian', normalize_y=True,\n",
       "             optimizer='fmin_l_bfgs_b',\n",
       "             random_state=<mtrand.RandomState object at 0x7fed55ee58b8>), GaussianProcessRegressor(alpha=0.0, copy_X_train=True,\n",
       "             kernel=1**2 * Matern(length_scale=1, nu=2.5) + WhiteKernel(noise_level=1),\n",
       "             n_restarts_optimizer=2, noise='gaussian', normalize_y=True,\n",
       "             optimizer='fmin_l_bfgs_b',\n",
       "             random_state=<mtrand.RandomState object at 0x7fed55ee50d8>), GaussianProcessRegressor(alpha=0.0, copy_X_train=True,\n",
       "             kernel=1**2 * Matern(length_scale=1, nu=2.5) + WhiteKernel(noise_level=1),\n",
       "             n_restarts_optimizer=2, noise='gaussian', normalize_y=True,\n",
       "             optimizer='fmin_l_bfgs_b',\n",
       "             random_state=<mtrand.RandomState object at 0x7fed55ee5b40>), GaussianProcessRegressor(alpha=0.0, copy_X_train=True,\n",
       "             kernel=1**2 * Matern(length_scale=1, nu=2.5) + WhiteKernel(noise_level=1),\n",
       "             n_restarts_optimizer=2, noise='gaussian', normalize_y=True,\n",
       "             optimizer='fmin_l_bfgs_b',\n",
       "             random_state=<mtrand.RandomState object at 0x7fed55ee55a0>), GaussianProcessRegressor(alpha=0.0, copy_X_train=True,\n",
       "             kernel=1**2 * Matern(length_scale=1, nu=2.5) + WhiteKernel(noise_level=1),\n",
       "             n_restarts_optimizer=2, noise='gaussian', normalize_y=True,\n",
       "             optimizer='fmin_l_bfgs_b',\n",
       "             random_state=<mtrand.RandomState object at 0x7fed55ee5318>), GaussianProcessRegressor(alpha=0.0, copy_X_train=True,\n",
       "             kernel=1**2 * Matern(length_scale=1, nu=2.5) + WhiteKernel(noise_level=1),\n",
       "             n_restarts_optimizer=2, noise='gaussian', normalize_y=True,\n",
       "             optimizer='fmin_l_bfgs_b',\n",
       "             random_state=<mtrand.RandomState object at 0x7fed55ee5438>), GaussianProcessRegressor(alpha=0.0, copy_X_train=True,\n",
       "             kernel=1**2 * Matern(length_scale=1, nu=2.5) + WhiteKernel(noise_level=1),\n",
       "             n_restarts_optimizer=2, noise='gaussian', normalize_y=True,\n",
       "             optimizer='fmin_l_bfgs_b',\n",
       "             random_state=<mtrand.RandomState object at 0x7fed55ee5b88>), GaussianProcessRegressor(alpha=0.0, copy_X_train=True,\n",
       "             kernel=1**2 * Matern(length_scale=1, nu=2.5) + WhiteKernel(noise_level=1),\n",
       "             n_restarts_optimizer=2, noise='gaussian', normalize_y=True,\n",
       "             optimizer='fmin_l_bfgs_b',\n",
       "             random_state=<mtrand.RandomState object at 0x7fed55ee5828>), GaussianProcessRegressor(alpha=0.0, copy_X_train=True,\n",
       "             kernel=1**2 * Matern(length_scale=1, nu=2.5) + WhiteKernel(noise_level=1),\n",
       "             n_restarts_optimizer=2, noise='gaussian', normalize_y=True,\n",
       "             optimizer='fmin_l_bfgs_b',\n",
       "             random_state=<mtrand.RandomState object at 0x7fed55ee5750>), GaussianProcessRegressor(alpha=0.0, copy_X_train=True,\n",
       "             kernel=1**2 * Matern(length_scale=1, nu=2.5) + WhiteKernel(noise_level=1),\n",
       "             n_restarts_optimizer=2, noise='gaussian', normalize_y=True,\n",
       "             optimizer='fmin_l_bfgs_b',\n",
       "             random_state=<mtrand.RandomState object at 0x7fed55ee5c18>), GaussianProcessRegressor(alpha=0.0, copy_X_train=True,\n",
       "             kernel=1**2 * Matern(length_scale=1, nu=2.5) + WhiteKernel(noise_level=1),\n",
       "             n_restarts_optimizer=2, noise='gaussian', normalize_y=True,\n",
       "             optimizer='fmin_l_bfgs_b',\n",
       "             random_state=<mtrand.RandomState object at 0x7fed55ee5af8>), GaussianProcessRegressor(alpha=0.0, copy_X_train=True,\n",
       "             kernel=1**2 * Matern(length_scale=1, nu=2.5) + WhiteKernel(noise_level=1),\n",
       "             n_restarts_optimizer=2, noise='gaussian', normalize_y=True,\n",
       "             optimizer='fmin_l_bfgs_b',\n",
       "             random_state=<mtrand.RandomState object at 0x7fed55ee5a20>), GaussianProcessRegressor(alpha=0.0, copy_X_train=True,\n",
       "             kernel=1**2 * Matern(length_scale=1, nu=2.5) + WhiteKernel(noise_level=1),\n",
       "             n_restarts_optimizer=2, noise='gaussian', normalize_y=True,\n",
       "             optimizer='fmin_l_bfgs_b',\n",
       "             random_state=<mtrand.RandomState object at 0x7fed55ee5c60>), GaussianProcessRegressor(alpha=0.0, copy_X_train=True,\n",
       "             kernel=1**2 * Matern(length_scale=1, nu=2.5) + WhiteKernel(noise_level=1),\n",
       "             n_restarts_optimizer=2, noise='gaussian', normalize_y=True,\n",
       "             optimizer='fmin_l_bfgs_b',\n",
       "             random_state=<mtrand.RandomState object at 0x7fed55ee5990>), GaussianProcessRegressor(alpha=0.0, copy_X_train=True,\n",
       "             kernel=1**2 * Matern(length_scale=1, nu=2.5) + WhiteKernel(noise_level=1),\n",
       "             n_restarts_optimizer=2, noise='gaussian', normalize_y=True,\n",
       "             optimizer='fmin_l_bfgs_b',\n",
       "             random_state=<mtrand.RandomState object at 0x7fed55ee5798>), GaussianProcessRegressor(alpha=0.0, copy_X_train=True,\n",
       "             kernel=1**2 * Matern(length_scale=1, nu=2.5) + WhiteKernel(noise_level=1),\n",
       "             n_restarts_optimizer=2, noise='gaussian', normalize_y=True,\n",
       "             optimizer='fmin_l_bfgs_b',\n",
       "             random_state=<mtrand.RandomState object at 0x7fed55ee5d80>), GaussianProcessRegressor(alpha=0.0, copy_X_train=True,\n",
       "             kernel=1**2 * Matern(length_scale=1, nu=2.5) + WhiteKernel(noise_level=1),\n",
       "             n_restarts_optimizer=2, noise='gaussian', normalize_y=True,\n",
       "             optimizer='fmin_l_bfgs_b',\n",
       "             random_state=<mtrand.RandomState object at 0x7fed55ee5cf0>), GaussianProcessRegressor(alpha=0.0, copy_X_train=True,\n",
       "             kernel=1**2 * Matern(length_scale=1, nu=2.5) + WhiteKernel(noise_level=1),\n",
       "             n_restarts_optimizer=2, noise='gaussian', normalize_y=True,\n",
       "             optimizer='fmin_l_bfgs_b',\n",
       "             random_state=<mtrand.RandomState object at 0x7fed55ee5bd0>), GaussianProcessRegressor(alpha=0.0, copy_X_train=True,\n",
       "             kernel=1**2 * Matern(length_scale=1, nu=2.5) + WhiteKernel(noise_level=1),\n",
       "             n_restarts_optimizer=2, noise='gaussian', normalize_y=True,\n",
       "             optimizer='fmin_l_bfgs_b',\n",
       "             random_state=<mtrand.RandomState object at 0x7fed55ee5ee8>), GaussianProcessRegressor(alpha=0.0, copy_X_train=True,\n",
       "             kernel=1**2 * Matern(length_scale=1, nu=2.5) + WhiteKernel(noise_level=1),\n",
       "             n_restarts_optimizer=2, noise='gaussian', normalize_y=True,\n",
       "             optimizer='fmin_l_bfgs_b',\n",
       "             random_state=<mtrand.RandomState object at 0x7fed55ee5ab0>), GaussianProcessRegressor(alpha=0.0, copy_X_train=True,\n",
       "             kernel=1**2 * Matern(length_scale=1, nu=2.5) + WhiteKernel(noise_level=1),\n",
       "             n_restarts_optimizer=2, noise='gaussian', normalize_y=True,\n",
       "             optimizer='fmin_l_bfgs_b',\n",
       "             random_state=<mtrand.RandomState object at 0x7fed55ee5f30>), GaussianProcessRegressor(alpha=0.0, copy_X_train=True,\n",
       "             kernel=1**2 * Matern(length_scale=1, nu=2.5) + WhiteKernel(noise_level=1),\n",
       "             n_restarts_optimizer=2, noise='gaussian', normalize_y=True,\n",
       "             optimizer='fmin_l_bfgs_b',\n",
       "             random_state=<mtrand.RandomState object at 0x7fed55ee5ea0>), GaussianProcessRegressor(alpha=0.0, copy_X_train=True,\n",
       "             kernel=1**2 * Matern(length_scale=1, nu=2.5) + WhiteKernel(noise_level=1),\n",
       "             n_restarts_optimizer=2, noise='gaussian', normalize_y=True,\n",
       "             optimizer='fmin_l_bfgs_b',\n",
       "             random_state=<mtrand.RandomState object at 0x7fed55ee5fc0>), GaussianProcessRegressor(alpha=0.0, copy_X_train=True,\n",
       "             kernel=1**2 * Matern(length_scale=1, nu=2.5) + WhiteKernel(noise_level=1),\n",
       "             n_restarts_optimizer=2, noise='gaussian', normalize_y=True,\n",
       "             optimizer='fmin_l_bfgs_b',\n",
       "             random_state=<mtrand.RandomState object at 0x7fed55ee5dc8>), GaussianProcessRegressor(alpha=0.0, copy_X_train=True,\n",
       "             kernel=1**2 * Matern(length_scale=1, nu=2.5) + WhiteKernel(noise_level=1),\n",
       "             n_restarts_optimizer=2, noise='gaussian', normalize_y=True,\n",
       "             optimizer='fmin_l_bfgs_b',\n",
       "             random_state=<mtrand.RandomState object at 0x7fed55ee5e10>), GaussianProcessRegressor(alpha=0.0, copy_X_train=True,\n",
       "             kernel=1**2 * Matern(length_scale=1, nu=2.5) + WhiteKernel(noise_level=1),\n",
       "             n_restarts_optimizer=2, noise='gaussian', normalize_y=True,\n",
       "             optimizer='fmin_l_bfgs_b',\n",
       "             random_state=<mtrand.RandomState object at 0x7fed55ee5e58>), GaussianProcessRegressor(alpha=0.0, copy_X_train=True,\n",
       "             kernel=1**2 * Matern(length_scale=1, nu=2.5) + WhiteKernel(noise_level=1),\n",
       "             n_restarts_optimizer=2, noise='gaussian', normalize_y=True,\n",
       "             optimizer='fmin_l_bfgs_b',\n",
       "             random_state=<mtrand.RandomState object at 0x7fed55ee5d38>), GaussianProcessRegressor(alpha=0.0, copy_X_train=True,\n",
       "             kernel=1**2 * Matern(length_scale=1, nu=2.5) + WhiteKernel(noise_level=1),\n",
       "             n_restarts_optimizer=2, noise='gaussian', normalize_y=True,\n",
       "             optimizer='fmin_l_bfgs_b',\n",
       "             random_state=<mtrand.RandomState object at 0x7fed53e9e2d0>), GaussianProcessRegressor(alpha=0.0, copy_X_train=True,\n",
       "             kernel=1**2 * Matern(length_scale=1, nu=2.5) + WhiteKernel(noise_level=1),\n",
       "             n_restarts_optimizer=2, noise='gaussian', normalize_y=True,\n",
       "             optimizer='fmin_l_bfgs_b',\n",
       "             random_state=<mtrand.RandomState object at 0x7fed53e9e120>), GaussianProcessRegressor(alpha=0.0, copy_X_train=True,\n",
       "             kernel=1**2 * Matern(length_scale=1, nu=2.5) + WhiteKernel(noise_level=1),\n",
       "             n_restarts_optimizer=2, noise='gaussian', normalize_y=True,\n",
       "             optimizer='fmin_l_bfgs_b',\n",
       "             random_state=<mtrand.RandomState object at 0x7fed53e9e240>), GaussianProcessRegressor(alpha=0.0, copy_X_train=True,\n",
       "             kernel=1**2 * Matern(length_scale=1, nu=2.5) + WhiteKernel(noise_level=1),\n",
       "             n_restarts_optimizer=2, noise='gaussian', normalize_y=True,\n",
       "             optimizer='fmin_l_bfgs_b',\n",
       "             random_state=<mtrand.RandomState object at 0x7fed53e9e090>), GaussianProcessRegressor(alpha=0.0, copy_X_train=True,\n",
       "             kernel=1**2 * Matern(length_scale=1, nu=2.5) + WhiteKernel(noise_level=1),\n",
       "             n_restarts_optimizer=2, noise='gaussian', normalize_y=True,\n",
       "             optimizer='fmin_l_bfgs_b',\n",
       "             random_state=<mtrand.RandomState object at 0x7fed53e9e168>), GaussianProcessRegressor(alpha=0.0, copy_X_train=True,\n",
       "             kernel=1**2 * Matern(length_scale=1, nu=2.5) + WhiteKernel(noise_level=1),\n",
       "             n_restarts_optimizer=2, noise='gaussian', normalize_y=True,\n",
       "             optimizer='fmin_l_bfgs_b',\n",
       "             random_state=<mtrand.RandomState object at 0x7fed53e9e1f8>), GaussianProcessRegressor(alpha=0.0, copy_X_train=True,\n",
       "             kernel=1**2 * Matern(length_scale=1, nu=2.5) + WhiteKernel(noise_level=1),\n",
       "             n_restarts_optimizer=2, noise='gaussian', normalize_y=True,\n",
       "             optimizer='fmin_l_bfgs_b',\n",
       "             random_state=<mtrand.RandomState object at 0x7fed53e9e048>), GaussianProcessRegressor(alpha=0.0, copy_X_train=True,\n",
       "             kernel=1**2 * Matern(length_scale=1, nu=2.5) + WhiteKernel(noise_level=1),\n",
       "             n_restarts_optimizer=2, noise='gaussian', normalize_y=True,\n",
       "             optimizer='fmin_l_bfgs_b',\n",
       "             random_state=<mtrand.RandomState object at 0x7fed53e9e0d8>), GaussianProcessRegressor(alpha=0.0, copy_X_train=True,\n",
       "             kernel=1**2 * Matern(length_scale=1, nu=2.5) + WhiteKernel(noise_level=1),\n",
       "             n_restarts_optimizer=2, noise='gaussian', normalize_y=True,\n",
       "             optimizer='fmin_l_bfgs_b',\n",
       "             random_state=<mtrand.RandomState object at 0x7fed53e9e318>), GaussianProcessRegressor(alpha=0.0, copy_X_train=True,\n",
       "             kernel=1**2 * Matern(length_scale=1, nu=2.5) + WhiteKernel(noise_level=1),\n",
       "             n_restarts_optimizer=2, noise='gaussian', normalize_y=True,\n",
       "             optimizer='fmin_l_bfgs_b',\n",
       "             random_state=<mtrand.RandomState object at 0x7fed53e9e438>), GaussianProcessRegressor(alpha=0.0, copy_X_train=True,\n",
       "             kernel=1**2 * Matern(length_scale=1, nu=2.5) + WhiteKernel(noise_level=1),\n",
       "             n_restarts_optimizer=2, noise='gaussian', normalize_y=True,\n",
       "             optimizer='fmin_l_bfgs_b',\n",
       "             random_state=<mtrand.RandomState object at 0x7fed53e9e1b0>), GaussianProcessRegressor(alpha=0.0, copy_X_train=True,\n",
       "             kernel=1**2 * Matern(length_scale=1, nu=2.5) + WhiteKernel(noise_level=1),\n",
       "             n_restarts_optimizer=2, noise='gaussian', normalize_y=True,\n",
       "             optimizer='fmin_l_bfgs_b',\n",
       "             random_state=<mtrand.RandomState object at 0x7fed53e9e4c8>), GaussianProcessRegressor(alpha=0.0, copy_X_train=True,\n",
       "             kernel=1**2 * Matern(length_scale=1, nu=2.5) + WhiteKernel(noise_level=1),\n",
       "             n_restarts_optimizer=2, noise='gaussian', normalize_y=True,\n",
       "             optimizer='fmin_l_bfgs_b',\n",
       "             random_state=<mtrand.RandomState object at 0x7fed53e9e360>), GaussianProcessRegressor(alpha=0.0, copy_X_train=True,\n",
       "             kernel=1**2 * Matern(length_scale=1, nu=2.5) + WhiteKernel(noise_level=1),\n",
       "             n_restarts_optimizer=2, noise='gaussian', normalize_y=True,\n",
       "             optimizer='fmin_l_bfgs_b',\n",
       "             random_state=<mtrand.RandomState object at 0x7fed53e9e480>), GaussianProcessRegressor(alpha=0.0, copy_X_train=True,\n",
       "             kernel=1**2 * Matern(length_scale=1, nu=2.5) + WhiteKernel(noise_level=1),\n",
       "             n_restarts_optimizer=2, noise='gaussian', normalize_y=True,\n",
       "             optimizer='fmin_l_bfgs_b',\n",
       "             random_state=<mtrand.RandomState object at 0x7fed53e9e288>), GaussianProcessRegressor(alpha=0.0, copy_X_train=True,\n",
       "             kernel=1**2 * Matern(length_scale=1, nu=2.5) + WhiteKernel(noise_level=1),\n",
       "             n_restarts_optimizer=2, noise='gaussian', normalize_y=True,\n",
       "             optimizer='fmin_l_bfgs_b',\n",
       "             random_state=<mtrand.RandomState object at 0x7fed53e9e3a8>), GaussianProcessRegressor(alpha=0.0, copy_X_train=True,\n",
       "             kernel=1**2 * Matern(length_scale=1, nu=2.5) + WhiteKernel(noise_level=1),\n",
       "             n_restarts_optimizer=2, noise='gaussian', normalize_y=True,\n",
       "             optimizer='fmin_l_bfgs_b',\n",
       "             random_state=<mtrand.RandomState object at 0x7fed53e9e3f0>), GaussianProcessRegressor(alpha=0.0, copy_X_train=True,\n",
       "             kernel=1**2 * Matern(length_scale=1, nu=2.5) + WhiteKernel(noise_level=1),\n",
       "             n_restarts_optimizer=2, noise='gaussian', normalize_y=True,\n",
       "             optimizer='fmin_l_bfgs_b',\n",
       "             random_state=<mtrand.RandomState object at 0x7fed53e9e5a0>), GaussianProcessRegressor(alpha=0.0, copy_X_train=True,\n",
       "             kernel=1**2 * Matern(length_scale=1, nu=2.5) + WhiteKernel(noise_level=1),\n",
       "             n_restarts_optimizer=2, noise='gaussian', normalize_y=True,\n",
       "             optimizer='fmin_l_bfgs_b',\n",
       "             random_state=<mtrand.RandomState object at 0x7fed53e9e5e8>), GaussianProcessRegressor(alpha=0.0, copy_X_train=True,\n",
       "             kernel=1**2 * Matern(length_scale=1, nu=2.5) + WhiteKernel(noise_level=1),\n",
       "             n_restarts_optimizer=2, noise='gaussian', normalize_y=True,\n",
       "             optimizer='fmin_l_bfgs_b',\n",
       "             random_state=<mtrand.RandomState object at 0x7fed53e9e708>), GaussianProcessRegressor(alpha=0.0, copy_X_train=True,\n",
       "             kernel=1**2 * Matern(length_scale=1, nu=2.5) + WhiteKernel(noise_level=1),\n",
       "             n_restarts_optimizer=2, noise='gaussian', normalize_y=True,\n",
       "             optimizer='fmin_l_bfgs_b',\n",
       "             random_state=<mtrand.RandomState object at 0x7fed53e9e510>), GaussianProcessRegressor(alpha=0.0, copy_X_train=True,\n",
       "             kernel=1**2 * Matern(length_scale=1, nu=2.5) + WhiteKernel(noise_level=1),\n",
       "             n_restarts_optimizer=2, noise='gaussian', normalize_y=True,\n",
       "             optimizer='fmin_l_bfgs_b',\n",
       "             random_state=<mtrand.RandomState object at 0x7fed53e9e798>), GaussianProcessRegressor(alpha=0.0, copy_X_train=True,\n",
       "             kernel=1**2 * Matern(length_scale=1, nu=2.5) + WhiteKernel(noise_level=1),\n",
       "             n_restarts_optimizer=2, noise='gaussian', normalize_y=True,\n",
       "             optimizer='fmin_l_bfgs_b',\n",
       "             random_state=<mtrand.RandomState object at 0x7fed53e9e630>), GaussianProcessRegressor(alpha=0.0, copy_X_train=True,\n",
       "             kernel=1**2 * Matern(length_scale=1, nu=2.5) + WhiteKernel(noise_level=1),\n",
       "             n_restarts_optimizer=2, noise='gaussian', normalize_y=True,\n",
       "             optimizer='fmin_l_bfgs_b',\n",
       "             random_state=<mtrand.RandomState object at 0x7fed53e9e750>), GaussianProcessRegressor(alpha=0.0, copy_X_train=True,\n",
       "             kernel=1**2 * Matern(length_scale=1, nu=2.5) + WhiteKernel(noise_level=1),\n",
       "             n_restarts_optimizer=2, noise='gaussian', normalize_y=True,\n",
       "             optimizer='fmin_l_bfgs_b',\n",
       "             random_state=<mtrand.RandomState object at 0x7fed53e9e558>), GaussianProcessRegressor(alpha=0.0, copy_X_train=True,\n",
       "             kernel=1**2 * Matern(length_scale=1, nu=2.5) + WhiteKernel(noise_level=1),\n",
       "             n_restarts_optimizer=2, noise='gaussian', normalize_y=True,\n",
       "             optimizer='fmin_l_bfgs_b',\n",
       "             random_state=<mtrand.RandomState object at 0x7fed53e9e678>), GaussianProcessRegressor(alpha=0.0, copy_X_train=True,\n",
       "             kernel=1**2 * Matern(length_scale=1, nu=2.5) + WhiteKernel(noise_level=1),\n",
       "             n_restarts_optimizer=2, noise='gaussian', normalize_y=True,\n",
       "             optimizer='fmin_l_bfgs_b',\n",
       "             random_state=<mtrand.RandomState object at 0x7fed53e9e828>), GaussianProcessRegressor(alpha=0.0, copy_X_train=True,\n",
       "             kernel=1**2 * Matern(length_scale=1, nu=2.5) + WhiteKernel(noise_level=1),\n",
       "             n_restarts_optimizer=2, noise='gaussian', normalize_y=True,\n",
       "             optimizer='fmin_l_bfgs_b',\n",
       "             random_state=<mtrand.RandomState object at 0x7fed53e9e870>), GaussianProcessRegressor(alpha=0.0, copy_X_train=True,\n",
       "             kernel=1**2 * Matern(length_scale=1, nu=2.5) + WhiteKernel(noise_level=1),\n",
       "             n_restarts_optimizer=2, noise='gaussian', normalize_y=True,\n",
       "             optimizer='fmin_l_bfgs_b',\n",
       "             random_state=<mtrand.RandomState object at 0x7fed53e9e990>), GaussianProcessRegressor(alpha=0.0, copy_X_train=True,\n",
       "             kernel=1**2 * Matern(length_scale=1, nu=2.5) + WhiteKernel(noise_level=1),\n",
       "             n_restarts_optimizer=2, noise='gaussian', normalize_y=True,\n",
       "             optimizer='fmin_l_bfgs_b',\n",
       "             random_state=<mtrand.RandomState object at 0x7fed53e9e6c0>), GaussianProcessRegressor(alpha=0.0, copy_X_train=True,\n",
       "             kernel=1**2 * Matern(length_scale=1, nu=2.5) + WhiteKernel(noise_level=1),\n",
       "             n_restarts_optimizer=2, noise='gaussian', normalize_y=True,\n",
       "             optimizer='fmin_l_bfgs_b',\n",
       "             random_state=<mtrand.RandomState object at 0x7fed53e9ea20>), GaussianProcessRegressor(alpha=0.0, copy_X_train=True,\n",
       "             kernel=1**2 * Matern(length_scale=1, nu=2.5) + WhiteKernel(noise_level=1),\n",
       "             n_restarts_optimizer=2, noise='gaussian', normalize_y=True,\n",
       "             optimizer='fmin_l_bfgs_b',\n",
       "             random_state=<mtrand.RandomState object at 0x7fed53e9e8b8>), GaussianProcessRegressor(alpha=0.0, copy_X_train=True,\n",
       "             kernel=1**2 * Matern(length_scale=1, nu=2.5) + WhiteKernel(noise_level=1),\n",
       "             n_restarts_optimizer=2, noise='gaussian', normalize_y=True,\n",
       "             optimizer='fmin_l_bfgs_b',\n",
       "             random_state=<mtrand.RandomState object at 0x7fed53e9e9d8>), GaussianProcessRegressor(alpha=0.0, copy_X_train=True,\n",
       "             kernel=1**2 * Matern(length_scale=1, nu=2.5) + WhiteKernel(noise_level=1),\n",
       "             n_restarts_optimizer=2, noise='gaussian', normalize_y=True,\n",
       "             optimizer='fmin_l_bfgs_b',\n",
       "             random_state=<mtrand.RandomState object at 0x7fed53e9e7e0>), GaussianProcessRegressor(alpha=0.0, copy_X_train=True,\n",
       "             kernel=1**2 * Matern(length_scale=1, nu=2.5) + WhiteKernel(noise_level=1),\n",
       "             n_restarts_optimizer=2, noise='gaussian', normalize_y=True,\n",
       "             optimizer='fmin_l_bfgs_b',\n",
       "             random_state=<mtrand.RandomState object at 0x7fed53e9e900>), GaussianProcessRegressor(alpha=0.0, copy_X_train=True,\n",
       "             kernel=1**2 * Matern(length_scale=1, nu=2.5) + WhiteKernel(noise_level=1),\n",
       "             n_restarts_optimizer=2, noise='gaussian', normalize_y=True,\n",
       "             optimizer='fmin_l_bfgs_b',\n",
       "             random_state=<mtrand.RandomState object at 0x7fed53e9e948>), GaussianProcessRegressor(alpha=0.0, copy_X_train=True,\n",
       "             kernel=1**2 * Matern(length_scale=1, nu=2.5) + WhiteKernel(noise_level=1),\n",
       "             n_restarts_optimizer=2, noise='gaussian', normalize_y=True,\n",
       "             optimizer='fmin_l_bfgs_b',\n",
       "             random_state=<mtrand.RandomState object at 0x7fed53e9eaf8>), GaussianProcessRegressor(alpha=0.0, copy_X_train=True,\n",
       "             kernel=1**2 * Matern(length_scale=1, nu=2.5) + WhiteKernel(noise_level=1),\n",
       "             n_restarts_optimizer=2, noise='gaussian', normalize_y=True,\n",
       "             optimizer='fmin_l_bfgs_b',\n",
       "             random_state=<mtrand.RandomState object at 0x7fed53e9eb40>), GaussianProcessRegressor(alpha=0.0, copy_X_train=True,\n",
       "             kernel=1**2 * Matern(length_scale=1, nu=2.5) + WhiteKernel(noise_level=1),\n",
       "             n_restarts_optimizer=2, noise='gaussian', normalize_y=True,\n",
       "             optimizer='fmin_l_bfgs_b',\n",
       "             random_state=<mtrand.RandomState object at 0x7fed53e9ec60>), GaussianProcessRegressor(alpha=0.0, copy_X_train=True,\n",
       "             kernel=1**2 * Matern(length_scale=1, nu=2.5) + WhiteKernel(noise_level=1),\n",
       "             n_restarts_optimizer=2, noise='gaussian', normalize_y=True,\n",
       "             optimizer='fmin_l_bfgs_b',\n",
       "             random_state=<mtrand.RandomState object at 0x7fed53e9ea68>), GaussianProcessRegressor(alpha=0.0, copy_X_train=True,\n",
       "             kernel=1**2 * Matern(length_scale=1, nu=2.5) + WhiteKernel(noise_level=1),\n",
       "             n_restarts_optimizer=2, noise='gaussian', normalize_y=True,\n",
       "             optimizer='fmin_l_bfgs_b',\n",
       "             random_state=<mtrand.RandomState object at 0x7fed53e9ecf0>), GaussianProcessRegressor(alpha=0.0, copy_X_train=True,\n",
       "             kernel=1**2 * Matern(length_scale=1, nu=2.5) + WhiteKernel(noise_level=1),\n",
       "             n_restarts_optimizer=2, noise='gaussian', normalize_y=True,\n",
       "             optimizer='fmin_l_bfgs_b',\n",
       "             random_state=<mtrand.RandomState object at 0x7fed53e9eb88>), GaussianProcessRegressor(alpha=0.0, copy_X_train=True,\n",
       "             kernel=1**2 * Matern(length_scale=1, nu=2.5) + WhiteKernel(noise_level=1),\n",
       "             n_restarts_optimizer=2, noise='gaussian', normalize_y=True,\n",
       "             optimizer='fmin_l_bfgs_b',\n",
       "             random_state=<mtrand.RandomState object at 0x7fed53e9eca8>), GaussianProcessRegressor(alpha=0.0, copy_X_train=True,\n",
       "             kernel=1**2 * Matern(length_scale=1, nu=2.5) + WhiteKernel(noise_level=1),\n",
       "             n_restarts_optimizer=2, noise='gaussian', normalize_y=True,\n",
       "             optimizer='fmin_l_bfgs_b',\n",
       "             random_state=<mtrand.RandomState object at 0x7fed53e9eab0>), GaussianProcessRegressor(alpha=0.0, copy_X_train=True,\n",
       "             kernel=1**2 * Matern(length_scale=1, nu=2.5) + WhiteKernel(noise_level=1),\n",
       "             n_restarts_optimizer=2, noise='gaussian', normalize_y=True,\n",
       "             optimizer='fmin_l_bfgs_b',\n",
       "             random_state=<mtrand.RandomState object at 0x7fed53e9ebd0>), GaussianProcessRegressor(alpha=0.0, copy_X_train=True,\n",
       "             kernel=1**2 * Matern(length_scale=1, nu=2.5) + WhiteKernel(noise_level=1),\n",
       "             n_restarts_optimizer=2, noise='gaussian', normalize_y=True,\n",
       "             optimizer='fmin_l_bfgs_b',\n",
       "             random_state=<mtrand.RandomState object at 0x7fed53e9ed80>), GaussianProcessRegressor(alpha=0.0, copy_X_train=True,\n",
       "             kernel=1**2 * Matern(length_scale=1, nu=2.5) + WhiteKernel(noise_level=1),\n",
       "             n_restarts_optimizer=2, noise='gaussian', normalize_y=True,\n",
       "             optimizer='fmin_l_bfgs_b',\n",
       "             random_state=<mtrand.RandomState object at 0x7fed53e9edc8>), GaussianProcessRegressor(alpha=0.0, copy_X_train=True,\n",
       "             kernel=1**2 * Matern(length_scale=1, nu=2.5) + WhiteKernel(noise_level=1),\n",
       "             n_restarts_optimizer=2, noise='gaussian', normalize_y=True,\n",
       "             optimizer='fmin_l_bfgs_b',\n",
       "             random_state=<mtrand.RandomState object at 0x7fed53e9ee10>), GaussianProcessRegressor(alpha=0.0, copy_X_train=True,\n",
       "             kernel=1**2 * Matern(length_scale=1, nu=2.5) + WhiteKernel(noise_level=1),\n",
       "             n_restarts_optimizer=2, noise='gaussian', normalize_y=True,\n",
       "             optimizer='fmin_l_bfgs_b',\n",
       "             random_state=<mtrand.RandomState object at 0x7fed53e9ed38>), GaussianProcessRegressor(alpha=0.0, copy_X_train=True,\n",
       "             kernel=1**2 * Matern(length_scale=1, nu=2.5) + WhiteKernel(noise_level=1),\n",
       "             n_restarts_optimizer=2, noise='gaussian', normalize_y=True,\n",
       "             optimizer='fmin_l_bfgs_b',\n",
       "             random_state=<mtrand.RandomState object at 0x7fed53e9ee58>)]\n",
       " random_state: <mtrand.RandomState object at 0x7fed9404c9d8>\n",
       "        space: Space([Real(low=-2.0, high=2.0, prior=uniform, transform=identity)])\n",
       "        specs: {'function': 'base_minimize', 'args': {'callback': None, 'n_jobs': 1, 'kappa': 1.96, 'y0': None, 'func': <function f at 0x7fed5618ab70>, 'acq_func': 'gp_hedge', 'dimensions': [(-2.0, 2.0)], 'n_restarts_optimizer': 5, 'verbose': False, 'xi': 0.01, 'n_calls': 100, 'random_state': None, 'acq_optimizer': 'lbfgs', 'x0': None, 'n_random_starts': 10, 'n_points': 10000, 'base_estimator': GaussianProcessRegressor(alpha=0.0, copy_X_train=True,\n",
       "             kernel=1**2 * Matern(length_scale=1, nu=2.5),\n",
       "             n_restarts_optimizer=2, noise='gaussian', normalize_y=True,\n",
       "             optimizer='fmin_l_bfgs_b',\n",
       "             random_state=<mtrand.RandomState object at 0x7fed9404c9d8>)}}\n",
       "            x: [-0.24751505410680383]\n",
       "      x_iters: [[0.96447510994656938], [1.180161568129459], [1.4761114133503299], [-0.71990279773761001], [1.6169823189307837], [-1.4398712132885869], [-1.3555787476765042], [-1.3245651017868734], [1.7248881579560202], [-1.6969426083102439], [-2.0], [-1.9999873585974397], [-1.999725702054711], [-2.0], [1.9991618056824336], [1.9993680219632317], [-1.9995754401258972], [-2.0], [1.9998984414981975], [-2.0], [1.9995168929945546], [-0.23123014866464375], [0.02658154539332775], [1.9993047247364935], [-0.32472223704689412], [-0.19324352930784411], [2.0], [-0.88707260821377432], [-0.22064843511863674], [1.9997330628276857], [-0.24751505410680383], [1.9996257324892945], [-0.26258451028359231], [-1.9996479343311657], [1.9999647586622356], [-0.24533278721000723], [-2.0], [1.999844220823483], [1.9997794289922695], [-0.2527920388112872], [-1.9997738596891241], [2.0], [-1.999899107517594], [1.9992705935381356], [-2.0], [-0.24818609299292979], [1.9994781146882543], [2.0], [-2.0], [-1.9999563181723059], [1.9999074005896671], [2.0], [-1.998779257308223], [-1.9999323272749319], [1.9996654998511998], [-1.999845996286048], [1.9997793158065402], [2.0], [-1.9997294217304895], [-0.042431130920553173], [2.0], [1.0395729022120266], [0.79920679871549494], [0.048844997492145215], [0.54775998311630891], [0.59090011459898317], [-1.9436312976172134], [0.82523006631757534], [-2.0], [-0.8049490812958866], [0.043056249979993133], [2.0], [0.20568822052265645], [2.0], [0.10888108321307399], [2.0], [-2.0], [2.0], [1.0813172032130893], [-2.0], [2.0], [-1.4492766273112752], [1.8266085929448397], [-0.45053693628721891], [0.7990806279090239], [2.0], [-1.5625304432073381], [1.591654394115654], [-1.2183424871805224], [2.0], [1.8613469378041168], [0.38219596448507387], [2.0], [1.1832448304002576], [0.98174177041843391], [-0.69219285363237137], [-2.0], [-2.0], [1.7267708365432797], [0.53890709861166686]]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import skopt\n",
    "\n",
    "def f(x):\n",
    "\n",
    "    return (np.sin(5 * x[0]) * (1 - np.tanh(x[0] ** 2)) * np.random.randn() * 0.1)\n",
    "\n",
    "result = skopt.gp_minimize(f, [(-2.0, 2.0)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.23204481252472509"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[\"fun\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The object returned is a `scipy.optimize.OptimizeResult`, which is essentially a dictionary subclass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['x_iters', 'random_state', 'models', 'space', 'func_vals', 'fun', 'x', 'specs'])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameter optimization of a machine learning algorithm is often an exchaustive exploration of a subset of the space of all hyperparameter configurations (for example, using a grid search like `sklearn.model_selection.GridSearchCV`), which can be time-consuming. Scikit-Optimize `gp_minimize` can be used to tune hyperparameters using sequential model-based optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### example: minimize the cross-validation mean absolute error objective function of a gradient boosting regression model over the Boston dataset as a function of its hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from skopt import gp_minimize\n",
    "\n",
    "boston = load_boston()\n",
    "X, y = boston.data, boston.target\n",
    "n_features = X.shape[1]\n",
    "\n",
    "model = GradientBoostingRegressor(\n",
    "    n_estimators = 50,\n",
    "    random_state = 0\n",
    ")\n",
    "\n",
    "def objective(parameters):\n",
    "\n",
    "    max_depth, learning_rate, max_features, min_samples_split, min_samples_leaf = parameters\n",
    "\n",
    "    model.set_params(\n",
    "        max_depth         = max_depth,\n",
    "        learning_rate     = learning_rate,\n",
    "        max_features      = max_features,\n",
    "        min_samples_split = min_samples_split, \n",
    "        min_samples_leaf  = min_samples_leaf\n",
    "    )\n",
    "\n",
    "    return -np.mean(\n",
    "        cross_val_score(\n",
    "            model,\n",
    "            X,\n",
    "            y,\n",
    "            cv      = 5,\n",
    "            n_jobs  = -1,\n",
    "            scoring = \"neg_mean_absolute_error\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "# bounds of the dimensions of the search space to explore\n",
    "space = [\n",
    "    (1, 5),                             # max_depth\n",
    "    (10 ** -5, 10 ** 0, \"log-uniform\"), # learning_rate\n",
    "    (1, n_features),                    # max_features\n",
    "    (2, 100),                           # min_samples_split\n",
    "    (1, 100)                            # min_samples_leaf\n",
    "]\n",
    "\n",
    "# sequential model-based optimisation\n",
    "result = gp_minimize(\n",
    "    objective,\n",
    "    space,\n",
    "    n_calls      = 100,\n",
    "    random_state = 0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "best score: 2.7846223438534983\n",
      "\n",
      "best parameters:\n",
      "\n",
      "- max_depth:         4\n",
      "- learning_rate:     0.164085851332118\n",
      "- max_features:      7\n",
      "- min_samples_split: 2\n",
      "- min_samples_leaf:  1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "\"\"\"\n",
    "best score: {best_score}\n",
    "\n",
    "best parameters:\n",
    "\n",
    "- max_depth:         {max_depth}\n",
    "- learning_rate:     {learning_rate}\n",
    "- max_features:      {max_features}\n",
    "- min_samples_split: {min_samples_split}\n",
    "- min_samples_leaf:  {min_samples_leaf}\n",
    "\"\"\".format(\n",
    "    best_score        = result[\"fun\"],\n",
    "    max_depth         = result[\"x\"][0],\n",
    "    learning_rate     = result[\"x\"][1],\n",
    "    max_features      = result[\"x\"][2],\n",
    "    min_samples_split = result[\"x\"][3], \n",
    "    min_samples_leaf  = result[\"x\"][4]\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7fed532be1d0>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZcAAAEpCAYAAABfpm8IAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fed532d23c8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from skopt.plots import plot_convergence\n",
    "%matplotlib inline\n",
    "sns.set(context = \"paper\", font = \"monospace\")\n",
    "\n",
    "plot_convergence(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
